{"id": "pwc:task/active-object-detection", "type": "task", "name": "Active Object Detection", "description": "Active Learning for Object Detection", "categories": []}
{"id": "pwc:task/active-learning", "type": "task", "name": "Active Learning", "description": "**Active Learning** is a paradigm in supervised machine learning which uses fewer training examples to achieve better optimization by iteratively training a predictor, and using the predictor in each iteration to choose the training examples which will increase its chances of finding better configurations and at the same time improving the accuracy of the prediction model\r\n\r\n\r\n<span class=\"description-source\">Source: [Polystore++: Accelerated Polystore System for Heterogeneous Workloads ](https://arxiv.org/abs/1905.10336)</span>", "categories": ["Natural Language Processing", "Computer Vision", "Methodology"]}
{"id": "pwc:task/handwriting-recognition", "type": "task", "name": "Handwriting Recognition", "description": "Image source: [Handwriting Recognition of Historical Documents with few labeled data](https://arxiv.org/pdf/1811.07768v1.pdf)", "categories": []}
{"id": "pwc:task/handwritten-digit-recognition", "type": "task", "name": "Handwritten Digit Recognition", "description": "", "categories": []}
{"id": "pwc:task/irregular-text-recognition", "type": "task", "name": "Irregular Text Recognition", "description": "To read a text from an image might be difficult due to the improper angle of the text inside the image or due to surprising font. Hence, to recognize the text data from the image, Irregular Text Recognition is used.", "categories": []}
{"id": "pwc:task/handwritten-chinese-text-recognition", "type": "task", "name": "Handwritten Chinese Text Recognition", "description": "Handwritten Chinese text recognition is the task of interpreting handwritten Chinese input, e.g., from images of documents or scans.", "categories": []}
{"id": "pwc:task/offline-handwritten-chinese-character", "type": "task", "name": "Offline Handwritten Chinese Character Recognition", "description": "Handwritten Chinese characters recognition is the task of detecting and interpreting the components of Chinese characters (i.e. radicals and two-dimensional structures).", "categories": []}
{"id": "pwc:task/word-spotting-in-handwritten-documents", "type": "task", "name": "Word Spotting In Handwritten Documents", "description": "", "categories": []}
{"id": "pwc:task/handwritten-digit-image-synthesis", "type": "task", "name": "Handwritten Digit Image Synthesis", "description": "", "categories": []}
{"id": "pwc:task/optical-character-recognition", "type": "task", "name": "Optical Character Recognition", "description": "Optical character recognition or optical character reader (OCR) is the electronic or mechanical conversion of images of typed, handwritten or printed text into machine-encoded text, whether from a scanned document, a photo of a document, a scene-photo (for example the text on signs and billboards in a landscape photo, license plates in cars...) or from subtitle text superimposed on an image (for example: from a television broadcast)", "categories": ["Natural Language Processing", "Computer Vision", "Methodology"]}
{"id": "pwc:task/blind-image-deblurring", "type": "task", "name": "Blind Image Deblurring", "description": "**Blind Image Deblurring** is a classical problem in image processing and computer vision, which aims to recover a latent image from a blurred input.\r\n\r\n\r\n<span class=\"description-source\">Source: [Learning a Discriminative Prior for Blind Image Deblurring ](https://arxiv.org/abs/1803.03363)</span>", "categories": []}
{"id": "pwc:task/single-image-blind-deblurring", "type": "task", "name": "Single-Image Blind Deblurring", "description": "", "categories": []}
{"id": "pwc:task/deblurring", "type": "task", "name": "Deblurring", "description": "<span style=\"color:grey; opacity: 0.6\">( Image credit: [Deblurring Face Images using Uncertainty Guided Multi-Stream Semantic Networks](https://arxiv.org/pdf/1907.13106v1.pdf) )</span>", "categories": ["Computer Vision"]}
{"id": "pwc:task/foveation", "type": "task", "name": "Foveation", "description": "", "categories": ["Computer Vision"]}
{"id": "pwc:task/crime-prediction", "type": "task", "name": "Crime Prediction", "description": "", "categories": ["Miscellaneous"]}
{"id": "pwc:task/structured-prediction", "type": "task", "name": "Structured Prediction", "description": "**Structured Prediction** is an area of machine learning focusing on representations of spaces with combinatorial structure, and algorithms for inference and parameter estimation over these structures. Core methods include both tractable exact approaches like dynamic programming and spanning tree algorithms as well as heuristic techniques such as linear programming relaxations and greedy search.\n\n\n<span class=\"description-source\">Source: [Torch-Struct: Deep Structured Prediction Library ](https://arxiv.org/abs/2002.00876)</span>", "categories": ["Methodology"]}
{"id": "pwc:task/trajectory-forecasting", "type": "task", "name": "Trajectory Forecasting", "description": "Trajectory forecasting is a sequential prediction task, where a forecasting model predicts future trajectories of all moving agents (humans, vehicles, etc.) in a scene, based on their past trajectories and/or the scene context.\r\n\r\n(Illustrative figure from [Social NCE: Contrastive Learning of Socially-aware Motion Representations](https://github.com/vita-epfl/social-nce))", "categories": []}
{"id": "pwc:task/human-motion-prediction", "type": "task", "name": "Human motion prediction", "description": "Action prediction is a pre-fact video understanding task, which focuses on future states, in other words, it needs to reason about future states or infer action labels before the end of action execution.", "categories": []}
{"id": "pwc:task/trajectory-prediction", "type": "task", "name": "Trajectory Prediction", "description": "**Trajectory Prediction** is the problem of predicting the short-term (1-3 seconds) and long-term (3-5 seconds) spatial coordinates of various road-agents such as cars, buses, pedestrians, rickshaws, and animals, etc. These road-agents have different dynamic behaviors that may correspond to aggressive or conservative driving styles.\r\n\r\n\r\n<span class=\"description-source\">Source: [Forecasting Trajectory and Behavior of Road-Agents Using Spectral Clustering in Graph-LSTMs ](https://arxiv.org/abs/1912.01118)</span>", "categories": ["Miscellaneous", "Time Series", "Natural Language Processing", "Computer Vision"]}
{"id": "pwc:task/monocular-depth-estimation", "type": "task", "name": "Monocular Depth Estimation", "description": "**Monocular Depth Estimation** is the task of estimating the depth value (distance relative to the camera) of each pixel given a single (monocular) RGB image. This challenging task is a key prerequisite for determining scene understanding for applications such as 3D scene reconstruction, autonomous driving, and AR. State-of-the-art methods usually fall into one of two categories: designing a complex network that is powerful enough to directly regress the depth map, or splitting the input into bins or windows to reduce computational complexity.  The most popular benchmarks are the KITTI and NYUv2 datasets. Models are typically evaluated using RMSE or absolute relative error. \r\n\r\n<span class=\"description-source\">Source: [Defocus Deblurring Using Dual-Pixel Data ](https://arxiv.org/abs/2005.00305)</span>", "categories": []}
{"id": "pwc:task/stereo-depth-estimation", "type": "task", "name": "Stereo Depth Estimation", "description": "", "categories": []}
{"id": "pwc:task/face-anti-spoofing", "type": "task", "name": "Face Anti-Spoofing", "description": "Facial anti-spoofing is the task of preventing false facial verification by using a photo, video, mask or a different substitute for an authorized person\u2019s face. Some examples of attacks:\r\n\r\n- **Print attack**: The attacker uses someone\u2019s photo. The image is printed or displayed on a digital device.\r\n\r\n- **Replay/video attack**: A more sophisticated way to trick the system, which usually requires a looped video of a victim\u2019s face. This approach ensures behaviour and facial movements to look more \u2018natural\u2019 compared to holding someone\u2019s photo.\r\n\r\n- **3D mask attack**: During this type of attack, a mask is used as the tool of choice for spoofing. It\u2019s an even more sophisticated attack than playing a face video. In addition to natural facial movements, it enables ways to deceive some extra layers of protection such as depth sensors.\r\n\r\n\r\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [Learning Generalizable and Identity-Discriminative Representations for Face Anti-Spoofing](https://github.com/XgTu/GFA-CNN) )</span>", "categories": []}
{"id": "pwc:task/depth-and-camera-motion", "type": "task", "name": "Depth And Camera Motion", "description": "", "categories": ["Computer Vision"]}
{"id": "pwc:task/transparent-object-depth-estimation", "type": "task", "name": "Transparent Object Depth Estimation", "description": "Estimating the 3D shape of transparent objects", "categories": []}
{"id": "pwc:task/3d-depth-estimation", "type": "task", "name": "3D Depth Estimation", "description": "Image: [monodepth2](https://github.com/nianticlabs/monodepth2)", "categories": ["Computer Vision"]}
{"id": "pwc:task/stereo-lidar-fusion", "type": "task", "name": "Stereo-LiDAR Fusion", "description": "Depth estimation using stereo cameras and a LiDAR sensor.", "categories": []}
{"id": "pwc:task/depth-map-super-resolution", "type": "task", "name": "Depth Map Super-Resolution", "description": "Depth map super-resolution is the task of upsampling depth images.\r\n\r\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [A Joint Intensity and Depth Co-Sparse Analysis Model\r\nfor Depth Map Super-Resolution](https://arxiv.org/pdf/1304.5319v1.pdf) )</span>", "categories": []}
{"id": "pwc:task/indoor-monocular-depth-estimation", "type": "task", "name": "Indoor Monocular Depth Estimation", "description": "", "categories": []}
{"id": "pwc:task/depth-image-upsampling", "type": "task", "name": "Depth Image Upsampling", "description": "", "categories": []}
{"id": "pwc:task/depth-estimation", "type": "task", "name": "Depth Estimation", "description": "**Depth Estimation** is the task of measuring the distance of each pixel relative to the camera. Depth is extracted from either monocular (single) or stereo (multiple views of a scene) images. Traditional methods use multi-view geometry to find the relationship between the images. Newer methods can directly estimate depth by minimizing the regression loss, or by learning to generate a novel view from a sequence. The most popular benchmarks are KITTI and NYUv2. Models are typically evaluated according to a RMS metric.\r\n\r\n<span class=\"description-source\">Source: [DIODE: A Dense Indoor and Outdoor DEpth Dataset ](https://arxiv.org/abs/1908.00463)</span>", "categories": ["Computer Vision"]}
{"id": "pwc:task/malaria-risk-exposure-prediction", "type": "task", "name": "Malaria Risk Exposure Prediction", "description": "", "categories": ["Medical"]}
{"id": "pwc:task/diabetes-prediction", "type": "task", "name": "Diabetes Prediction", "description": "", "categories": ["Medical"]}
{"id": "pwc:task/medical-code-prediction", "type": "task", "name": "Medical Code Prediction", "description": "Context: Prediction of medical codes from clinical notes is both a practical and essential need for every healthcare delivery organization within current medical systems. Automating annotation will save significant time and excessive effort by human coders today. A new milestone will mark a meaningful step toward fully Autonomous Medical Coding in machines reaching parity with human coders' performance in medical code prediction.\r\n\r\nQuestion: What exactly is the medical code prediction problem?\r\n\r\nAnswer: Clinical notes contain much information about what precisely happened during the patient's entire stay. And those clinical notes (e.g., discharge summary) is typically long, loosely structured, consists of medical domain language, and sometimes riddled with spelling errors. So, it's a highly multi-label classification problem, and the forthcoming ICD-11 standard will add more complexity to the problem! The medical code prediction problem is to annotate this clinical note with multiple codes subset from nearly 70K total codes (in the current ICD-10 system, for example).", "categories": ["Medical"]}
{"id": "pwc:task/physical-attribute-prediction", "type": "task", "name": "Physical Attribute Prediction", "description": "", "categories": ["Computer Vision"]}
{"id": "pwc:task/text-based-stock-prediction", "type": "task", "name": "Text-Based Stock Prediction", "description": "Make stock predictions based on text (e.g., news articles, twitters, etc.).", "categories": []}
{"id": "pwc:task/event-driven-trading", "type": "task", "name": "Event-Driven Trading", "description": "Making stock trading decisions based on events.", "categories": []}
{"id": "pwc:task/stock-prediction", "type": "task", "name": "Stock Prediction", "description": "", "categories": ["Natural Language Processing"]}
{"id": "pwc:task/stock-price-prediction", "type": "task", "name": "Stock Price Prediction", "description": "", "categories": []}
{"id": "pwc:task/stock-trend-prediction", "type": "task", "name": "Stock Trend Prediction", "description": "", "categories": ["Time Series"]}
{"id": "pwc:task/stock-market-prediction", "type": "task", "name": "Stock Market Prediction", "description": "", "categories": ["Time Series"]}
{"id": "pwc:task/inductive-link-prediction", "type": "task", "name": "Inductive Link Prediction", "description": "In inductive link prediction inference is performed on a new, unseen graph whereas classical transductive link prediction performs both training and inference on the same graph.", "categories": []}
{"id": "pwc:task/dynamic-link-prediction", "type": "task", "name": "Dynamic Link Prediction", "description": "", "categories": []}
{"id": "pwc:task/calibration-for-link-prediction", "type": "task", "name": "Calibration for Link Prediction", "description": "", "categories": []}
{"id": "pwc:task/link-prediction-on-dh-kgs", "type": "task", "name": "Link prediction on DH-KGs", "description": "", "categories": []}
{"id": "pwc:task/anchor-link-prediction", "type": "task", "name": "Anchor link prediction", "description": "", "categories": []}
{"id": "pwc:task/link-prediction", "type": "task", "name": "Link Prediction", "description": "Link prediction is a task to estimate the probability of links between nodes in a graph.\r\n\r\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [Inductive Representation Learning on Large Graphs](https://arxiv.org/pdf/1706.02216v4.pdf) )</span>", "categories": ["Natural Language Processing", "Graphs", "Knowledge Base"]}
{"id": "pwc:task/word-alignment", "type": "task", "name": "Word Alignment", "description": "**Word Alignment** is the task of finding the correspondence between source and target words in a pair of sentences that are translations of each other.\n\n\n<span class=\"description-source\">Source: [Neural Network-based Word Alignment through Score Aggregation ](https://arxiv.org/abs/1606.09560)</span>", "categories": ["Natural Language Processing"]}
{"id": "pwc:task/graph-construction", "type": "task", "name": "graph construction", "description": "", "categories": ["Graphs"]}
{"id": "pwc:task/multi-frame-super-resolution", "type": "task", "name": "Multi-Frame Super-Resolution", "description": "When multiple images of the same view are taken from slightly different positions, perhaps also at different times, then they collectively contain more information than any single image on its own. Multi-Frame Super-Resolution fuses these low-res inputs into a composite high-res image that can reveal some of the original detail that cannot be recovered from any low-res image alone.\r\n\r\n<span style=\"color:grey; opacity: 0.6\">( Credit: [HighRes-net](https://github.com/ElementAI/HighRes-net) )</span>", "categories": []}
{"id": "pwc:task/burst-image-super-resolution", "type": "task", "name": "Burst Image Super-Resolution", "description": "Reconstruct a high-resolution image from a set of low-quality images, very like the multi-frame super-resolution task.", "categories": []}
{"id": "pwc:task/stereo-image-super-resolution", "type": "task", "name": "Stereo Image Super-Resolution", "description": "", "categories": []}
{"id": "pwc:task/satellite-image-super-resolution", "type": "task", "name": "satellite image super-resolution", "description": "", "categories": []}
{"id": "pwc:task/image-super-resolution", "type": "task", "name": "Image Super-Resolution", "description": "Super-resolution of images refers to augmenting and increasing the resolution of an image using classic and advanced super-resolution techniques. Often the term 'hallucinate' is used to refer to the process of creating data points.", "categories": ["Computer Vision"]}
{"id": "pwc:task/single-image-super-resolution", "type": "task", "name": "Single Image Super Resolution", "description": "", "categories": []}
{"id": "pwc:task/key-frame-based-video-super-resolution--k---15-", "type": "task", "name": "Key-Frame-based Video Super-Resolution (K = 15)", "description": "Key-Frame-based Video Super-Resolution is a sub-task of [Video Super-Resolution](https://paperswithcode.com/task/video-super-resolution), where, in addition to the low-resolution frames, high-resolution ground-truth frames for every Kth input frame are also provided as inputs to the model. For example, if `[LR-frame-1, LR-frame-2, LR-frame-3, ..., LR-frame-100]` is the sequence of low-resolution frames to be upscaled, the Key-Frame-based Video Super-Resolution (K = 15) model is also provided with the high-resolution frames `[HR-frame-1, HR-frame-16, ..., HR-frame-91]` . Key-frames are excluded when measuring the evaluation metrics.", "categories": []}
{"id": "pwc:task/video-super-resolution", "type": "task", "name": "Video Super-Resolution", "description": "Video super-resolution is the task of upscaling a video from a low-resolution to a high-resolution.\r\n\r\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [Detail-revealing Deep Video Super-Resolution](https://github.com/jiangsutx/SPMC_VideoSR) )</span>", "categories": ["Computer Vision"]}
{"id": "pwc:task/reference-based-super-resolution", "type": "task", "name": "Reference-based Super-Resolution", "description": "Reference-based Super-Resolution aims to recover high-resolution images by utilizing external reference images containing similar content to generate rich textures.", "categories": []}
{"id": "pwc:task/depth-map-super-resolution", "type": "task", "name": "Depth Map Super-Resolution", "description": "Depth map super-resolution is the task of upsampling depth images.\r\n\r\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [A Joint Intensity and Depth Co-Sparse Analysis Model\r\nfor Depth Map Super-Resolution](https://arxiv.org/pdf/1304.5319v1.pdf) )</span>", "categories": []}
{"id": "pwc:task/reference-based-video-super-resolution", "type": "task", "name": "Reference-based Video Super-Resolution", "description": "Reference-based video super-resolution (RefVSR) is an expansion of reference-based super-resolution (RefSR) to the video super-resolution (VSR). RefVSR inherits the objectives of both RefSR and VSR tasks and utilizes a Ref video for reconstructing an HR video from an LR video\r\nvideo from an LR video.", "categories": []}
{"id": "pwc:task/3d-object-super-resolution", "type": "task", "name": "3D Object Super-Resolution", "description": "3D object super-resolution is the task of up-sampling 3D objects.\r\n\r\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [Multi-View Silhouette and Depth Decomposition for High Resolution 3D Object Representation](https://github.com/EdwardSmith1884/Multi-View-Silhouette-and-Depth-Decomposition-for-High-Resolution-3D-Object-Representation) )</span>", "categories": ["Computer Vision"]}
{"id": "pwc:task/super-resolution", "type": "task", "name": "Super-Resolution", "description": "Super resolution is the task of taking an input of a low resolution (LR) and upscaling it to that of a high resolution.\r\n\r\nYou can find relevant leaderboards in the subtasks below. \r\n\r\n<span style=\"color:grey; opacity: 0.6\">( Credit: [MemNet](https://github.com/tyshiwo/MemNet) )</span>", "categories": ["Computer Vision"]}
{"id": "pwc:task/single-cell-modeling", "type": "task", "name": "Single-cell modeling", "description": "Single Cell RNA sequencing (scRNAseq) revolutionized our understanding of the fundamental of life sciences. The technology enables an unprecedented resolution to study heterogeneity in cell populations and their functionalities.", "categories": ["Medical"]}
{"id": "pwc:task/eeg-decoding", "type": "task", "name": "Eeg Decoding", "description": "**EEG Decoding** - extracting useful information directly from EEG data.", "categories": ["Time Series", "Medical"]}
{"id": "pwc:task/eeg-denoising", "type": "task", "name": "EEG Denoising", "description": "", "categories": []}
{"id": "pwc:task/attention-score-prediction", "type": "task", "name": "Attention Score Prediction", "description": "Auditory Attention Score Prediction: Estimating the attention level of Listener from physiological signals (EEG, GSR, PPG), a regression task. The attention score ranges from 0 to 100.", "categories": []}
{"id": "pwc:task/semanticity-prediction", "type": "task", "name": "Semanticity prediction", "description": "T3: Semanticity Prediction: Estimating the semanticity perceived by Listener from physiological signals (EEG, GSP, PPG). Label: 0-(semantic), 1-(non-semantic). Binary classification problem.", "categories": []}
{"id": "pwc:task/noise-level-prediction", "type": "task", "name": "Noise Level Prediction", "description": "T2: Noise Level Prediction: Estimating the noise level experienced by the Listener from physiological signals (EEG, GSR, PPG). Six different levels of  background noise (SNR)\r\nLabel: -6, -3, 0, 3, 6, and inf (noise-free)  in dB.", "categories": []}
{"id": "pwc:task/lwr-classification", "type": "task", "name": "LWR Classification", "description": "T4: LWR Classification: Predicting if the subject is Listening, Writing, or Resting from physiological signals (EEG, GSR, PPG). Labels: 0-listening, 1-writing, 2-resting. Classification tasks.", "categories": []}
{"id": "pwc:task/eeg", "type": "task", "name": "EEG", "description": "Electroencephalogram (EEG) is a method of recording brain activity using electrophysiological indexes. When the brain is active, a large number of postsynaptic potentials generated synchronously by neurons are formed after summation. It records the changes of electric waves during brain activity and is the overall reflection of the electrophysiological activities of brain nerve cells on the surface of cerebral cortex or scalp. Brain waves originate from the postsynaptic potential of the apical dendrites of pyramidal cells. The formation of synchronous rhythm of EEG is also related to the activity of nonspecific projection system of cortex and thalamus. EEG is the basic theoretical research of brain science. EEG monitoring is widely used in its clinical application.", "categories": ["Time Series", "Medical", "Methodology"]}
{"id": "pwc:task/medical-x-ray-image-segmentation", "type": "task", "name": "Medical X-Ray Image Segmentation", "description": "", "categories": []}
{"id": "pwc:task/low-dose-x-ray-ct-reconstruction", "type": "task", "name": "Low-Dose X-Ray Ct Reconstruction", "description": "", "categories": []}
{"id": "pwc:task/bone-suppression-from-dual-energy-chest-x-rays", "type": "task", "name": "Bone Suppression From Dual Energy Chest X-Rays", "description": "", "categories": []}
{"id": "pwc:task/joint-vertebrae-identification-and", "type": "task", "name": "Joint Vertebrae Identification And Localization In Spinal Ct Images", "description": "", "categories": []}
{"id": "pwc:task/finding-pulmonary-nodules-in-large-scale-ct-images", "type": "task", "name": "Finding Pulmonary Nodules In Large-Scale Ct Images", "description": "", "categories": []}
{"id": "pwc:task/mapping-of-lung-nodules-in-low-dose-ct-images", "type": "task", "name": "Mapping Of Lung Nodules In Low-Dose Ct Images", "description": "", "categories": []}
{"id": "pwc:task/cbct-artifact-reduction", "type": "task", "name": "Cbct Artifact Reduction", "description": "", "categories": []}
{"id": "pwc:task/x-ray", "type": "task", "name": "X-Ray", "description": "", "categories": ["Medical"]}
{"id": "pwc:task/breast-cancer-detection", "type": "task", "name": "Breast Cancer Detection", "description": "", "categories": []}
{"id": "pwc:task/skin-cancer-classification", "type": "task", "name": "Skin Cancer Classification", "description": "", "categories": []}
{"id": "pwc:task/breast-cancer-histology-image-classification-1", "type": "task", "name": "Breast Cancer Histology Image Classification (20% labels)", "description": "Model's breast cancer histology image classification performance on BreakHis dataset with limited training data labels of 20%.", "categories": []}
{"id": "pwc:task/breast-cancer-histology-image-classification", "type": "task", "name": "Breast Cancer Histology Image Classification", "description": "", "categories": ["Computer Vision", "Knowledge Base"]}
{"id": "pwc:task/lung-cancer-diagnosis", "type": "task", "name": "Lung Cancer Diagnosis", "description": "", "categories": []}
{"id": "pwc:task/classification-of-breast-cancer-histology", "type": "task", "name": "Classification Of Breast Cancer Histology Images", "description": "", "categories": []}
{"id": "pwc:task/respiratory-motion-forecasting", "type": "task", "name": "Respiratory motion forecasting", "description": "Respiratory motion forecasting to compensate for the latency of the radiotherapy treatment systems and target more accurately chest tumors.", "categories": []}
{"id": "pwc:task/prediction-of-cancer-cell-line-sensitivity", "type": "task", "name": "Prediction Of Cancer Cell Line Sensitivity", "description": "", "categories": []}
{"id": "pwc:task/oral-cancer-classification", "type": "task", "name": "Oral Cancer Classification", "description": "", "categories": []}
{"id": "pwc:task/discovery-of-integrative-cancer-subtypes", "type": "task", "name": "Discovery Of Integrative Cancer Subtypes", "description": "", "categories": []}
{"id": "pwc:task/colon-cancer-detection-in-confocal-laser", "type": "task", "name": "Colon Cancer Detection In Confocal Laser Microscopy Images", "description": "", "categories": []}
{"id": "pwc:task/cancer", "type": "task", "name": "Cancer", "description": "", "categories": ["Knowledge Base", "Medical"]}
{"id": "pwc:task/3d-human-reconstruction", "type": "task", "name": "3D Human Reconstruction", "description": "", "categories": []}
{"id": "pwc:task/3d-semantic-scene-completion-from-a-single", "type": "task", "name": "3D Semantic Scene Completion from a single RGB image", "description": "This task relies on a single RGB image to infer the dense 3D voxelized semantic scene.", "categories": []}
{"id": "pwc:task/single-view-3d-reconstruction", "type": "task", "name": "Single-View 3D Reconstruction", "description": "", "categories": ["Computer Vision"]}
{"id": "pwc:task/tone-mapping", "type": "task", "name": "Tone Mapping", "description": "", "categories": []}
{"id": "pwc:task/single-image-based-hdr-reconstruction", "type": "task", "name": "Single-Image-Based Hdr Reconstruction", "description": "", "categories": ["Computer Vision"]}
{"id": "pwc:task/reconstruction", "type": "task", "name": "Reconstruction", "description": "", "categories": ["Computer Vision"]}
{"id": "pwc:task/age-invariant-face-recognition", "type": "task", "name": "Age-Invariant Face Recognition", "description": "Age-invariant face recognition is the task of performing face recognition that is invariant to differences in age.\r\n\r\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [Look Across Elapse](https://arxiv.org/pdf/1809.00338v2.pdf) )</span>", "categories": []}
{"id": "pwc:task/face-quality-assessement", "type": "task", "name": "Face Quality Assessement", "description": "Estimate the usability of a given face image for recognition", "categories": []}
{"id": "pwc:task/unsupervised-face-recognition", "type": "task", "name": "Unsupervised face recognition", "description": "", "categories": []}
{"id": "pwc:task/face-recognition", "type": "task", "name": "Face Recognition", "description": "Facial recognition is the task of making a positive identification of a face in a photo or video image against a pre-existing database of faces. It begins with detection - distinguishing human faces from other objects in the image - and then works on identification of those detected faces.\r\n\r\nThe state of the art tables for this task are contained mainly in the consistent parts of the task : the face verification and face identification tasks.\r\n\r\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [Face Verification](https://shuftipro.com/face-verification) )</span>", "categories": ["Computer Vision"]}
{"id": "pwc:task/occluded-face-detection", "type": "task", "name": "Occluded Face Detection", "description": "", "categories": []}
