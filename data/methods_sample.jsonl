{"url": "https://paperswithcode.com/method/stochastic-depth", "name": "Stochastic Depth", "full_name": "Stochastic Depth", "description": "**Stochastic Depth** aims to shrink the depth of a network during training, while\r\nkeeping it unchanged during testing. This is achieved by randomly dropping entire [ResBlocks](https://paperswithcode.com/method/residual-block) during training and bypassing their transformations through skip connections. \r\n\r\nLet $b\\_{l} \\in$ {$0, 1$} denote a Bernoulli random variable, which indicates whether the $l$th ResBlock is active ($b\\_{l} = 1$) or inactive ($b\\_{l} = 0$). Further, let us denote the \u201csurvival\u201d probability of ResBlock $l$ as $p\\_{l} = \\text{Pr}\\left(b\\_{l} = 1\\right)$. With this definition we can bypass the $l$th ResBlock by multiplying its function $f\\_{l}$ with $b\\_{l}$ and we extend the update rule to:\r\n\r\n$$ H\\_{l} = \\text{ReLU}\\left(b\\_{l}f\\_{l}\\left(H\\_{l-1}\\right) + \\text{id}\\left(H\\_{l-1}\\right)\\right) $$\r\n\r\nIf $b\\_{l} = 1$, this reduces to the original [ResNet](https://paperswithcode.com/method/resnet) update and this ResBlock remains unchanged. If $b\\_{l} = 0$, the ResBlock reduces to the identity function, $H\\_{l} = \\text{id}\\left((H\\_{l}\u22121\\right)$.", "paper": {"title": "Deep Networks with Stochastic Depth", "url": "https://paperswithcode.com/paper/deep-networks-with-stochastic-depth"}, "introduced_year": 2000, "source_url": "http://arxiv.org/abs/1603.09382v3", "source_title": "Deep Networks with Stochastic Depth", "code_snippet_url": "https://github.com/osmr/imgclsmob/blob/69d63f99a4929d61a98e0975a4ab45b554835b9e/gluon/gluoncv2/models/resdropresnet_cifar.py#L17", "num_papers": 188, "id": "pwc:method/stochastic-depth", "type": "method"}
{"url": "https://paperswithcode.com/method/polyconv", "name": "PolyConv", "full_name": "Polynomial Convolution", "description": "PolyConv learns continuous distributions as the convolutional filters to share the weights across different vertices of graphs or points of point clouds.", "paper": {"title": "PolyNet: Polynomial Neural Network for 3D Shape Recognition with PolyShape Representation", "url": "https://paperswithcode.com/paper/polynet-polynomial-neural-network-for-3d"}, "introduced_year": 2000, "source_url": "https://arxiv.org/abs/2110.07882v1", "source_title": "PolyNet: Polynomial Neural Network for 3D Shape Recognition with PolyShape Representation", "code_snippet_url": "https://github.com/myavartanoo/PolyNet_PyTorch/blob/main/src/model/polyconv.py", "num_papers": 1, "id": "pwc:method/polyconv", "type": "method"}
{"url": "https://paperswithcode.com/method/base", "name": "BASE", "full_name": "Balanced Selection", "description": "", "paper": {"title": "Active Learning at the ImageNet Scale", "url": "https://paperswithcode.com/paper/active-learning-at-the-imagenet-scale"}, "introduced_year": 2000, "source_url": "https://arxiv.org/abs/2111.12880v1", "source_title": "Active Learning at the ImageNet Scale", "code_snippet_url": null, "num_papers": 1233, "id": "pwc:method/base", "type": "method"}
{"url": "https://paperswithcode.com/method/genet", "name": "GENet", "full_name": "GPU-Efficient Network", "description": "**GENets**, or **GPU-Efficient Networks**, are a family of efficient models found through [neural architecture search](https://paperswithcode.com/methods/category/neural-architecture-search). The search occurs over several types of convolutional block, which include [depth-wise convolutions](https://paperswithcode.com/method/depthwise-convolution), [batch normalization](https://paperswithcode.com/method/batch-normalization), [ReLU](https://paperswithcode.com/method/relu), and an [inverted bottleneck](https://paperswithcode.com/method/inverted-residual-block) structure.", "paper": {"title": "Neural Architecture Design for GPU-Efficient Networks", "url": "https://paperswithcode.com/paper/neural-architecture-design-for-gpu-efficient"}, "introduced_year": 2000, "source_url": "https://arxiv.org/abs/2006.14090v4", "source_title": "Neural Architecture Design for GPU-Efficient Networks", "code_snippet_url": "", "num_papers": 2, "id": "pwc:method/genet", "type": "method"}
{"url": "https://paperswithcode.com/method/eligibility-trace", "name": "Eligibility Trace", "full_name": "Eligibility Trace", "description": "An **Eligibility Trace** is a memory vector $\\textbf{z}\\_{t} \\in \\mathbb{R}^{d}$ that parallels the long-term weight vector $\\textbf{w}\\_{t} \\in \\mathbb{R}^{d}$. The idea is that when a component of $\\textbf{w}\\_{t}$ participates in producing an estimated value, the corresponding component of $\\textbf{z}\\_{t}$ is bumped up and then begins to fade away. Learning will then occur in that component of $\\textbf{w}\\_{t}$ if a nonzero TD error occurs before the trade falls back to zero. The trace-decay parameter $\\lambda \\in \\left[0, 1\\right]$ determines the rate at which the trace falls.\r\n\r\nIntuitively, they tackle the credit assignment problem by capturing both a frequency heuristic - states that are visited more often deserve more credit - and a recency heuristic - states that are visited more recently deserve more credit.\r\n\r\n$$E\\_{0}\\left(s\\right) = 0 $$\r\n$$E\\_{t}\\left(s\\right) = \\gamma\\lambda{E}\\_{t-1}\\left(s\\right) + \\textbf{1}\\left(S\\_{t} = s\\right) $$\r\n\r\nSource: Sutton and Barto, Reinforcement Learning, 2nd Edition", "paper": null, "introduced_year": 2000, "source_url": null, "source_title": null, "code_snippet_url": null, "num_papers": 10, "id": "pwc:method/eligibility-trace", "type": "method"}
{"url": "https://paperswithcode.com/method/soho", "name": "SOHO", "full_name": "SOHO", "description": "SOHO (\u201cSee Out of tHe bOx\u201d) that takes a whole image as input, and learns vision-language representation in an end-to-end manner. SOHO does not require bounding box annotations which enables inference 10 times faster than region-based approaches. Text embeddings are used to extract textual embedding features. A trainable CNN is used to extract visual representations. SOHO learns to extract comprehensive yet compact image features through a visual dictionary (VD) that facilitates cross-modal understanding. VD is designed to represent consistent visual abstractions of similar semantics. It is updated on-the-fly and utilized in the proposed pre-training task Masked Visual Modeling (MVM).", "paper": {"title": "Seeing Out of tHe bOx: End-to-End Pre-training for Vision-Language Representation Learning", "url": "https://paperswithcode.com/paper/seeing-out-of-the-box-end-to-end-pre-training"}, "introduced_year": 2000, "source_url": "https://arxiv.org/abs/2104.03135v2", "source_title": "Seeing Out of tHe bOx: End-to-End Pre-training for Vision-Language Representation Learning", "code_snippet_url": "", "num_papers": 3, "id": "pwc:method/soho", "type": "method"}
{"url": "https://paperswithcode.com/method/octave-convolution", "name": "Octave Convolution", "full_name": "Octave Convolution", "description": "An **Octave Convolution (OctConv)** stores and process feature maps that vary spatially \u201cslower\u201d at a lower spatial resolution reducing both memory and computation cost. It takes in feature maps containing tensors of two frequencies one octave apart, and extracts information directly from the\r\nlow-frequency maps without the need of decoding it back to the high-frequency. The motivation is that in natural images, information is conveyed at different frequencies where higher frequencies are usually encoded with fine details and lower frequencies are usually encoded with global structures.", "paper": {"title": "Drop an Octave: Reducing Spatial Redundancy in Convolutional Neural Networks with Octave Convolution", "url": "https://paperswithcode.com/paper/drop-an-octave-reducing-spatial-redundancy-in"}, "introduced_year": 2000, "source_url": "https://arxiv.org/abs/1904.05049v3", "source_title": "Drop an Octave: Reducing Spatial Redundancy in Convolutional Neural Networks with Octave Convolution", "code_snippet_url": "https://github.com/lxtGH/OctaveConv_pytorch/blob/079f7da29d55c2eeed8985d33f0b2f765d7a469e/libs/nn/OctaveConv2.py#L11", "num_papers": 8, "id": "pwc:method/octave-convolution", "type": "method"}
{"url": "https://paperswithcode.com/method/invertible-nxn-convolution", "name": "Invertible NxN Convolution", "full_name": "Invertible NxN Convolution", "description": "", "paper": {"title": "CInC Flow: Characterizable Invertible 3x3 Convolution", "url": "https://paperswithcode.com/paper/cinc-flow-characterizable-invertible-3x3"}, "introduced_year": 2000, "source_url": "https://arxiv.org/abs/2107.01358v1", "source_title": "CInC Flow: Characterizable Invertible 3x3 Convolution", "code_snippet_url": null, "num_papers": 1, "id": "pwc:method/invertible-nxn-convolution", "type": "method"}
{"url": "https://paperswithcode.com/method/cp-n3-rp", "name": "CP N3", "full_name": "CP with N3 Regularizer", "description": "CP with N3 Regularizer", "paper": {"title": "Canonical Tensor Decomposition for Knowledge Base Completion", "url": "https://paperswithcode.com/paper/canonical-tensor-decomposition-for-knowledge"}, "introduced_year": 2000, "source_url": "http://arxiv.org/abs/1806.07297v1", "source_title": "Canonical Tensor Decomposition for Knowledge Base Completion", "code_snippet_url": "https://github.com/facebookresearch/kbc", "num_papers": 1, "id": "pwc:method/cp-n3-rp", "type": "method"}
{"url": "https://paperswithcode.com/method/scaled", "name": "Scaled Dot-Product Attention", "full_name": "Scaled Dot-Product Attention", "description": "**Scaled dot-product attention** is an attention mechanism where the dot products are scaled down by $\\sqrt{d_k}$. Formally we have a query $Q$, a key $K$ and a value $V$ and calculate the attention as:\r\n\r\n$$ {\\text{Attention}}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^{T}}{\\sqrt{d_k}}\\right)V $$\r\n\r\nIf we assume that $q$ and $k$ are $d_k$-dimensional vectors whose components are independent random variables with mean $0$ and variance $1$, then their dot product, $q \\cdot k = \\sum_{i=1}^{d_k} u_iv_i$, has mean $0$ and variance $d_k$.  Since we would prefer these values to have variance $1$, we divide by $\\sqrt{d_k}$.", "paper": {"title": "Attention Is All You Need", "url": "https://paperswithcode.com/paper/attention-is-all-you-need"}, "introduced_year": 2000, "source_url": "http://arxiv.org/abs/1706.03762v5", "source_title": "Attention Is All You Need", "code_snippet_url": "https://github.com/jadore801120/attention-is-all-you-need-pytorch/blob/5c0264915ab43485adc576f88971fc3d42b10445/transformer/Modules.py#L7", "num_papers": 9970, "id": "pwc:method/scaled", "type": "method"}
{"url": "https://paperswithcode.com/method/dynamic-smoothl1-loss", "name": "Dynamic SmoothL1 Loss", "full_name": "Dynamic SmoothL1 Loss", "description": "**Dynamic SmoothL1 Loss (DSL)** is a loss function in object detection where we change the shape of loss function to gradually focus on high quality samples:\r\n\r\n$$\\text{DSL}\\left(x, \\beta\\_{now}\\right) = 0.5|{x}|^{2}/\\beta\\_{now}, \\text{ if } |x| < \\beta\\_{now}\\text{,} $$ \r\n$$\\text{DSL}\\left(x, \\beta\\_{now}\\right) = |{x}| - 0.5\\beta\\_{now}\\text{, otherwise} $$ \r\n\r\nDSL will change the value of $\\beta\\_{now}$ according to the statistics of regression errors which can reflect the localization accuracy. It was introduced as part of the [Dynamic R-CNN](https://paperswithcode.com/method/dynamic-r-cnn) model.", "paper": {"title": "Dynamic R-CNN: Towards High Quality Object Detection via Dynamic Training", "url": "https://paperswithcode.com/paper/dynamic-r-cnn-towards-high-quality-object"}, "introduced_year": 2000, "source_url": "https://arxiv.org/abs/2004.06002v2", "source_title": "Dynamic R-CNN: Towards High Quality Object Detection via Dynamic Training", "code_snippet_url": "https://github.com/hkzhang95/DynamicRCNN/blob/5b6cbe552231c7dbf2cdbd139369c313f16d2a72/dynamic_rcnn/det_opr/loss.py#L10", "num_papers": 2, "id": "pwc:method/dynamic-smoothl1-loss", "type": "method"}
{"url": "https://paperswithcode.com/method/leaky-relu", "name": "Leaky ReLU", "full_name": "Leaky ReLU", "description": "**Leaky Rectified Linear Unit**, or **Leaky ReLU**, is a type of activation function based on a [ReLU](https://paperswithcode.com/method/relu), but it has a small slope for negative values instead of a flat slope. The slope coefficient is determined before training, i.e. it is not learnt during training. This type of activation function is popular in tasks where we may suffer from sparse gradients, for example training generative adversarial networks.", "paper": null, "introduced_year": 2014, "source_url": null, "source_title": null, "code_snippet_url": "https://github.com/pytorch/pytorch/blob/96aaa311c0251d24decb9dc5da4957b7c590af6f/torch/nn/modules/activation.py#L649", "num_papers": 899, "id": "pwc:method/leaky-relu", "type": "method"}
{"url": "https://paperswithcode.com/method/delight-block", "name": "DeLighT Block", "full_name": "DeLighT Block", "description": "A **DeLighT Block** is a block used in the [DeLighT](https://paperswithcode.com/method/delight) [transformer](https://paperswithcode.com/method/transformer) architecture. It uses a [DExTra](https://paperswithcode.com/method/dextra) transformation to reduce the dimensionality of the vectors entered into the attention layer, where a [single-headed attention](https://paperswithcode.com/method/single-headed-attention) module is used.  Since the DeLighT block learns wider representations of the input across different layers using DExTra, it enables the authors to replace [multi-head attention](https://paperswithcode.com/method/multi-head-attention) with single-head attention. This is then followed by a light-weight FFN which, rather than expanding the dimension (as in normal Transformers which widen to a dimension 4x the size), imposes a bottleneck and squeezes the dimensions. Again, the reason for this is that the DExTra transformation has already incorporated wider representations so we can squeeze instead at this layer.", "paper": {"title": "DeLighT: Deep and Light-weight Transformer", "url": "https://paperswithcode.com/paper/delight-very-deep-and-light-weight"}, "introduced_year": 2000, "source_url": "https://arxiv.org/abs/2008.00623v2", "source_title": "DeLighT: Deep and Light-weight Transformer", "code_snippet_url": "https://github.com/sacmehta/delight/blob/1197d8ffbcff5c3bfc3b6a040a2ae8af811278c4/fairseq/modules/delight_transformer_layer.py#L19", "num_papers": 1, "id": "pwc:method/delight-block", "type": "method"}
{"url": "https://paperswithcode.com/method/alis", "name": "ALIS", "full_name": "Aligning Latent and Image Spaces", "description": "An infinite image generator which is based on a patch-wise, periodically equivariant generator.", "paper": {"title": "Aligning Latent and Image Spaces to Connect the Unconnectable", "url": "https://paperswithcode.com/paper/aligning-latent-and-image-spaces-to-connect"}, "introduced_year": 2000, "source_url": "https://arxiv.org/abs/2104.06954v1", "source_title": "Aligning Latent and Image Spaces to Connect the Unconnectable", "code_snippet_url": null, "num_papers": 1, "id": "pwc:method/alis", "type": "method"}
{"url": "https://paperswithcode.com/method/spatial-reduction-attention", "name": "Spatial-Reduction Attention", "full_name": "Spatial-Reduction Attention", "description": "**Spatial-Reduction Attention**, or **SRA**, is a [multi-head attention](https://paperswithcode.com/method/multi-head-attention) module used in the [Pyramid Vision Transformer](https://paperswithcode.com/method/pvt) architecture which reduces the spatial scale of the key $K$ and value $V$ before the attention operation. This reduces the computational/memory overhead. Details of the SRA in the stage $i$ can be formulated as follows:\r\n\r\n$$\r\n\\text{SRA}(Q, K, V)=\\text { Concat }\\left(\\operatorname{head}\\_{0}, \\ldots \\text { head }\\_{N\\_{i}}\\right) W^{O} $$\r\n\r\n$$\\text{ head}\\_{j}=\\text { Attention }\\left(Q W\\_{j}^{Q}, \\operatorname{SR}(K) W\\_{j}^{K}, \\operatorname{SR}(V) W\\_{j}^{V}\\right)\r\n$$\r\n\r\nwhere Concat $(\\cdot)$ is the concatenation operation. $W\\_{j}^{Q} \\in \\mathbb{R}^{C\\_{i} \\times d\\_{\\text {head }}}$, $W\\_{j}^{K} \\in \\mathbb{R}^{C\\_{i} \\times d\\_{\\text {head }}}$, $W\\_{j}^{V} \\in \\mathbb{R}^{C\\_{i} \\times d\\_{\\text {head }}}$, and $W^{O} \\in \\mathbb{R}^{C\\_{i} \\times C\\_{i}}$ are linear projection parameters. $N\\_{i}$ is the head number of the attention layer in Stage $i$. Therefore, the dimension of each head (i.e. $\\left.d\\_{\\text {head }}\\right)$ is equal to $\\frac{C\\_{i}}{N\\_{i}} . \\text{SR}(\\cdot)$ is the operation for reducing the spatial dimension of the input sequence ($K$ or $V$ ), which is written as:\r\n\r\n$$\r\n\\text{SR}(\\mathbf{x})=\\text{Norm}\\left(\\operatorname{Reshape}\\left(\\mathbf{x}, R\\_{i}\\right) W^{S}\\right)\r\n$$\r\n\r\nHere, $\\mathbf{x} \\in \\mathbb{R}^{\\left(H\\_{i} W\\_{i}\\right) \\times C\\_{i}}$ represents a input sequence, and $R\\_{i}$ denotes the reduction ratio of the attention layers in Stage $i .$ Reshape $\\left(\\mathbf{x}, R\\_{i}\\right)$ is an operation of reshaping the input sequence $\\mathbf{x}$ to a sequence of size $\\frac{H\\_{i} W\\_{i}}{R\\_{i}^{2}} \\times\\left(R\\_{i}^{2} C\\_{i}\\right)$. $W\\_{S} \\in \\mathbb{R}^{\\left(R\\_{i}^{2} C\\_{i}\\right) \\times C\\_{i}}$ is a linear projection that reduces the dimension of the input sequence to $C\\_{i}$. $\\text{Norm}(\\cdot)$ refers to layer normalization.", "paper": {"title": "Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions", "url": "https://paperswithcode.com/paper/pyramid-vision-transformer-a-versatile"}, "introduced_year": 2000, "source_url": "https://arxiv.org/abs/2102.12122v2", "source_title": "Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions", "code_snippet_url": "", "num_papers": 9, "id": "pwc:method/spatial-reduction-attention", "type": "method"}
{"url": "https://paperswithcode.com/method/low-rank-tensor-learning-paradigms", "name": "Low Rank Tensor Learning Paradigms", "full_name": "Time-homogenuous Top-K Ranking", "description": "Please enter a description about the method here", "paper": null, "introduced_year": 2000, "source_url": null, "source_title": null, "code_snippet_url": null, "num_papers": 1, "id": "pwc:method/low-rank-tensor-learning-paradigms", "type": "method"}
{"url": "https://paperswithcode.com/method/visual-parsing", "name": "Visual Parsing", "full_name": "Visual Parsing", "description": "Visual Parsing is a vision and language pretrained model that adopts self-attention for visual feature learning where each visual token is an approximate weighted mixture of all tokens. Thus, visual parsing provides the dependencies of each visual token pair.  It helps better learning of visual relation with the language and promote inter modal alignment. The model is composed of a vision Transformer that takes an image as input and outputs the visual tokens and a multimodal Transformer. \r\nIt applies a linear layer and a Layer Normalization to embed the vision tokens. It follows BERT to get word embeddings. Vision and language tokens are concatenated to form the input sequences. A multi-modal Transformer is used to fuse the vision and language modality. A metric named Inter-Modality Flow (IMF) is used to quantify the interactions between two modalities.\r\nThree pretraining tasks are adopted: Masked Language Modeling (MLM), Image-Text Matching (ITM), and Masked Feature Regression (MFR). MFR is a novel task that is included to mask visual tokens with similar or correlated semantics in this framework.", "paper": {"title": "Probing Inter-modality: Visual Parsing with Self-Attention for Vision-and-Language Pre-training", "url": "https://paperswithcode.com/paper/probing-inter-modality-visual-parsing-with-1"}, "introduced_year": 2000, "source_url": "https://openreview.net/forum?id=e0nZIFEpmYh", "source_title": "Probing Inter-modality: Visual Parsing with Self-Attention for Vision-and-Language Pre-training", "code_snippet_url": "", "num_papers": 3, "id": "pwc:method/visual-parsing", "type": "method"}
{"url": "https://paperswithcode.com/method/rendezvous", "name": "Rendezvous", "full_name": "Multi-head of Mixed Attention", "description": "Multi-heads of both self and cross attentions", "paper": {"title": "Rendezvous: Attention Mechanisms for the Recognition of Surgical Action Triplets in Endoscopic Videos", "url": "https://paperswithcode.com/paper/rendezvous-attention-mechanisms-for-the"}, "introduced_year": 2000, "source_url": "https://arxiv.org/abs/2109.03223v2", "source_title": "Rendezvous: Attention Mechanisms for the Recognition of Surgical Action Triplets in Endoscopic Videos", "code_snippet_url": null, "num_papers": 14, "id": "pwc:method/rendezvous", "type": "method"}
{"url": "https://paperswithcode.com/method/hierarchical-dnn-interpretations", "name": "Agglomerative Contextual Decomposition", "full_name": "Agglomerative Contextual Decomposition", "description": "**Agglomerative Contextual Decomposition (ACD)** is an interpretability method that produces hierarchical interpretations for a single prediction made by a neural network, by scoring interactions and building them into a tree. Given a prediction from a trained neural network, ACD produces a hierarchical clustering of the input features, along with the contribution of each cluster to the final prediction. This hierarchy is optimized to identify clusters of features that the DNN learned are predictive.", "paper": {"title": "Hierarchical interpretations for neural network predictions", "url": "https://paperswithcode.com/paper/hierarchical-interpretations-for-neural"}, "introduced_year": 2000, "source_url": "http://arxiv.org/abs/1806.05337v2", "source_title": "Hierarchical interpretations for neural network predictions", "code_snippet_url": "https://github.com/csinva/hierarchical-dnn-interpretations", "num_papers": 1, "id": "pwc:method/hierarchical-dnn-interpretations", "type": "method"}
{"url": "https://paperswithcode.com/method/pgnet", "name": "PGNet", "full_name": "Point Gathering Network", "description": "**PGNet** is a point-gathering network for reading arbitrarily-shaped text in real-time. It is a single-shot text spotter, where the pixel-level character classification map is learned with proposed PG-CTC loss avoiding the usage of character-level annotations. With a PG-CTC decoder, we gather high-level character classification vectors from two-dimensional space and decode them into text symbols without NMS and RoI operations involved, which guarantees high efficiency. Additionally, reasoning the relations between each character and its neighbors, a graph refinement module (GRM) is proposed to optimize the coarse recognition and improve the end-to-end performance.", "paper": {"title": "PGNet: Real-time Arbitrarily-Shaped Text Spotting with Point Gathering Network", "url": "https://paperswithcode.com/paper/pgnet-real-time-arbitrarily-shaped-text"}, "introduced_year": 2000, "source_url": "https://arxiv.org/abs/2104.05458v1", "source_title": "PGNet: Real-time Arbitrarily-Shaped Text Spotting with Point Gathering Network", "code_snippet_url": null, "num_papers": 2, "id": "pwc:method/pgnet", "type": "method"}
{"url": "https://paperswithcode.com/method/gps", "name": "GPS", "full_name": "Greedy Policy Search", "description": "**Greedy Policy Search** (GPS) is a simple algorithm that learns a policy for test-time data augmentation based on the predictive performance on a validation set. GPS starts with an empty policy and builds it in an iterative fashion. Each step selects a sub-policy that provides the largest improvement in calibrated log-likelihood of ensemble predictions and adds it to the current policy.", "paper": {"title": "Greedy Policy Search: A Simple Baseline for Learnable Test-Time Augmentation", "url": "https://paperswithcode.com/paper/greedy-policy-search-a-simple-baseline-for"}, "introduced_year": 2000, "source_url": "https://arxiv.org/abs/2002.09103v2", "source_title": "Greedy Policy Search: A Simple Baseline for Learnable Test-Time Augmentation", "code_snippet_url": null, "num_papers": 341, "id": "pwc:method/gps", "type": "method"}
{"url": "https://paperswithcode.com/method/dblock", "name": "DBlock", "full_name": "DBlock", "description": "**DBlock** is a residual based block used in the discriminator of the [GAN-TTS](https://paperswithcode.com/method/gan-tts) architecture. They are similar to the [GBlocks](https://paperswithcode.com/method/gblock) used in the generator, but without batch normalisation.", "paper": {"title": "High Fidelity Speech Synthesis with Adversarial Networks", "url": "https://paperswithcode.com/paper/high-fidelity-speech-synthesis-with-1"}, "introduced_year": 2000, "source_url": "https://arxiv.org/abs/1909.11646v2", "source_title": "High Fidelity Speech Synthesis with Adversarial Networks", "code_snippet_url": "https://github.com/yanggeng1995/GAN-TTS/blob/4675fa108c4c52f190d27a32a8d9e9ce1c68d7a1/models/discriminator.py#L101", "num_papers": 2, "id": "pwc:method/dblock", "type": "method"}
{"url": "https://paperswithcode.com/method/pixelcnn", "name": "PixelCNN", "full_name": "PixelCNN", "description": "A **PixelCNN** is a generative model that uses autoregressive connections to model images pixel by pixel, decomposing the joint image distribution as a product of conditionals. PixelCNNs are much faster to train than [PixelRNNs](https://paperswithcode.com/method/pixelrnn) because convolutions are inherently easier to parallelize; given the vast number of pixels present in large image datasets this is an important advantage.", "paper": {"title": "Pixel Recurrent Neural Networks", "url": "https://paperswithcode.com/paper/pixel-recurrent-neural-networks"}, "introduced_year": 2000, "source_url": "http://arxiv.org/abs/1601.06759v3", "source_title": "Pixel Recurrent Neural Networks", "code_snippet_url": "https://github.com/openai/pixel-cnn", "num_papers": 32, "id": "pwc:method/pixelcnn", "type": "method"}
{"url": "https://paperswithcode.com/method/inception-a", "name": "Inception-A", "full_name": "Inception-A", "description": "**Inception-A** is an image model block used in the [Inception-v4](https://paperswithcode.com/method/inception-v4) architecture.", "paper": {"title": "Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning", "url": "https://paperswithcode.com/paper/inception-v4-inception-resnet-and-the-impact"}, "introduced_year": 2000, "source_url": "http://arxiv.org/abs/1602.07261v2", "source_title": "Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning", "code_snippet_url": "https://github.com/kentsommer/keras-inceptionV4/blob/ef1db6f09b6511779c05fab47d374741bc89b5ee/inception_v4.py#L71", "num_papers": 11, "id": "pwc:method/inception-a", "type": "method"}
{"url": "https://paperswithcode.com/method/scale-aggregation-block", "name": "Scale Aggregation Block", "full_name": "Scale Aggregation Block", "description": "A **Scale Aggregation Block** concatenates feature maps at a wide range of scales. Feature maps for each scale are generated by a stack of downsampling, [convolution](https://paperswithcode.com/method/convolution) and upsampling operations. The proposed scale aggregation block is a standard computational module which readily replaces any given transformation $\\mathbf{Y}=\\mathbf{T}(\\mathbf{X})$, where $\\mathbf{X}\\in \\mathbb{R}^{H\\times W\\times C}$, $\\mathbf{Y}\\in \\mathbb{R}^{H\\times W\\times C_o}$ with $C$ and $C_o$ being the input and output channel number respectively. $\\mathbf{T}$ is any operator such as a convolution layer or a series of convolution layers. Assume we have $L$ scales. Each scale $l$ is generated by sequentially conducting a downsampling $\\mathbf{D}_l$, a transformation $\\mathbf{T}_l$ and an unsampling operator $\\mathbf{U}_l$:\r\n\r\n$$\r\n\\mathbf{X}^{'}_l=\\mathbf{D}_l(\\mathbf{X}),\r\n\\label{eq:eq_d}\r\n$$\r\n\r\n$$\r\n\\mathbf{Y}^{'}_l=\\mathbf{T}_l(\\mathbf{X}^{'}_l),\r\n\\label{eq:eq_tl}\r\n$$\r\n\r\n$$\r\n\\mathbf{Y}_l=\\mathbf{U}_l(\\mathbf{Y}^{'}_l),\r\n\\label{eq:eq_u}\r\n$$\r\n\r\nwhere $\\mathbf{X}^{'}_l\\in \\mathbb{R}^{H_l\\times W_l\\times C}$,\r\n$\\mathbf{Y}^{'}_l\\in \\mathbb{R}^{H_l\\times W_l\\times C_l}$, and\r\n$\\mathbf{Y}_l\\in \\mathbb{R}^{H\\times W\\times C_l}$.\r\nNotably, $\\mathbf{T}_l$ has the similar structure as $\\mathbf{T}$.\r\nWe can concatenate all $L$ scales together, getting\r\n\r\n$$\r\n\\mathbf{Y}^{'}=\\Vert^L_1\\mathbf{U}_l(\\mathbf{T}_l(\\mathbf{D}_l(\\mathbf{X}))),\r\n\\label{eq:eq_all}\r\n$$\r\n\r\nwhere $\\Vert$ indicates concatenating feature maps along the channel dimension, and $\\mathbf{Y}^{'} \\in \\mathbb{R}^{H\\times W\\times \\sum^L_1 C_l}$ is the final output feature maps of the scale aggregation block.\r\n\r\nIn the reference implementation, the downsampling $\\mathbf{D}_l$ with factor $s$ is implemented by a max pool layer with $s\\times s$ kernel size and  $s$ stride. The upsampling $\\mathbf{U}_l$ is implemented by resizing with the nearest neighbor  interpolation.", "paper": {"title": "Data-Driven Neuron Allocation for Scale Aggregation Networks", "url": "https://paperswithcode.com/paper/190409460"}, "introduced_year": 2000, "source_url": "http://arxiv.org/abs/1904.09460v1", "source_title": "Data-Driven Neuron Allocation for Scale Aggregation Networks", "code_snippet_url": "https://github.com/Eli-YiLi/ScaleNet/blob/2c27b4207691dbe72f7e19fd88bfccc5ce5b3080/pytorch/scalenet.py#L8", "num_papers": 4, "id": "pwc:method/scale-aggregation-block", "type": "method"}
{"url": "https://paperswithcode.com/method/4d-a", "name": "4D A*", "full_name": "Four-dimensional A-star", "description": "The aim of 4D A* is to find the shortest path between two four-dimensional (4D) nodes of a 4D search space - a starting node and a target node - as long as there is a path. It achieves both optimality and completeness. The former is because the path is shortest possible, and the latter because if the solution exists the algorithm is guaranteed to find it.", "paper": {"title": "Artificial Intelligence Control in 4D Cylindrical Space for Industrial Robotic Applications", "url": "https://paperswithcode.com/paper/artificial-intelligence-control-in-4d"}, "introduced_year": 2000, "source_url": "https://doi.org/10.1109/ACCESS.2020.3026193", "source_title": "Artificial Intelligence Control in 4D Cylindrical Space for Industrial Robotic Applications", "code_snippet_url": "", "num_papers": 1, "id": "pwc:method/4d-a", "type": "method"}
{"url": "https://paperswithcode.com/method/adadelta", "name": "AdaDelta", "full_name": "AdaDelta", "description": "**AdaDelta** is a stochastic optimization technique that allows for per-dimension learning rate method for [SGD](https://paperswithcode.com/method/sgd). It is an extension of [Adagrad](https://paperswithcode.com/method/adagrad) that seeks to reduce its aggressive, monotonically decreasing learning rate. Instead of accumulating all past squared gradients, Adadelta restricts the window of accumulated past gradients to a fixed size $w$.\r\n\r\nInstead of inefficiently storing $w$ previous squared gradients, the sum of gradients is recursively defined as a decaying average of all past squared gradients. The running average $E\\left[g^{2}\\right]\\_{t}$ at time step $t$ then depends only on the previous average and current gradient:\r\n\r\n$$E\\left[g^{2}\\right]\\_{t} = \\gamma{E}\\left[g^{2}\\right]\\_{t-1} + \\left(1-\\gamma\\right)g^{2}\\_{t}$$\r\n\r\nUsually $\\gamma$ is set to around $0.9$. Rewriting SGD updates in terms of the parameter update vector:\r\n\r\n$$ \\Delta\\theta_{t} = -\\eta\\cdot{g\\_{t, i}}$$\r\n$$\\theta\\_{t+1}  = \\theta\\_{t} + \\Delta\\theta_{t}$$\r\n\r\nAdaDelta takes the form:\r\n\r\n$$ \\Delta\\theta_{t} = -\\frac{\\eta}{\\sqrt{E\\left[g^{2}\\right]\\_{t} + \\epsilon}}g_{t} $$\r\n\r\nThe main advantage of AdaDelta is that we do not need to set a default learning rate.", "paper": {"title": "ADADELTA: An Adaptive Learning Rate Method", "url": "https://paperswithcode.com/paper/adadelta-an-adaptive-learning-rate-method"}, "introduced_year": 2000, "source_url": "http://arxiv.org/abs/1212.5701v1", "source_title": "ADADELTA: An Adaptive Learning Rate Method", "code_snippet_url": "https://github.com/pytorch/pytorch/blob/b7bda236d18815052378c88081f64935427d7716/torch/optim/adadelta.py#L6", "num_papers": 12, "id": "pwc:method/adadelta", "type": "method"}
{"url": "https://paperswithcode.com/method/gaussian-process", "name": "Gaussian Process", "full_name": "Gaussian Process", "description": "**Gaussian Processes** are non-parametric models for approximating functions. They rely upon a measure of similarity between points (the kernel function) to predict the value for an unseen point from training data. The models are fully probabilistic so uncertainty bounds are baked in with the model.\r\n\r\nImage Source: Gaussian Processes for Machine Learning, C. E. Rasmussen & C. K. I. Williams", "paper": null, "introduced_year": 2000, "source_url": null, "source_title": null, "code_snippet_url": null, "num_papers": 1664, "id": "pwc:method/gaussian-process", "type": "method"}
{"url": "https://paperswithcode.com/method/dall-e-2", "name": "DALL\u00b7E 2", "full_name": "DALL\u00b7E 2", "description": "**DALL\u00b7E 2** is a generative text-to-image model made up of two main components: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding.", "paper": {"title": "Hierarchical Text-Conditional Image Generation with CLIP Latents", "url": "https://paperswithcode.com/paper/hierarchical-text-conditional-image"}, "introduced_year": 2000, "source_url": "https://arxiv.org/abs/2204.06125v1", "source_title": "Hierarchical Text-Conditional Image Generation with CLIP Latents", "code_snippet_url": "", "num_papers": 1, "id": "pwc:method/dall-e-2", "type": "method"}
{"url": "https://paperswithcode.com/method/advprop", "name": "AdvProp", "full_name": "AdvProp", "description": "**AdvProp** is an adversarial training scheme which treats adversarial examples as additional examples, to prevent overfitting. Key to the method is the usage of a separate auxiliary batch norm for adversarial examples, as they have different underlying distributions to normal examples.", "paper": {"title": "Adversarial Examples Improve Image Recognition", "url": "https://paperswithcode.com/paper/adversarial-examples-improve-image"}, "introduced_year": 2000, "source_url": "https://arxiv.org/abs/1911.09665v2", "source_title": "Adversarial Examples Improve Image Recognition", "code_snippet_url": null, "num_papers": 5, "id": "pwc:method/advprop", "type": "method"}
{"url": "https://paperswithcode.com/method/zca-whitening", "name": "ZCA Whitening", "full_name": "ZCA Whitening", "description": "**ZCA Whitening** is an image preprocessing method that leads to a transformation of data such that the covariance matrix $\\Sigma$ is the identity matrix, leading to decorrelated features.\r\n\r\nImage Source: [Alex Krizhevsky](http://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf)", "paper": null, "introduced_year": 2000, "source_url": null, "source_title": null, "code_snippet_url": null, "num_papers": 6, "id": "pwc:method/zca-whitening", "type": "method"}
{"url": "https://paperswithcode.com/method/ga", "name": "GA", "full_name": "Genetic Algorithms", "description": "Genetic Algorithms are search algorithms that mimic Darwinian biological evolution in order to select and propagate better solutions.", "paper": {"title": "Genetic Algorithms and the Traveling Salesman Problem a historical Review", "url": "https://paperswithcode.com/paper/genetic-algorithms-and-the-traveling-salesman"}, "introduced_year": 2000, "source_url": "http://arxiv.org/abs/1901.05737v1", "source_title": "Genetic Algorithms and the Traveling Salesman Problem a historical Review", "code_snippet_url": "", "num_papers": 141, "id": "pwc:method/ga", "type": "method"}
{"url": "https://paperswithcode.com/method/a2c", "name": "A2C", "full_name": "A2C", "description": "**A2C**, or **Advantage Actor Critic**, is a synchronous version of the [A3C](https://paperswithcode.com/method/a3c) policy gradient method. As an alternative to the asynchronous implementation of A3C, A2C is a synchronous, deterministic implementation that waits for each actor to finish its segment of experience before updating, averaging over all of the actors. This more effectively uses GPUs due to larger batch sizes.\r\n\r\nImage Credit: [OpenAI Baselines](https://openai.com/blog/baselines-acktr-a2c/)", "paper": {"title": "Asynchronous Methods for Deep Reinforcement Learning", "url": "https://paperswithcode.com/paper/asynchronous-methods-for-deep-reinforcement"}, "introduced_year": 2000, "source_url": "http://arxiv.org/abs/1602.01783v2", "source_title": "Asynchronous Methods for Deep Reinforcement Learning", "code_snippet_url": null, "num_papers": 55, "id": "pwc:method/a2c", "type": "method"}
{"url": "https://paperswithcode.com/method/condconv", "name": "CondConv", "full_name": "CondConv", "description": "**CondConv**, or **Conditionally Parameterized Convolutions**, are a type of [convolution](https://paperswithcode.com/method/convolution) which learn specialized convolutional kernels for each example. In particular, we parameterize the convolutional kernels in a CondConv layer as a linear combination of $n$ experts $(\\alpha_1 W_1 + \\ldots + \\alpha_n W_n) * x$, where $\\alpha_1, \\ldots, \\alpha_n$ are functions of the input learned through gradient descent. To efficiently increase the capacity of a CondConv layer, developers can increase the number of experts. This can be more computationally efficient than increasing the size of the convolutional kernel itself, because the convolutional kernel is applied at many different positions within the input, while the experts are combined only once per input.", "paper": {"title": "CondConv: Conditionally Parameterized Convolutions for Efficient Inference", "url": "https://paperswithcode.com/paper/soft-conditional-computation"}, "introduced_year": 2000, "source_url": "https://arxiv.org/abs/1904.04971v3", "source_title": "CondConv: Conditionally Parameterized Convolutions for Efficient Inference", "code_snippet_url": "https://github.com/tensorflow/tpu/blob/bb03c18d7c2501b4df070c9936a46a9dcd6ad1cb/models/official/efficientnet/condconv/condconv_layers.py#L63", "num_papers": 4, "id": "pwc:method/condconv", "type": "method"}
{"url": "https://paperswithcode.com/method/xlnet", "name": "XLNet", "full_name": "XLNet", "description": "**XLNet** is an autoregressive [Transformer](https://paperswithcode.com/method/transformer) that leverages the best of both autoregressive language modeling and autoencoding while attempting to avoid their limitations. Instead of using a fixed forward or backward factorization order as in conventional autoregressive models, XLNet maximizes the expected log likelihood of a sequence w.r.t. all possible permutations of the factorization order. Thanks to the permutation operation, the context for each position can consist of tokens from both left and right. In expectation, each position learns to utilize contextual information from all positions, i.e., capturing bidirectional context.\r\n\r\nAdditionally, inspired by the latest advancements in autogressive language modeling, XLNet integrates the segment recurrence mechanism and relative encoding scheme of [Transformer-XL](https://paperswithcode.com/method/transformer-xl) into pretraining, which empirically improves the performance especially for tasks involving a longer text sequence.", "paper": {"title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding", "url": "https://paperswithcode.com/paper/xlnet-generalized-autoregressive-pretraining"}, "introduced_year": 2000, "source_url": "https://arxiv.org/abs/1906.08237v2", "source_title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding", "code_snippet_url": null, "num_papers": 139, "id": "pwc:method/xlnet", "type": "method"}
{"url": "https://paperswithcode.com/method/gcnet", "name": "GCNet", "full_name": "GCNet", "description": "A **Global Context Network**, or **GCNet**, utilises global context blocks to model long-range dependencies in images. It is based on the [Non-Local Network](https://paperswithcode.com/method/non-local-block), but it modifies the architecture so less computation is required. Global context blocks are applied to multiple layers in a backbone network to construct the GCNet.", "paper": {"title": "GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond", "url": "https://paperswithcode.com/paper/gcnet-non-local-networks-meet-squeeze"}, "introduced_year": 2000, "source_url": "http://arxiv.org/abs/1904.11492v1", "source_title": "GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond", "code_snippet_url": "https://github.com/xvjiarui/GCNet", "num_papers": 9, "id": "pwc:method/gcnet", "type": "method"}
{"url": "https://paperswithcode.com/method/dilated-sliding-window-attention", "name": "Dilated Sliding Window Attention", "full_name": "Dilated Sliding Window Attention", "description": "**Dilated Sliding Window Attention** is an attention pattern for attention-based models. It was proposed as part of the [Longformer](https://paperswithcode.com/method/longformer) architecture. It is motivated by the fact that non-sparse attention in the original [Transformer](https://paperswithcode.com/method/transformer) formulation has a [self-attention component](https://paperswithcode.com/method/scaled) with $O\\left(n^{2}\\right)$ time and memory complexity where $n$ is the input sequence length and thus, is not efficient to scale to long inputs. \r\n\r\nCompared to a [Sliding Window Attention](https://paperswithcode.com/method/sliding-window-attention) pattern, we can further increase the receptive field without increasing computation by making the sliding window \"dilated\". This is analogous to [dilated CNNs](https://paperswithcode.com/method/dilated-convolution) where the window has gaps of size dilation $d$. Assuming a fixed $d$ and $w$ for all layers, the receptive field is $l \u00d7 d \u00d7 w$, which can reach tens of thousands of tokens even for small values of $d$.", "paper": {"title": "Longformer: The Long-Document Transformer", "url": "https://paperswithcode.com/paper/longformer-the-long-document-transformer"}, "introduced_year": 2000, "source_url": "https://arxiv.org/abs/2004.05150v2", "source_title": "Longformer: The Long-Document Transformer", "code_snippet_url": "https://github.com/allenai/longformer/blob/74523f7f2897b3a5ffcdb537f67a03f83aa1affb/longformer/sliding_chunks.py#L40", "num_papers": 45, "id": "pwc:method/dilated-sliding-window-attention", "type": "method"}
{"url": "https://paperswithcode.com/method/masked-convolution", "name": "Masked Convolution", "full_name": "Masked Convolution", "description": "A **Masked Convolution** is a type of [convolution](https://paperswithcode.com/method/convolution) which masks certain pixels so that the model can only predict based on pixels already seen. This type of convolution was introduced with [PixelRNN](https://paperswithcode.com/method/pixelrnn) generative models, where an image is generated pixel by pixel, to ensure that the model was conditional only on pixels already visited.", "paper": {"title": "Pixel Recurrent Neural Networks", "url": "https://paperswithcode.com/paper/pixel-recurrent-neural-networks"}, "introduced_year": 2000, "source_url": "http://arxiv.org/abs/1601.06759v3", "source_title": "Pixel Recurrent Neural Networks", "code_snippet_url": "https://github.com/jzbontar/pixelcnn-pytorch/blob/14c9414602e0694692c77a5e0d87188adcded118/main.py#L17", "num_papers": 15, "id": "pwc:method/masked-convolution", "type": "method"}
{"url": "https://paperswithcode.com/method/channel-attention", "name": "Channel attention", "full_name": "squeeze-and-excitation networks", "description": "SENet pioneered channel attention. The core of SENet is a squeeze-and-excitation (SE) block which is used to collect global information, capture channel-wise relationships and improve representation ability.\r\nSE blocks are divided into two parts, a squeeze module and an excitation module. Global spatial information is collected in the squeeze module by global average pooling. The excitation module captures channel-wise relationships and outputs an attention vector by using fully-connected layers and non-linear layers (ReLU and sigmoid). Then, each channel of the input feature is scaled by multiplying the corresponding element in the attention vector. Overall, a squeeze-and-excitation block $F_\\text{se}$ (with parameter $\\theta$) which takes $X$ as input and outputs $Y$ can be formulated \r\nas:\r\n\\begin{align}\r\n    s = F_\\text{se}(X, \\theta) & = \\sigma (W_{2} \\delta (W_{1}\\text{GAP}(X)))\r\n\\end{align}\r\n\\begin{align}\r\n    Y = sX\r\n\\end{align}", "paper": {"title": "Squeeze-and-Excitation Networks", "url": "https://paperswithcode.com/paper/squeeze-and-excitation-networks"}, "introduced_year": 2000, "source_url": "https://arxiv.org/abs/1709.01507v4", "source_title": "Squeeze-and-Excitation Networks", "code_snippet_url": "", "num_papers": 65, "id": "pwc:method/channel-attention", "type": "method"}
{"url": "https://paperswithcode.com/method/centermask", "name": "CenterMask", "full_name": "CenterMask", "description": "**CenterMask** is an anchor-free instance segmentation method that adds a novel [spatial attention-guided mask](https://paperswithcode.com/method/spatial-attention-guided-mask) (SAG-Mask) branch to anchor-free one stage object detector ([FCOS](https://paperswithcode.com/method/fcos)) in the same vein with [Mask R-CNN](https://paperswithcode.com/method/mask-r-cnn). Plugged into the FCOS object detector, the SAG-Mask branch predicts a segmentation mask on each detected box with the spatial attention map that helps to focus on informative pixels and suppress noise.", "paper": {"title": "CenterMask : Real-Time Anchor-Free Instance Segmentation", "url": "https://paperswithcode.com/paper/centermask-real-time-anchor-free-instance-1"}, "introduced_year": 2000, "source_url": "https://arxiv.org/abs/1911.06667v6", "source_title": "CenterMask : Real-Time Anchor-Free Instance Segmentation", "code_snippet_url": "https://github.com/youngwanLEE/CenterMask", "num_papers": 4, "id": "pwc:method/centermask", "type": "method"}
{"url": "https://paperswithcode.com/method/cida", "name": "CIDA", "full_name": "Continuously Indexed Domain Adaptation", "description": "**Continuously Indexed Domain Adaptation** combines traditional adversarial adaptation with a novel discriminator that models the encoding-conditioned domain index distribution.\r\n\r\nImage Source: [Wang et al.](https://arxiv.org/pdf/2007.01807v2.pdf)", "paper": {"title": "Continuously Indexed Domain Adaptation", "url": "https://paperswithcode.com/paper/continuously-indexed-domain-adaptation"}, "introduced_year": 2000, "source_url": "https://arxiv.org/abs/2007.01807v2", "source_title": "Continuously Indexed Domain Adaptation", "code_snippet_url": "", "num_papers": 3, "id": "pwc:method/cida", "type": "method"}
{"url": "https://paperswithcode.com/method/spectral-dropout", "name": "Spectral Dropout", "full_name": "Spectral Dropout", "description": "Please enter a description about the method here", "paper": {"title": "Regularization of Deep Neural Networks with Spectral Dropout", "url": "https://paperswithcode.com/paper/regularization-of-deep-neural-networks-with"}, "introduced_year": 2000, "source_url": "http://arxiv.org/abs/1711.08591v1", "source_title": "Regularization of Deep Neural Networks with Spectral Dropout", "code_snippet_url": null, "num_papers": 1, "id": "pwc:method/spectral-dropout", "type": "method"}
{"url": "https://paperswithcode.com/method/ebm", "name": "EBM", "full_name": "energy-based model", "description": "", "paper": {"title": "A Theory of Generative ConvNet", "url": "https://paperswithcode.com/paper/a-theory-of-generative-convnet"}, "introduced_year": 2000, "source_url": "http://arxiv.org/abs/1602.03264v3", "source_title": "A Theory of Generative ConvNet", "code_snippet_url": null, "num_papers": 69, "id": "pwc:method/ebm", "type": "method"}
{"url": "https://paperswithcode.com/method/rescal-rp", "name": "RESCAL RP", "full_name": "RESCAL", "description": "", "paper": {"title": "A Three-Way Model for Collective Learning on Multi-Relational Data", "url": "https://paperswithcode.com/paper/a-three-way-model-for-collective-learning-on"}, "introduced_year": 2000, "source_url": "https://icml.cc/2011/papers/438_icmlpaper.pdf", "source_title": "A Three-Way Model for Collective Learning on Multi-Relational Data", "code_snippet_url": "", "num_papers": 1, "id": "pwc:method/rescal-rp", "type": "method"}
{"url": "https://paperswithcode.com/method/tridentnet-block", "name": "TridentNet Block", "full_name": "TridentNet Block", "description": "A **TridentNet Block** is a feature extractor used in object detection models. Instead of feeding in multi-scale inputs like the image pyramid, in a [TridentNet](https://paperswithcode.com/method/tridentnet) block we adapt the backbone network for different scales. These blocks create multiple scale-specific feature maps. With the help of dilated convolutions, different branches of trident blocks have the same network structure and share the\r\nsame parameters yet have different receptive fields. Furthermore, to avoid training objects with extreme scales, a scale-aware training scheme is employed to make each branch specific to a given scale range matching its receptive field. Weight sharing is used to prevent overfitting.", "paper": {"title": "Scale-Aware Trident Networks for Object Detection", "url": "https://paperswithcode.com/paper/scale-aware-trident-networks-for-object"}, "introduced_year": 2000, "source_url": "https://arxiv.org/abs/1901.01892v2", "source_title": "Scale-Aware Trident Networks for Object Detection", "code_snippet_url": "https://github.com/facebookresearch/detectron2/blob/d250fcc1b66d5a3686c15144480441b7abe31dec/projects/TridentNet/tridentnet/trident_backbone.py#L15", "num_papers": 4, "id": "pwc:method/tridentnet-block", "type": "method"}
{"url": "https://paperswithcode.com/method/cascade-corner-pooling", "name": "Cascade Corner Pooling", "full_name": "Cascade Corner Pooling", "description": "**Cascade Corner Pooling** is a pooling layer for object detection that builds upon the [corner pooling](https://paperswithcode.com/method/corner-pooling) operation. Corners are often outside the objects, which lacks local appearance features. [CornerNet](https://paperswithcode.com/method/cornernet) uses corner pooling to address this issue, where we find the maximum values on the boundary directions so as to determine corners. However, it makes corners sensitive to the edges. To address this problem, we need to let corners see the visual patterns of objects. Cascade corner pooling first looks along a boundary to find a boundary maximum value, then looks inside along the location of the boundary maximum value to find an internal maximum value, and finally, add the two maximum values together. By doing this, the corners obtain both the the boundary information and the visual patterns of objects.", "paper": {"title": "CenterNet: Keypoint Triplets for Object Detection", "url": "https://paperswithcode.com/paper/centernet-object-detection-with-keypoint"}, "introduced_year": 2000, "source_url": "http://arxiv.org/abs/1904.08189v3", "source_title": "CenterNet: Keypoint Triplets for Object Detection", "code_snippet_url": "https://github.com/Duankaiwen/CenterNet", "num_papers": 30, "id": "pwc:method/cascade-corner-pooling", "type": "method"}
{"url": "https://paperswithcode.com/method/dtw", "name": "DTW", "full_name": "Dynamic Time Warping", "description": "Dynamic Time Warping (DTW) [1] is one of well-known distance measures between a pairwise of time series. The main idea of DTW is to compute the distance from the matching of similar elements between time series. It uses the dynamic programming technique to find the optimal temporal matching between elements of two time series.\r\n\r\nFor instance, similarities in walking could be detected using DTW, even if one person was walking faster than the other, or if there were accelerations and decelerations during the course of an observation. DTW has been applied to temporal sequences of video, audio, and graphics data \u2014 indeed, any data that can be turned into a linear sequence can be analyzed with DTW. A well known application has been automatic speech recognition, to cope with different speaking speeds. Other applications include speaker recognition and online signature recognition. It can also be used in partial shape matching application.\r\n\r\nIn general, DTW is a method that calculates an optimal match between two given sequences (e.g. time series) with certain restriction and rules:\r\n\r\n1. Every index from the first sequence must be matched with one or more indices from the other sequence, and vice versa\r\n2. The first index from the first sequence must be matched with the first index from the other sequence (but it does not have to be its only match)\r\n3. The last index from the first sequence must be matched with the last index from the other sequence (but it does not have to be its only match)\r\n4. The mapping of the indices from the first sequence to indices from the other sequence must be monotonically increasing, and vice versa, i.e. if j>i  are indices from the first sequence, then there must not be two indices l>k in the other sequence, such that index i is matched with index l and index j is matched with index k, and vice versa.\r\n\r\n[1] Sakoe, Hiroaki, and Seibi Chiba. \"Dynamic programming algorithm optimization for spoken word recognition.\" IEEE transactions on acoustics, speech, and signal processing 26, no. 1 (1978): 43-49.", "paper": null, "introduced_year": 2000, "source_url": null, "source_title": null, "code_snippet_url": "https://dynamictimewarping.github.io/", "num_papers": 116, "id": "pwc:method/dtw", "type": "method"}
{"url": "https://paperswithcode.com/method/pp-ocr", "name": "PP-OCR", "full_name": "PP-OCR", "description": "**PP-OCR** is an OCR system that consists of three parts, text detection, detected boxes rectification and text recognition. The purpose of text detection is to locate the text area in the image. In PP-OCR, Differentiable Binarization (DB) is used as text detector which is based on a simple segmentation network. It integrates feature extraction and sequence modeling. It adopts the Connectionist Temporal Classification (CTC) loss to avoid the inconsistency between prediction and label.", "paper": {"title": "PP-OCR: A Practical Ultra Lightweight OCR System", "url": "https://paperswithcode.com/paper/pp-ocr-a-practical-ultra-lightweight-ocr"}, "introduced_year": 2000, "source_url": "https://arxiv.org/abs/2009.09941v3", "source_title": "PP-OCR: A Practical Ultra Lightweight OCR System", "code_snippet_url": null, "num_papers": 2, "id": "pwc:method/pp-ocr", "type": "method"}
{"url": "https://paperswithcode.com/method/gated-convolution", "name": "Gated Convolution", "full_name": "Gated Convolution", "description": "A **Gated Convolution** is a type of temporal [convolution](https://paperswithcode.com/method/convolution) with a gating mechanism. Zero-padding is used to ensure that future context can not be seen.", "paper": {"title": "Language Modeling with Gated Convolutional Networks", "url": "https://paperswithcode.com/paper/language-modeling-with-gated-convolutional"}, "introduced_year": 2000, "source_url": "http://arxiv.org/abs/1612.08083v3", "source_title": "Language Modeling with Gated Convolutional Networks", "code_snippet_url": null, "num_papers": 20, "id": "pwc:method/gated-convolution", "type": "method"}
{"url": "https://paperswithcode.com/method/vip-deeplab", "name": "ViP-DeepLab", "full_name": "ViP-DeepLab", "description": "**ViP-DeepLab** is a model for depth-aware video panoptic segmentation. It extends Panoptic-[DeepLab](https://paperswithcode.com/method/deeplab) by adding a depth prediction head to perform monocular depth estimation and a next-frame instance branch which regresses to the object centers in frame $t$ for frame $t + 1$.  This allows the model to jointly perform video panoptic segmentation and monocular depth estimation.", "paper": {"title": "ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation", "url": "https://paperswithcode.com/paper/vip-deeplab-learning-visual-perception-with"}, "introduced_year": 2000, "source_url": "https://arxiv.org/abs/2012.05258v1", "source_title": "ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation", "code_snippet_url": null, "num_papers": 1, "id": "pwc:method/vip-deeplab", "type": "method"}
{"url": "https://paperswithcode.com/method/shufflenet", "name": "ShuffleNet", "full_name": "ShuffleNet", "description": "**ShuffleNet** is a convolutional neural network designed specially for mobile devices with very limited computing power. The architecture utilizes two new operations, pointwise group [convolution](https://paperswithcode.com/method/convolution) and [channel shuffle](https://paperswithcode.com/method/channel-shuffle), to reduce computation cost while maintaining accuracy.", "paper": {"title": "ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices", "url": "https://paperswithcode.com/paper/shufflenet-an-extremely-efficient"}, "introduced_year": 2000, "source_url": "http://arxiv.org/abs/1707.01083v2", "source_title": "ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices", "code_snippet_url": "https://github.com/mindspore-ecosystem/mindcv/blob/main/mindcv/models/shufflenetv1.py", "num_papers": 41, "id": "pwc:method/shufflenet", "type": "method"}
{"url": "https://paperswithcode.com/method/featurenms", "name": "FeatureNMS", "full_name": "FeatureNMS", "description": "**Feature Non-Maximum Suppression**, or **FeatureNMS**, is a post-processing step for object detection models that removes duplicates where there are multiple detections outputted per object. FeatureNMS recognizes duplicates not only based on the intersection over union between the bounding boxes, but also based on the difference of feature vectors. These feature vectors can encode more information like visual appearance.", "paper": {"title": "FeatureNMS: Non-Maximum Suppression by Learning Feature Embeddings", "url": "https://paperswithcode.com/paper/featurenms-non-maximum-suppression-by"}, "introduced_year": 2000, "source_url": "https://arxiv.org/abs/2002.07662v2", "source_title": "FeatureNMS: Non-Maximum Suppression by Learning Feature Embeddings", "code_snippet_url": null, "num_papers": 1, "id": "pwc:method/featurenms", "type": "method"}
{"url": "https://paperswithcode.com/method/cv-mim", "name": "CV-MIM", "full_name": "Contrastive Cross-View Mutual Information Maximization", "description": "**CV-MIM**, or **Contrastive Cross-View Mutual Information Maximization**, is a representation learning method to disentangle pose-dependent as well as view-dependent factors from 2D human poses. The method trains a network using cross-view mutual information maximization, which maximizes mutual information of the same pose performed from different viewpoints in a contrastive learning manner. It further utilizes two regularization terms to ensure disentanglement and smoothness of the learned representations.", "paper": {"title": "Learning View-Disentangled Human Pose Representation by Contrastive Cross-View Mutual Information Maximization", "url": "https://paperswithcode.com/paper/learning-view-disentangled-human-pose"}, "introduced_year": 2000, "source_url": "https://arxiv.org/abs/2012.01405v2", "source_title": "Learning View-Disentangled Human Pose Representation by Contrastive Cross-View Mutual Information Maximization", "code_snippet_url": null, "num_papers": 2, "id": "pwc:method/cv-mim", "type": "method"}
{"url": "https://paperswithcode.com/method/ssds", "name": "SSDS", "full_name": "Self-Supervised Deep Supervision", "description": "The method exploits the finding that high correlation of segmentation performance among each U-Net's decoder layer -- with discriminative layer attached -- tends to have higher segmentation performance in the final segmentation map. By introducing an \"Inter-layer Divergence Loss\", based on Kulback-Liebler Divergence, to promotes the consistency between each discriminative output from decoder layers by minimizing the divergence.\r\n\r\nIf we assume that each decoder layer is equivalent to PDE function parameterized by weight parameter $\\theta$:\r\n\r\n$Decoder_i(x;\\theta_i) \\equiv PDE(x;\\theta_i)$\r\n\r\nThen our objective is trying to make each discriminative output similar to each other:\r\n\r\n$PDE(x; \\theta_d) \\sim PDE(x; \\theta_i);\\text{ } 0 \\leq i < d$\r\n\r\nHence the objective is to $\\text{minimize} \\sum_{i=0}^{d} D_{KL}(\\hat{y} || Decoder_i)$.", "paper": {"title": "OCTAve: 2D en face Optical Coherence Tomography Angiography Vessel Segmentation in Weakly-Supervised Learning with Locality Augmentation", "url": "https://paperswithcode.com/paper/octave-2d-en-face-optical-coherence"}, "introduced_year": 2000, "source_url": "https://arxiv.org/abs/2207.12238v1", "source_title": "OCTAve: 2D en face Optical Coherence Tomography Angiography Vessel Segmentation in Weakly-Supervised Learning with Locality Augmentation", "code_snippet_url": "", "num_papers": 1, "id": "pwc:method/ssds", "type": "method"}
{"url": "https://paperswithcode.com/method/dgi", "name": "DGI", "full_name": "Deep Graph Infomax", "description": "Deep Graph Infomax (DGI), a general approach for learning node representations within graph-structured data in an unsupervised manner. DGI relies on maximizing mutual information between patch representations and corresponding high-level summaries of graphs\u2014both derived using established graph convolutional network architectures. The learnt patch representations summarize subgraphs centered around nodes of interest, and can thus be reused for downstream node-wise learning tasks. In contrast to most prior approaches to unsupervised learning with GCNs, DGI does not rely on random walk objectives, and is readily applicable to both transductive and inductive learning setups.\r\n\r\nDescription and image from: [DEEP GRAPH INFOMAX](https://arxiv.org/pdf/1809.10341.pdf)", "paper": {"title": "Deep Graph Infomax", "url": "https://paperswithcode.com/paper/deep-graph-infomax"}, "introduced_year": 2000, "source_url": "http://arxiv.org/abs/1809.10341v2", "source_title": "Deep Graph Infomax", "code_snippet_url": "", "num_papers": 7, "id": "pwc:method/dgi", "type": "method"}
{"url": "https://paperswithcode.com/method/waveglow", "name": "WaveGlow", "full_name": "WaveGlow", "description": "**WaveGlow** is a flow-based generative model that generates audio by sampling from a distribution. Specifically samples are taken from a zero mean spherical Gaussian with the same number of dimensions as our desired output, and those samples are put through a series of layers that transforms the simple distribution to one which has the desired distribution.", "paper": {"title": "WaveGlow: A Flow-based Generative Network for Speech Synthesis", "url": "https://paperswithcode.com/paper/waveglow-a-flow-based-generative-network-for"}, "introduced_year": 2000, "source_url": "http://arxiv.org/abs/1811.00002v1", "source_title": "WaveGlow: A Flow-based Generative Network for Speech Synthesis", "code_snippet_url": null, "num_papers": 17, "id": "pwc:method/waveglow", "type": "method"}
{"url": "https://paperswithcode.com/method/wegl", "name": "WEGL", "full_name": "Wasserstein Embedding for Graph Learning", "description": "Please enter a description here", "paper": {"title": "Wasserstein Embedding for Graph Learning", "url": "https://paperswithcode.com/paper/wasserstein-embedding-for-graph-learning"}, "introduced_year": 2000, "source_url": "https://arxiv.org/abs/2006.09430v2", "source_title": "Wasserstein Embedding for Graph Learning", "code_snippet_url": null, "num_papers": 1, "id": "pwc:method/wegl", "type": "method"}
{"url": "https://paperswithcode.com/method/nuqsgd", "name": "NUQSGD", "full_name": "Nonuniform Quantization for Stochastic Gradient Descent", "description": "As the size and complexity of models and datasets grow, so does the need for communication-efficient variants of stochastic gradient descent that can be deployed to perform parallel model training. One popular communication-compression method for data-parallel [SGD](https://paperswithcode.com/method/sgd) is QSGD (Alistarh et al., 2017), which quantizes and encodes gradients to reduce communication costs. The baseline variant of QSGD provides strong theoretical guarantees, however, for practical purposes, the authors proposed a heuristic variant which we call QSGDinf, which demonstrated impressive empirical gains for distributed training of large neural networks. In this paper, we build on this work to propose a new gradient quantization scheme, and show that it has both stronger theoretical guarantees than QSGD, and matches and exceeds the empirical performance of the QSGDinf heuristic and of other compression methods.", "paper": {"title": "NUQSGD: Provably Communication-efficient Data-parallel SGD via Nonuniform Quantization", "url": "https://paperswithcode.com/paper/nuqsgd-provably-communication-efficient-data"}, "introduced_year": 2000, "source_url": "https://arxiv.org/abs/1908.06077", "source_title": "NUQSGD: Provably Communication-efficient Data-parallel SGD via Nonuniform Quantization", "code_snippet_url": "https://github.com/fartashf/nuqsgd", "num_papers": 1, "id": "pwc:method/nuqsgd", "type": "method"}
{"url": "https://paperswithcode.com/method/routing-transformer", "name": "Routing Transformer", "full_name": "Routing Transformer", "description": "The **Routing Transformer** is a [Transformer](https://paperswithcode.com/method/transformer) that endows self-attention with a sparse routing module based on online k-means. Each attention module considers a clustering of the space: the current timestep only attends to context belonging to the same cluster. In other word, the current time-step query is routed to a limited number of context through its cluster assignment.", "paper": {"title": "Efficient Content-Based Sparse Attention with Routing Transformers", "url": "https://paperswithcode.com/paper/efficient-content-based-sparse-attention-with-1"}, "introduced_year": 2000, "source_url": "https://arxiv.org/abs/2003.05997v5", "source_title": "Efficient Content-Based Sparse Attention with Routing Transformers", "code_snippet_url": null, "num_papers": 3, "id": "pwc:method/routing-transformer", "type": "method"}
{"url": "https://paperswithcode.com/method/child-tuning", "name": "Child-Tuning", "full_name": "Child-Tuning", "description": "**Child-Tuning** is a fine-tuning technique that updates a subset of parameters (called child network) of large pretrained models via strategically masking out the gradients of the non-child network during the backward process. It decreases the hypothesis space of the model via a task-specific mask applied to the full gradients, helping to effectively adapt the large-scale pretrained model to various tasks and meanwhile aiming to maintain its original generalization ability.", "paper": {"title": "Raise a Child in Large Language Model: Towards Effective and Generalizable Fine-tuning", "url": "https://paperswithcode.com/paper/raise-a-child-in-large-language-model-towards"}, "introduced_year": 2000, "source_url": "https://arxiv.org/abs/2109.05687v1", "source_title": "Raise a Child in Large Language Model: Towards Effective and Generalizable Fine-tuning", "code_snippet_url": null, "num_papers": 1, "id": "pwc:method/child-tuning", "type": "method"}
{"url": "https://paperswithcode.com/method/mlfpn", "name": "MLFPN", "full_name": "MLFPN", "description": "**Multi-Level Feature Pyramid Network**, or **MLFPN**, is a feature pyramid block used in object detection models, notably [M2Det](https://paperswithcode.com/method/m2det). We first fuse multi-level features (i.e. multiple layers) extracted by a backbone as a base feature, and then feed it into a block of alternating joint Thinned U-shape Modules ([TUM](https://paperswithcode.com/method/tum)) and Feature Fusion Modules (FFM) to extract more representative, multi-level multi-scale features. Finally, we gather up the feature maps with equivalent scales to construct the final feature pyramid for object detection. Decoder layers that form the final feature pyramid are much deeper than the layers in the backbone, namely, they are more representative. Moreover, each feature map in the final feature pyramid consists of the decoder layers from multiple levels. Hence, the feature pyramid block is called Multi-Level Feature Pyramid Network (MLFPN).", "paper": {"title": "M2Det: A Single-Shot Object Detector based on Multi-Level Feature Pyramid Network", "url": "https://paperswithcode.com/paper/m2det-a-single-shot-object-detector-based-on"}, "introduced_year": 2000, "source_url": "http://arxiv.org/abs/1811.04533v3", "source_title": "M2Det: A Single-Shot Object Detector based on Multi-Level Feature Pyramid Network", "code_snippet_url": "https://github.com/qijiezhao/M2Det/blob/ade4f3d12979800c367bf1e46d2e316e73a87514/m2det.py", "num_papers": 2, "id": "pwc:method/mlfpn", "type": "method"}
{"url": "https://paperswithcode.com/method/1cycle", "name": "1cycle", "full_name": "1cycle learning rate scheduling policy", "description": "", "paper": {"title": "A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, momentum, and weight decay", "url": "https://paperswithcode.com/paper/a-disciplined-approach-to-neural-network"}, "introduced_year": 2000, "source_url": "http://arxiv.org/abs/1803.09820v2", "source_title": "A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, momentum, and weight decay", "code_snippet_url": null, "num_papers": 10, "id": "pwc:method/1cycle", "type": "method"}
{"url": "https://paperswithcode.com/method/deepdrug", "name": "DeepDrug", "full_name": "DeepDrug", "description": "**DeepDrug** is a deep learning framework to overcome these shortcomings by using graph convolutional networks to learn the graphical representations of drugs and proteins such as molecular fingerprints and residual structures in order to boost the prediction accuracy.", "paper": {"title": "DeepDrug: A General Graph-Based Deep Learning Framework for Drug Relation Prediction", "url": "https://paperswithcode.com/paper/deepdrug-a-general-graph-based-deep-learning"}, "introduced_year": 2000, "source_url": "https://www.researchgate.net/publication/346825980_DeepDrug_A_general_graph-based_deep_learning_framework_for_drug_relation_prediction", "source_title": "DeepDrug: A General Graph-Based Deep Learning Framework for Drug Relation Prediction", "code_snippet_url": null, "num_papers": 1, "id": "pwc:method/deepdrug", "type": "method"}
{"url": "https://paperswithcode.com/method/h-bemd", "name": "H-BEMD", "full_name": "Hue \u2014 Bi-Dimensional Empirical Mode Decomposition", "description": "", "paper": {"title": "Deep Learning for Landslide Recognition in Satellite Architecture", "url": "https://paperswithcode.com/paper/deep-learning-for-landslide-recognition-in"}, "introduced_year": 2000, "source_url": "https://ieeexplore.ieee.org/document/9159123", "source_title": "Deep Learning for Landslide Recognition in Satellite Architecture", "code_snippet_url": null, "num_papers": 1, "id": "pwc:method/h-bemd", "type": "method"}
{"url": "https://paperswithcode.com/method/fractal-block", "name": "Fractal Block", "full_name": "Fractal Block", "description": "A **Fractal Block** is an image model block that utilizes an expansion rule that yields a structural layout of truncated fractals. For the base case where $f\\_{1}\\left(z\\right) = \\text{conv}\\left(z\\right)$ is a convolutional layer, we then have recursive fractals of the form:\r\n\r\n$$ f\\_{C+1}\\left(z\\right) = \\left[\\left(f\\_{C}\\circ{f\\_{C}}\\right)\\left(z\\right)\\right] \\oplus \\left[\\text{conv}\\left(z\\right)\\right]$$\r\n\r\nWhere $C$ is the number of columns. For the join layer (green in Figure), we use the element-wise mean rather than concatenation or addition.", "paper": {"title": "FractalNet: Ultra-Deep Neural Networks without Residuals", "url": "https://paperswithcode.com/paper/fractalnet-ultra-deep-neural-networks-without"}, "introduced_year": 2000, "source_url": "http://arxiv.org/abs/1605.07648v4", "source_title": "FractalNet: Ultra-Deep Neural Networks without Residuals", "code_snippet_url": "https://github.com/osmr/imgclsmob/blob/68335927ba27f2356093b985bada0bc3989836b1/pytorch/pytorchcv/models/fractalnet_cifar.py#L103", "num_papers": 6, "id": "pwc:method/fractal-block", "type": "method"}
{"url": "https://paperswithcode.com/method/gan", "name": "GAN", "full_name": "Generative Adversarial Network", "description": "A **GAN**, or **Generative Adversarial Network**, is a generative model that simultaneously trains\r\ntwo models: a generative model $G$ that captures the data distribution, and a discriminative model $D$ that estimates the\r\nprobability that a sample came from the training data rather than $G$.\r\n\r\nThe training procedure for $G$ is to maximize the probability of $D$ making\r\na mistake. This framework corresponds to a minimax two-player game. In the\r\nspace of arbitrary functions $G$ and $D$, a unique solution exists, with $G$\r\nrecovering the training data distribution and $D$ equal to $\\frac{1}{2}$\r\neverywhere. In the case where $G$ and $D$ are defined by multilayer perceptrons,\r\nthe entire system can be trained with backpropagation. \r\n\r\n(Image Source: [here](http://www.kdnuggets.com/2017/01/generative-adversarial-networks-hot-topic-machine-learning.html))", "paper": {"title": "Generative Adversarial Networks", "url": "https://paperswithcode.com/paper/generative-adversarial-networks"}, "introduced_year": 2000, "source_url": "https://arxiv.org/abs/1406.2661v1", "source_title": "Generative Adversarial Networks", "code_snippet_url": "https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/gan/gan.py", "num_papers": 1188, "id": "pwc:method/gan", "type": "method"}
{"url": "https://paperswithcode.com/method/alphazero", "name": "AlphaZero", "full_name": "AlphaZero", "description": "**AlphaZero** is a reinforcement learning agent for playing board games such as Go, chess, and shogi. ", "paper": {"title": "Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm", "url": "https://paperswithcode.com/paper/mastering-chess-and-shogi-by-self-play-with-a"}, "introduced_year": 2000, "source_url": "http://arxiv.org/abs/1712.01815v1", "source_title": "Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm", "code_snippet_url": null, "num_papers": 61, "id": "pwc:method/alphazero", "type": "method"}
{"url": "https://paperswithcode.com/method/single-headed-attention", "name": "Single-Headed Attention", "full_name": "Single-Headed Attention", "description": "**Single-Headed Attention** is a single-headed attention module used in the [SHA-RNN](https://paperswithcode.com/method/sha-rnn) language model. The principle design reasons for single-headedness were simplicity (avoiding running out of memory) and scepticism about the benefits of using multiple heads.", "paper": {"title": "Single Headed Attention RNN: Stop Thinking With Your Head", "url": "https://paperswithcode.com/paper/single-headed-attention-rnn-stop-thinking"}, "introduced_year": 2000, "source_url": "https://arxiv.org/abs/1911.11423v2", "source_title": "Single Headed Attention RNN: Stop Thinking With Your Head", "code_snippet_url": "https://github.com/Smerity/sha-rnn/blob/218d748022dbcf32d50bbbb4d151a9b6de3f8bba/model.py#L53", "num_papers": 5, "id": "pwc:method/single-headed-attention", "type": "method"}
{"url": "https://paperswithcode.com/method/good-feature-matching", "name": "Good Feature Matching", "full_name": "Good Feature Matching", "description": "**Good Feature Matching** is an active map-to-frame feature matching method. Feature matching effort is tied to submatrix selection, which has combinatorial time complexity and requires choosing a scoring metric. Via simulation, the Max-logDet matrix revealing metric is shown to perform best.", "paper": {"title": "Good Feature Matching: Towards Accurate, Robust VO/VSLAM with Low Latency", "url": "https://paperswithcode.com/paper/good-feature-matching-towards-accurate-robust"}, "introduced_year": 2000, "source_url": "https://arxiv.org/abs/2001.00714v1", "source_title": "Good Feature Matching: Towards Accurate, Robust VO/VSLAM with Low Latency", "code_snippet_url": "", "num_papers": 1, "id": "pwc:method/good-feature-matching", "type": "method"}
{"url": "https://paperswithcode.com/method/gpt-2", "name": "GPT-2", "full_name": "GPT-2", "description": "**GPT-2** is a [Transformer](https://paperswithcode.com/methods/category/transformers) architecture that was notable for its size (1.5 billion parameters) on its release. The model is pretrained on a WebText dataset - text from 45 million website links. It largely follows the previous [GPT](https://paperswithcode.com/method/gpt) architecture with some modifications:\r\n\r\n- [Layer normalization](https://paperswithcode.com/method/layer-normalization) is moved to the input of each sub-block, similar to a\r\npre-activation residual network and an additional layer normalization was added after the final self-attention block. \r\n\r\n- A modified initialization which accounts for the accumulation on the residual path with model depth\r\nis used. Weights of residual layers are scaled at initialization by a factor of $1/\\sqrt{N}$ where $N$ is the number of residual layers. \r\n\r\n- The vocabulary is expanded to 50,257. The context size is expanded from 512 to 1024 tokens and\r\na larger batch size of 512 is used.", "paper": {"title": "Language Models are Unsupervised Multitask Learners", "url": "https://paperswithcode.com/paper/language-models-are-unsupervised-multitask"}, "introduced_year": 2000, "source_url": "https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf", "source_title": "Language Models are Unsupervised Multitask Learners", "code_snippet_url": null, "num_papers": 361, "id": "pwc:method/gpt-2", "type": "method"}
{"url": "https://paperswithcode.com/method/xlm", "name": "XLM", "full_name": "XLM", "description": "**XLM** is a [Transformer](https://paperswithcode.com/method/transformer) based architecture that is pre-trained using one of three language modelling objectives:\r\n\r\n1. Causal Language Modeling - models the probability of a word given the previous words in a sentence.\r\n2. Masked Language Modeling - the masked language modeling objective of [BERT](https://paperswithcode.com/method/bert).\r\n3. Translation Language Modeling - a (new) translation language modeling objective for improving cross-lingual pre-training.\r\n\r\nThe authors find that both the CLM and MLM approaches provide strong cross-lingual features that can be used for pretraining models.", "paper": {"title": "Cross-lingual Language Model Pretraining", "url": "https://paperswithcode.com/paper/cross-lingual-language-model-pretraining"}, "introduced_year": 2000, "source_url": "http://arxiv.org/abs/1901.07291v1", "source_title": "Cross-lingual Language Model Pretraining", "code_snippet_url": null, "num_papers": 47, "id": "pwc:method/xlm", "type": "method"}
{"url": "https://paperswithcode.com/method/lda2vec", "name": "lda2vec", "full_name": "lda2vec", "description": "**lda2vec** builds representations over both words and documents by mixing word2vec\u2019s skipgram architecture with Dirichlet-optimized sparse topic mixtures. \r\n\r\nThe Skipgram Negative-Sampling (SGNS) objective of word2vec is modified to utilize document-wide feature vectors while simultaneously learning continuous document weights loading onto topic vectors. The total loss term $L$ is the sum of the Skipgram Negative Sampling Loss (SGNS) $L^{neg}\\_{ij}$ with the addition of a Dirichlet-likelihood term over document weights, $L\\_{d}$. The loss is conducted using a context vector, $\\overrightarrow{c\\_{j}}$ , pivot word vector $\\overrightarrow{w\\_{j}}$, target word vector $\\overrightarrow{w\\_{i}}$, and negatively-sampled word vector $\\overrightarrow{w\\_{l}}$:\r\n\r\n$$ L = L^{d} + \\Sigma\\_{ij}L^{neg}\\_{ij} $$\r\n\r\n$$L^{neg}\\_{ij} = \\log\\sigma\\left(c\\_{j}\\cdot\\overrightarrow{w\\_{i}}\\right) + \\sum^{n}\\_{l=0}\\sigma\\left(-\\overrightarrow{c\\_{j}}\\cdot\\overrightarrow{w\\_{l}}\\right)$$", "paper": {"title": "Mixing Dirichlet Topic Models and Word Embeddings to Make lda2vec", "url": "https://paperswithcode.com/paper/mixing-dirichlet-topic-models-and-word"}, "introduced_year": 2000, "source_url": "http://arxiv.org/abs/1605.02019v1", "source_title": "Mixing Dirichlet Topic Models and Word Embeddings to Make lda2vec", "code_snippet_url": null, "num_papers": 1, "id": "pwc:method/lda2vec", "type": "method"}
{"url": "https://paperswithcode.com/method/modern", "name": "MODERN", "full_name": "Modulated Residual Network", "description": "**MODERN**, or **Modulated Residual Network**, is an architecture for [visual question answering](https://paperswithcode.com/task/visual-question-answering) (VQA). It employs [conditional batch normalization](https://paperswithcode.com/method/conditional-batch-normalization) to allow a linguistic embedding from an [LSTM](https://paperswithcode.com/method/lstm) to modulate the [batch normalization](https://paperswithcode.com/method/batch-normalization) parameters of a [ResNet](https://paperswithcode.com/method/resnet). This enables the linguistic embedding to manipulate entire feature maps by scaling them up or down, negating them, or shutting them off, etc.", "paper": {"title": "Modulating early visual processing by language", "url": "https://paperswithcode.com/paper/modulating-early-visual-processing-by"}, "introduced_year": 2000, "source_url": "http://arxiv.org/abs/1707.00683v3", "source_title": "Modulating early visual processing by language", "code_snippet_url": null, "num_papers": 1, "id": "pwc:method/modern", "type": "method"}
{"url": "https://paperswithcode.com/method/center-pooling", "name": "Center Pooling", "full_name": "Center Pooling", "description": "**Center Pooling** is a pooling technique for object detection that aims to capture richer and more recognizable visual patterns. The geometric centers of objects do not necessarily convey very recognizable visual patterns (e.g., the human head contains strong visual patterns, but the center keypoint is often in the middle of the human body). \r\n\r\nThe detailed process of center pooling is as follows: the backbone outputs a feature map, and to determine if a pixel in the feature map is a center keypoint, we need to find the maximum value in its both horizontal and vertical directions and add them together. By doing this, center pooling helps the better detection of center keypoints.", "paper": {"title": "CenterNet: Keypoint Triplets for Object Detection", "url": "https://paperswithcode.com/paper/centernet-object-detection-with-keypoint"}, "introduced_year": 2000, "source_url": "http://arxiv.org/abs/1904.08189v3", "source_title": "CenterNet: Keypoint Triplets for Object Detection", "code_snippet_url": "https://github.com/Duankaiwen/CenterNet/blob/435b86aa602a4d28768c192884173727d4b45ea2/models/CenterNet-104.py#L106", "num_papers": 30, "id": "pwc:method/center-pooling", "type": "method"}
{"url": "https://paperswithcode.com/method/cs-gan", "name": "CS-GAN", "full_name": "CS-GAN", "description": "**CS-GAN** is a type of generative adversarial network that uses a form of deep compressed sensing, and [latent optimisation](https://paperswithcode.com/method/latent-optimisation), to improve the quality of generated samples.", "paper": {"title": "Deep Compressed Sensing", "url": "https://paperswithcode.com/paper/deep-compressed-sensing"}, "introduced_year": 2000, "source_url": "https://arxiv.org/abs/1905.06723v2", "source_title": "Deep Compressed Sensing", "code_snippet_url": "https://github.com/deepmind/deepmind-research/tree/master/cs_gan", "num_papers": 3, "id": "pwc:method/cs-gan", "type": "method"}
{"url": "https://paperswithcode.com/method/piou-loss", "name": "PIoU Loss", "full_name": "PIoU Loss", "description": "**PIoU Loss** is a loss function for oriented object detection which is formulated to exploit both the angle and IoU for accurate oriented bounding box regression. The PIoU loss is derived from IoU metric with a pixel-wise form.", "paper": {"title": "PIoU Loss: Towards Accurate Oriented Object Detection in Complex Environments", "url": "https://paperswithcode.com/paper/piou-loss-towards-accurate-oriented-object"}, "introduced_year": 2000, "source_url": "https://arxiv.org/abs/2007.09584v1", "source_title": "PIoU Loss: Towards Accurate Oriented Object Detection in Complex Environments", "code_snippet_url": null, "num_papers": 1, "id": "pwc:method/piou-loss", "type": "method"}
{"url": "https://paperswithcode.com/method/cam", "name": "CAM", "full_name": "Class-activation map", "description": "Class activation maps could be used to interpret the prediction decision made by the convolutional neural network (CNN).\r\n\r\nImage source: [Learning Deep Features for Discriminative Localization](https://paperswithcode.com/paper/learning-deep-features-for-discriminative)", "paper": {"title": "Is Object Localization for Free? - Weakly-Supervised Learning With Convolutional Neural Networks", "url": "https://paperswithcode.com/paper/is-object-localization-for-free-weakly"}, "introduced_year": 2000, "source_url": "http://openaccess.thecvf.com/content_cvpr_2015/html/Oquab_Is_Object_Localization_2015_CVPR_paper.html", "source_title": "Is Object Localization for Free? - Weakly-Supervised Learning With Convolutional Neural Networks", "code_snippet_url": "", "num_papers": 127, "id": "pwc:method/cam", "type": "method"}
{"url": "https://paperswithcode.com/method/proxylessnet-cpu", "name": "ProxylessNet-CPU", "full_name": "ProxylessNet-CPU", "description": "**ProxylessNet-CPU** is an image model learnt with the [ProxylessNAS](https://paperswithcode.com/method/proxylessnas) [neural architecture search](https://paperswithcode.com/method/neural-architecture-search) algorithm that is optimized for CPU devices. It uses inverted residual blocks (MBConvs) from [MobileNetV2](https://paperswithcode.com/method/mobilenetv2) as its basic building block.", "paper": {"title": "ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware", "url": "https://paperswithcode.com/paper/proxylessnas-direct-neural-architecture"}, "introduced_year": 2000, "source_url": "http://arxiv.org/abs/1812.00332v2", "source_title": "ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware", "code_snippet_url": "https://github.com/mit-han-lab/proxylessnas/blob/0491e8e8678f5b3dcd3a856f3c842586d3dbf8a3/proxyless_nas/nas_modules.py#L54", "num_papers": 2, "id": "pwc:method/proxylessnet-cpu", "type": "method"}
{"url": "https://paperswithcode.com/method/pisa", "name": "PISA", "full_name": "PrIme Sample Attention", "description": "**PrIme Sample Attention (PISA)** directs the training of object detection frameworks towards prime samples. These are samples that play a key role in driving the detection performance. The authors define Hierarchical Local Rank (HLR) as a metric of importance. Specifically, they use IoU-HLR to rank positive samples and ScoreHLR to rank negative samples in each mini-batch. This ranking strategy places the positive samples with highest IoUs around each object and the negative samples with highest scores in each cluster to the top of the ranked list and directs the focus of the training process to them via a simple re-weighting scheme. The authors also devise a classification-aware regression loss to jointly optimize the classification and regression branches. Particularly, this loss would suppress those samples with large regression loss, thus reinforcing the attention to prime samples.", "paper": {"title": "Prime Sample Attention in Object Detection", "url": "https://paperswithcode.com/paper/prime-sample-attention-in-object-detection"}, "introduced_year": 2000, "source_url": "https://arxiv.org/abs/1904.04821v2", "source_title": "Prime Sample Attention in Object Detection", "code_snippet_url": "", "num_papers": 5, "id": "pwc:method/pisa", "type": "method"}
{"url": "https://paperswithcode.com/method/spreadsheetcoder", "name": "SpreadsheetCoder", "full_name": "SpreadsheetCoder", "description": "**SpreadsheetCoder** is a neural network architecture for spreadsheet formula prediction. It is a [BERT](https://paperswithcode.com/method/bert)-based model architecture to represent the tabular context in both row-based and column-based formats. A [BERT](https://paperswithcode.com/method/bert) encoder computes an embedding vector for each input token, incorporating the contextual information from nearby rows and columns. The BERT encoder is initialized from the weights pre-trained on English text corpora, which is beneficial for encoding table headers. To handle cell references, a two-stage decoding process is used inspired by sketch learning for program synthesis. The decoder first generates a formula sketch, which does not include concrete cell references, and then predicts the corresponding cell ranges to generate the complete formula", "paper": {"title": "SpreadsheetCoder: Formula Prediction from Semi-structured Context", "url": "https://paperswithcode.com/paper/spreadsheetcoder-formula-prediction-from-semi-1"}, "introduced_year": 2000, "source_url": "https://arxiv.org/abs/2106.15339v1", "source_title": "SpreadsheetCoder: Formula Prediction from Semi-structured Context", "code_snippet_url": null, "num_papers": 1, "id": "pwc:method/spreadsheetcoder", "type": "method"}
{"url": "https://paperswithcode.com/method/fast-autoaugment", "name": "Fast AutoAugment", "full_name": "Fast AutoAugment", "description": "**Fast AutoAugment** is an image data augmentation algorithm that finds effective augmentation policies via a search strategy based on density matching, motivated by Bayesian DA. The strategy is to improve the generalization performance of a given network by learning the augmentation policies which treat augmented data as missing data points of training data. However, different from Bayesian DA, the proposed method recovers those missing data points by the exploitation-and-exploration of a family of inference-time augmentations via Bayesian optimization in the policy search phase. This is realized by using an efficient density matching algorithm that does not require any back-propagation for network training for each policy evaluation.", "paper": {"title": "Fast AutoAugment", "url": "https://paperswithcode.com/paper/fast-autoaugment"}, "introduced_year": 2000, "source_url": "https://arxiv.org/abs/1905.00397v2", "source_title": "Fast AutoAugment", "code_snippet_url": "https://github.com/kakaobrain/fast-autoaugment", "num_papers": 6, "id": "pwc:method/fast-autoaugment", "type": "method"}
{"url": "https://paperswithcode.com/method/cascadepsp", "name": "CascadePSP", "full_name": "CascadePSP", "description": "**CascadePSP** is a general segmentation refinement model that refines any given segmentation from low to high resolution. The model takes as input an initial mask that can be an output of any algorithm to provide a rough object location. Then the CascadePSP will output a refined mask. The model is designed in a cascade fashion that generates refined segmentation in a coarse-to-fine manner. Coarse outputs from the early levels predict object structure which will be used as input to the latter levels to refine boundary details.", "paper": {"title": "CascadePSP: Toward Class-Agnostic and Very High-Resolution Segmentation via Global and Local Refinement", "url": "https://paperswithcode.com/paper/cascadepsp-toward-class-agnostic-and-very"}, "introduced_year": 2000, "source_url": "https://arxiv.org/abs/2005.02551v1", "source_title": "CascadePSP: Toward Class-Agnostic and Very High-Resolution Segmentation via Global and Local Refinement", "code_snippet_url": "", "num_papers": 1, "id": "pwc:method/cascadepsp", "type": "method"}
{"url": "https://paperswithcode.com/method/arshoe", "name": "ARShoe", "full_name": "ARShoe", "description": "**ARShoe** is a multi-branch network for pose estimation and segmentation tackling the \"try-on\" problem for augmented reality shoes. Consisting of an encoder and a decoder, the multi-branch network is trained to predict keypoints [heatmap](https://paperswithcode.com/method/heatmap) (heatmap), [PAFs](https://paperswithcode.com/method/pafs) heatmap (pafmap), and segmentation results (segmap) simultaneously. Post processes are then performed for a smooth and realistic virtual try-on.", "paper": {"title": "ARShoe: Real-Time Augmented Reality Shoe Try-on System on Smartphones", "url": "https://paperswithcode.com/paper/arshoe-real-time-augmented-reality-shoe-try"}, "introduced_year": 2000, "source_url": "https://arxiv.org/abs/2108.10515v1", "source_title": "ARShoe: Real-Time Augmented Reality Shoe Try-on System on Smartphones", "code_snippet_url": null, "num_papers": 1, "id": "pwc:method/arshoe", "type": "method"}
{"url": "https://paperswithcode.com/method/nnformer", "name": "nnFormer", "full_name": "nnFormer", "description": "**nnFormer**, or **not-another transFormer**, is a semantic segmentation model with an interleaved architecture based on empirical combination of self-attention and [convolution](https://paperswithcode.com/method/convolution). Firstly, a light-weight convolutional embedding layer ahead is used ahead of [transformer](https://paperswithcode.com/method/transformer) blocks. In comparison to directly flattening raw pixels and applying 1D pre-processing, the convolutional embedding layer encodes precise (i.e., pixel-level) spatial information and provide low-level yet high-resolution 3D features. After the embedding block, transformer and convolutional down-sampling blocks are interleaved to fully entangle long-term dependencies with high-level and hierarchical object concepts at various scales, which helps improve the generalization ability and robustness of learned representations.", "paper": {"title": "nnFormer: Interleaved Transformer for Volumetric Segmentation", "url": "https://paperswithcode.com/paper/nnformer-interleaved-transformer-for"}, "introduced_year": 2000, "source_url": "https://arxiv.org/abs/2109.03201v6", "source_title": "nnFormer: Interleaved Transformer for Volumetric Segmentation", "code_snippet_url": "", "num_papers": 2, "id": "pwc:method/nnformer", "type": "method"}
{"url": "https://paperswithcode.com/method/cgnn", "name": "CGNN", "full_name": "Crystal Graph Neural Network", "description": "The full architecture of CGNN is presented at [CGNN's official site](https://tony-y.github.io/cgnn/architectures/).", "paper": {"title": "Crystal Graph Neural Networks for Data Mining in Materials Science", "url": "https://paperswithcode.com/paper/crystal-graph-neural-networks-for-data-mining"}, "introduced_year": 2000, "source_url": "https://www.researchgate.net/publication/333667001_Crystal_Graph_Neural_Networks_for_Data_Mining_in_Materials_Science", "source_title": "Crystal Graph Neural Networks for Data Mining in Materials Science", "code_snippet_url": "", "num_papers": 5, "id": "pwc:method/cgnn", "type": "method"}
{"url": "https://paperswithcode.com/method/pixel-bert", "name": "Pixel-BERT", "full_name": "Pixel-BERT", "description": "Pixel-BERT is pre-trained model that is trained to align image pixels with text. It is an end to end framework that includes CNN based visual encoder and cross modal transformers for visual and language embedding learning.\r\nThis model has three parts: one fully convolutional neural network that takes pixels of image as input, one word level token embedding based on BERT, and a multimodal transformers for jointly learning of visual and language embedding.\r\n\r\nFor language, it uses other pretraining works to use Masked Language Modeling (MLM) for the prediction of masked tokens with surrounding text and image. For vision it uses the random pixel sampling mechanism that makes up for the challenge of predicting pixel level features. This mechanism is also good for solving overfitting issues and improving the robustness of visual features. \r\n\r\nFor vision and language interaction, it applies Image-Text_Matching (ITM) to classify whether an image and sentence pair is matched or not. \r\n\r\nSome cross modality tasks like VQA, image captioning is required to understand both language and visual semantics. Region based visual features extracted from object detection models like Faster RCNN is used for better performance in the newer version of the model.", "paper": {"title": "Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal Transformers", "url": "https://paperswithcode.com/paper/pixel-bert-aligning-image-pixels-with-text-by"}, "introduced_year": 2000, "source_url": "https://arxiv.org/abs/2004.00849v2", "source_title": "Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal Transformers", "code_snippet_url": "", "num_papers": 2, "id": "pwc:method/pixel-bert", "type": "method"}
{"url": "https://paperswithcode.com/method/specgan", "name": "SpecGAN", "full_name": "SpecGAN", "description": "**SpecGAN** is a generative adversarial network method for spectrogram-based, frequency-domain audio generation. The problem is suited for GANs designed for image generation. The model can be approximately inverted. \r\n\r\nTo process audio into suitable spectrograms, the authors perform the short-time Fourier transform with 16 ms windows and 8ms stride, resulting in 128 frequency bins, linearly spaced from 0 to 8 kHz. They take the magnitude of the resultant spectra and scale amplitude values logarithmically to better-align with human perception. They then normalize each frequency bin to have zero mean and unit variance. They clip the spectra to $3$ standard deviations and rescale to $\\left[\u22121, 1\\right]$.\r\n\r\nThey then use the [DCGAN](https://paperswithcode.com/method/dcgan) approach on the result spectra.", "paper": {"title": "Adversarial Audio Synthesis", "url": "https://paperswithcode.com/paper/adversarial-audio-synthesis"}, "introduced_year": 2000, "source_url": "http://arxiv.org/abs/1802.04208v3", "source_title": "Adversarial Audio Synthesis", "code_snippet_url": null, "num_papers": 1, "id": "pwc:method/specgan", "type": "method"}
{"url": "https://paperswithcode.com/method/locally-grouped-self-attention", "name": "Locally-Grouped Self-Attention", "full_name": "Locally-Grouped Self-Attention", "description": "**Locally-Grouped Self-Attention**, or **LSA**, is a local attention mechanism used in the [Twins-SVT](https://paperswithcode.com/method/twins-svt) architecture. Locally-grouped self-attention (LSA). Motivated by the group design in depthwise convolutions for efficient inference, we first equally divide the 2D feature maps into sub-windows, making self-attention communications only happen within each sub-window. This design also resonates with the multi-head design in self-attention, where the communications only occur within the channels of the same head. To be specific, the feature maps are divided into $m \\times n$ sub-windows. Without loss of generality, we assume $H \\% m=0$ and $W \\% n=0$. Each group contains $\\frac{H W}{m n}$ elements, and thus the computation cost of the self-attention in this window is $\\mathcal{O}\\left(\\frac{H^{2} W^{2}}{m^{2} n^{2}} d\\right)$, and the total cost is $\\mathcal{O}\\left(\\frac{H^{2} W^{2}}{m n} d\\right)$. If we let $k\\_{1}=\\frac{H}{n}$ and $k\\_{2}=\\frac{W}{n}$, the cost can be computed as $\\mathcal{O}\\left(k\\_{1} k\\_{2} H W d\\right)$, which is significantly more efficient when $k\\_{1} \\ll H$ and $k\\_{2} \\ll W$ and grows linearly with $H W$ if $k\\_{1}$ and $k\\_{2}$ are fixed.\r\n\r\nAlthough the locally-grouped self-attention mechanism is computation friendly, the image is divided into non-overlapping sub-windows. Thus, we need a mechanism to communicate between different sub-windows, as in Swin. Otherwise, the information would be limited to be processed locally, which makes the receptive field small and significantly degrades the performance as shown in our experiments. This resembles the fact that we cannot replace all standard convolutions by depth-wise convolutions in CNNs.", "paper": {"title": "Twins: Revisiting the Design of Spatial Attention in Vision Transformers", "url": "https://paperswithcode.com/paper/twins-revisiting-spatial-attention-design-in"}, "introduced_year": 2000, "source_url": "https://arxiv.org/abs/2104.13840v4", "source_title": "Twins: Revisiting the Design of Spatial Attention in Vision Transformers", "code_snippet_url": "", "num_papers": 1, "id": "pwc:method/locally-grouped-self-attention", "type": "method"}
{"url": "https://paperswithcode.com/method/uniter", "name": "UNITER", "full_name": "UNiversal Image-TExt Representation Learning", "description": "UNITER or UNiversal Image-TExt Representation model is a large-scale pre-trained model for joint multimodal embedding. It is pre-trained using four image-text datasets COCO, Visual Genome, Conceptual Captions, and SBU Captions. It can power heterogeneous downstream V+L tasks with joint multimodal embeddings. \r\nUNITER takes the visual regions of the image and textual tokens of the sentence as inputs. A faster R-CNN is used in Image Embedder to extract the visual features of each region and a Text Embedder is used to tokenize the input sentence into WordPieces.  \r\n\r\nIt proposes WRA via the Optimal Transport to provide more fine-grained alignment between word tokens and image regions that is effective in calculating the minimum cost of transporting the contextualized image embeddings to word embeddings and vice versa. \r\n\r\nFour pretraining tasks were designed for this model. They are Masked Language Modeling (MLM), Masked Region Modeling (MRM, with three variants), Image-Text Matching (ITM), and Word-Region Alignment (WRA). This model is different from the previous models because it uses conditional masking on pre-training tasks.", "paper": {"title": "UNITER: UNiversal Image-TExt Representation Learning", "url": "https://paperswithcode.com/paper/uniter-learning-universal-image-text-1"}, "introduced_year": 2000, "source_url": "https://arxiv.org/abs/1909.11740v3", "source_title": "UNITER: UNiversal Image-TExt Representation Learning", "code_snippet_url": "", "num_papers": 20, "id": "pwc:method/uniter", "type": "method"}
{"url": "https://paperswithcode.com/method/mpso", "name": "MPSO", "full_name": "Motion-Encoded Particle Swarm Optimization", "description": "", "paper": {"title": "Motion-Encoded Particle Swarm Optimization for Moving Target Search Using UAVs", "url": "https://paperswithcode.com/paper/motion-encoded-particle-swarm-optimization"}, "introduced_year": 2000, "source_url": "https://arxiv.org/abs/2010.02039v1", "source_title": "Motion-Encoded Particle Swarm Optimization for Moving Target Search Using UAVs", "code_snippet_url": null, "num_papers": 2, "id": "pwc:method/mpso", "type": "method"}
{"url": "https://paperswithcode.com/method/voicefilter-lite", "name": "VoiceFilter-Lite", "full_name": "VoiceFilter-Lite", "description": "**VoiceFilter-Lite** is a single-channel source separation model that runs on the device to preserve only the speech signals from a target user, as part of a streaming speech recognition system. In this architecture, the voice filtering model operates as a frame-by-frame frontend signal processor to enhance the features consumed by the speech recognizer, without reconstructing audio signals from the features. The key contributions are (1) A system to perform speech separation directly on ASR input features; (2) An asymmetric loss function to penalize oversuppression during training, to make the model harmless under various acoustic environments, (3) An adaptive suppression strength mechanism to adapt to different noise conditions.", "paper": {"title": "VoiceFilter-Lite: Streaming Targeted Voice Separation for On-Device Speech Recognition", "url": "https://paperswithcode.com/paper/voicefilter-lite-streaming-targeted-voice"}, "introduced_year": 2000, "source_url": "https://arxiv.org/abs/2009.04323v1", "source_title": "VoiceFilter-Lite: Streaming Targeted Voice Separation for On-Device Speech Recognition", "code_snippet_url": null, "num_papers": 3, "id": "pwc:method/voicefilter-lite", "type": "method"}
{"url": "https://paperswithcode.com/method/split-attention", "name": "Split Attention", "full_name": "Split Attention", "description": "A **Split Attention** block enables attention across feature-map groups. As in [ResNeXt blocks](https://paperswithcode.com/method/resnext-block), the feature can be divided into several groups, and the number of feature-map groups is given by a cardinality hyperparameter $K$. The resulting feature-map groups are called cardinal groups. Split Attention blocks introduce a new radix hyperparameter $R$ that indicates the number of splits within a cardinal group, so the total number of feature groups is $G = KR$. We may apply a series of transformations {$\\mathcal{F}\\_1, \\mathcal{F}\\_2, \\cdots\\mathcal{F}\\_G$} to each individual group, then the intermediate representation of each group is $U\\_i = \\mathcal{F}\\_i\\left(X\\right)$, for $i \\in$ {$1, 2, \\cdots{G}$}.\r\n\r\nA combined representation for each cardinal group can be obtained by fusing via an element-wise summation across multiple splits. The representation for $k$-th cardinal group is \r\n$\\hat{U}^k = \\sum_{j=R(k-1)+1}^{R k} U_j $, where $\\hat{U}^k \\in \\mathbb{R}^{H\\times W\\times C/K}$ for $k\\in{1,2,...K}$, and $H$, $W$ and $C$ are the block output feature-map sizes. \r\nGlobal contextual information with embedded channel-wise statistics can be gathered with [global average pooling](https://paperswithcode.com/method/global-average-pooling) across spatial dimensions  $s^k\\in\\mathbb{R}^{C/K}$. Here the $c$-th component is calculated as:\r\n\r\n$$\r\n    s^k\\_c = \\frac{1}{H\\times W} \\sum\\_{i=1}^H\\sum\\_{j=1}^W \\hat{U}^k\\_c(i, j).\r\n$$\r\n\r\nA weighted fusion of the cardinal group representation $V^k\\in\\mathbb{R}^{H\\times W\\times C/K}$ is aggregated using [channel-wise soft attention](https://paperswithcode.com/method/channel-wise-soft-attention), where each feature-map channel is produced using a weighted combination over splits. The $c$-th channel is calculated as:\r\n\r\n$$\r\n    V^k_c=\\sum_{i=1}^R a^k_i(c) U_{R(k-1)+i} ,\r\n$$\r\n\r\nwhere $a_i^k(c)$ denotes a (soft) assignment weight given by:\r\n\r\n$$\r\na_i^k(c) =\r\n\\begin{cases}\r\n  \\frac{exp(\\mathcal{G}^c_i(s^k))}{\\sum_{j=0}^R exp(\\mathcal{G}^c_j(s^k))} & \\quad\\textrm{if } R>1, \\\\\r\n   \\frac{1}{1+exp(-\\mathcal{G}^c_i(s^k))} & \\quad\\textrm{if } R=1,\\\\\r\n\\end{cases}\r\n$$\r\n\r\nand mapping $\\mathcal{G}_i^c$ determines the weight of each split for the $c$-th channel based on the global context representation $s^k$.", "paper": {"title": "ResNeSt: Split-Attention Networks", "url": "https://paperswithcode.com/paper/resnest-split-attention-networks"}, "introduced_year": 2000, "source_url": "https://arxiv.org/abs/2004.08955v2", "source_title": "ResNeSt: Split-Attention Networks", "code_snippet_url": "https://github.com/zhanghang1989/ResNeSt/blob/5fe47e93bd7e098d15bc278d8ab4812b82b49414/resnest/torch/splat.py#L11", "num_papers": 13, "id": "pwc:method/split-attention", "type": "method"}
{"url": "https://paperswithcode.com/method/enet-bottleneck", "name": "ENet Bottleneck", "full_name": "ENet Bottleneck", "description": "**ENet Bottleneck** is an image model block used in the [ENet](https://paperswithcode.com/method/enet) semantic segmentation architecture. Each block consists of three convolutional layers: a 1 \u00d7 1 projection that reduces the dimensionality, a main convolutional layer, and a 1 \u00d7 1 expansion. We place [Batch Normalization](https://paperswithcode.com/method/batch-normalization) and [PReLU](https://paperswithcode.com/method/prelu) between all convolutions. If the bottleneck is downsampling, a [max pooling](https://paperswithcode.com/method/max-pooling) layer is added to the main branch.\r\nAlso, the first 1 \u00d7 1 projection is replaced with a 2 \u00d7 2 [convolution](https://paperswithcode.com/method/convolution) with stride 2 in both dimensions. We zero pad the activations, to match the number of feature maps.", "paper": {"title": "ENet: A Deep Neural Network Architecture for Real-Time Semantic Segmentation", "url": "https://paperswithcode.com/paper/enet-a-deep-neural-network-architecture-for"}, "introduced_year": 2000, "source_url": "http://arxiv.org/abs/1606.02147v1", "source_title": "ENet: A Deep Neural Network Architecture for Real-Time Semantic Segmentation", "code_snippet_url": "https://github.com/yassouali/pytorch_segmentation/blob/473503a22c99e78d1f938c0099f7d91f85555917/models/enet.py#L22", "num_papers": 21, "id": "pwc:method/enet-bottleneck", "type": "method"}
{"url": "https://paperswithcode.com/method/aspp", "name": "ASPP", "full_name": "Atrous Spatial Pyramid Pooling", "description": "**Atrous Spatial Pyramid Pooling (ASPP)** is a semantic segmentation module for resampling a given feature layer at multiple rates prior to [convolution](https://paperswithcode.com/method/convolution). This amounts to probing the original image with multiple filters that have complementary effective fields of view, thus capturing objects as well as useful image context at multiple scales. Rather than actually resampling features, the mapping is implemented using multiple parallel atrous convolutional layers with different sampling rates.", "paper": {"title": "DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs", "url": "https://paperswithcode.com/paper/deeplab-semantic-image-segmentation-with-deep"}, "introduced_year": 2000, "source_url": "http://arxiv.org/abs/1606.00915v2", "source_title": "DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs", "code_snippet_url": "", "num_papers": 54, "id": "pwc:method/aspp", "type": "method"}
{"url": "https://paperswithcode.com/method/spatial-attention-module", "name": "Spatial Attention Module", "full_name": "Spatial Attention Module", "description": "A **Spatial Attention Module** is a module for spatial attention in convolutional neural networks. It generates a spatial attention map by utilizing the inter-spatial relationship of features. Different from the [channel attention](https://paperswithcode.com/method/channel-attention-module), the spatial attention focuses on where is an informative part, which is complementary to the channel attention. To compute the spatial attention, we first apply average-pooling and max-pooling operations along the channel axis and concatenate them to generate an efficient feature descriptor. On the concatenated feature descriptor, we apply a [convolution](https://paperswithcode.com/method/convolution) layer to generate a spatial attention map $\\textbf{M}\\_{s}\\left(F\\right) \\in \\mathcal{R}^{H\u00d7W}$ which encodes where to emphasize or suppress. \r\n\r\nWe aggregate channel information of a feature map by using two pooling operations, generating two 2D maps: $\\mathbf{F}^{s}\\_{avg} \\in \\mathbb{R}^{1\\times{H}\\times{W}}$ and $\\mathbf{F}^{s}\\_{max} \\in \\mathbb{R}^{1\\times{H}\\times{W}}$. Each denotes average-pooled features and max-pooled features across the channel. Those are then concatenated and convolved by a standard convolution layer, producing the 2D spatial attention map. In short, the spatial attention is computed as:\r\n\r\n$$ \\textbf{M}\\_{s}\\left(F\\right) = \\sigma\\left(f^{7x7}\\left(\\left[\\text{AvgPool}\\left(F\\right);\\text{MaxPool}\\left(F\\right)\\right]\\right)\\right) $$\r\n\r\n$$ \\textbf{M}\\_{s}\\left(F\\right) = \\sigma\\left(f^{7x7}\\left(\\left[\\mathbf{F}^{s}\\_{avg};\\mathbf{F}^{s}\\_{max} \\right]\\right)\\right) $$\r\n\r\nwhere $\\sigma$ denotes the sigmoid function and $f^{7\u00d77}$ represents a convolution operation with the filter size of 7 \u00d7 7.", "paper": {"title": "CBAM: Convolutional Block Attention Module", "url": "https://paperswithcode.com/paper/cbam-convolutional-block-attention-module"}, "introduced_year": 2000, "source_url": "http://arxiv.org/abs/1807.06521v2", "source_title": "CBAM: Convolutional Block Attention Module", "code_snippet_url": "https://github.com/Jongchan/attention-module/blob/5d3a54af0f6688bedca3f179593dff8da63e8274/MODELS/cbam.py#L72", "num_papers": 135, "id": "pwc:method/spatial-attention-module", "type": "method"}
{"url": "https://paperswithcode.com/method/residual-normal-distribution", "name": "Residual Normal Distribution", "full_name": "Residual Normal Distribution", "description": "**Residual Normal Distributions** are used to help the optimization of VAEs, preventing optimization from entering an unstable region. This can happen due to sharp gradients caused in situations where the encoder and decoder produce distributions far away from each other. The residual distribution parameterizes $q\\left(\\mathbf{z}|\\mathbf{x}\\right)$ relative to $p\\left(\\mathbf{z}\\right)$. Let $p\\left(z^{i}\\_{l}|\\mathbf{z}\\_{<l}\\right) := N \\left(\\mu\\_{i}\\left(\\mathbf{z}\\_{<l}\\right), \\sigma\\_{i}\\left(\\mathbf{z}\\_{<l}\\right)\\right)$ be a Normal distribution for the $i$th variable in $\\mathbf{z}\\_{l}$ in prior. Define $q\\left(z^{i}\\_{l}|\\mathbf{z}\\_{<l}, x\\right) := N\\left(\\mu\\_{i}\\left(\\mathbf{z}\\_{<l}\\right) + \\Delta\\mu\\_{i}\\left(\\mathbf{z}\\_{<l}, x\\right), \\sigma\\_{i}\\left(\\mathbf{z}\\_{<l}\\right) \\cdot \\Delta\\sigma\\_{i}\\left(\\mathbf{z}\\_{<l}, x\\right) \\right)$, where $\\Delta\\mu\\_{i}\\left(\\mathbf{z}\\_{<l}, \\mathbf{x}\\right)$ and $\\Delta\\sigma\\_{i}\\left(\\mathbf{z}\\_{<l}, \\mathbf{x}\\right)$ are the relative location and scale of the approximate posterior with respect to the prior. With this parameterization, when the prior moves, the approximate posterior moves accordingly, if not changed.", "paper": {"title": "NVAE: A Deep Hierarchical Variational Autoencoder", "url": "https://paperswithcode.com/paper/nvae-a-deep-hierarchical-variational"}, "introduced_year": 2000, "source_url": "https://arxiv.org/abs/2007.03898v3", "source_title": "NVAE: A Deep Hierarchical Variational Autoencoder", "code_snippet_url": null, "num_papers": 5, "id": "pwc:method/residual-normal-distribution", "type": "method"}
{"url": "https://paperswithcode.com/method/rbl", "name": "Rank-based Loss", "full_name": "Rank-based loss", "description": "", "paper": {"title": "Rank-based loss for learning hierarchical representations", "url": "https://paperswithcode.com/paper/rank-based-loss-for-learning-hierarchical"}, "introduced_year": 2000, "source_url": "https://arxiv.org/abs/2110.05941v2", "source_title": "Rank-based loss for learning hierarchical representations", "code_snippet_url": "", "num_papers": 2, "id": "pwc:method/rbl", "type": "method"}
{"url": "https://paperswithcode.com/method/deformable-position-sensitive-roi-pooling", "name": "Deformable Position-Sensitive RoI Pooling", "full_name": "Deformable Position-Sensitive RoI Pooling", "description": "**Deformable Position-Sensitive RoI Pooling** is similar to PS RoI Pooling but it adds an offset to each bin position in the regular bin partition. Offset learning follows the \u201cfully convolutional\u201d spirit. In the top branch, a convolutional layer generates the full spatial resolution offset fields. For each RoI (also for each class), PS RoI pooling is applied on such fields to obtain normalized offsets, which are then transformed to the real offsets, in the same way as in deformable RoI pooling.", "paper": {"title": "Deformable Convolutional Networks", "url": "https://paperswithcode.com/paper/deformable-convolutional-networks"}, "introduced_year": 2000, "source_url": "http://arxiv.org/abs/1703.06211v3", "source_title": "Deformable Convolutional Networks", "code_snippet_url": null, "num_papers": 1, "id": "pwc:method/deformable-position-sensitive-roi-pooling", "type": "method"}
{"url": "https://paperswithcode.com/method/disentangled-attribution-curves", "name": "Disentangled Attribution Curves", "full_name": "Disentangled Attribution Curves", "description": "**Disentangled Attribution Curves (DAC)** provide interpretations of tree ensemble methods in the form of (multivariate) feature importance curves. For a given variable, or group of variables, [DAC](https://paperswithcode.com/method/dac) plots the importance of a variable(s) as their value changes.\r\n\r\nThe Figure to the right shows an example. The tree depicts a decision tree which performs binary classification using two features (representing the XOR function). In this problem, knowing the value of one of the features without knowledge of the other feature yields no information - the classifier still has a 50% chance of predicting either class. As a result, DAC produces curves which assign 0 importance to either feature on its own. Knowing both features yields perfect information about the classifier, and thus the DAC curve for both features together correctly shows that the interaction of the features produces the model\u2019s predictions.", "paper": {"title": "Disentangled Attribution Curves for Interpreting Random Forests and Boosted Trees", "url": "https://paperswithcode.com/paper/disentangled-attribution-curves-for"}, "introduced_year": 2000, "source_url": "https://arxiv.org/abs/1905.07631v1", "source_title": "Disentangled Attribution Curves for Interpreting Random Forests and Boosted Trees", "code_snippet_url": "https://github.com/csinva/disentangled-attribution-curves", "num_papers": 1, "id": "pwc:method/disentangled-attribution-curves", "type": "method"}
{"url": "https://paperswithcode.com/method/logistic-regression", "name": "Logistic Regression", "full_name": "Logistic Regression", "description": "**Logistic Regression**, despite its name, is a linear model for classification rather than regression. Logistic regression is also known in the literature as logit regression, maximum-entropy classification (MaxEnt) or the log-linear classifier. In this model, the probabilities describing the possible outcomes of a single trial are modeled using a logistic function.\r\n\r\nSource: [scikit-learn](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression)\r\n\r\nImage: [Michaelg2015](https://commons.wikimedia.org/wiki/User:Michaelg2015)", "paper": null, "introduced_year": 2000, "source_url": null, "source_title": null, "code_snippet_url": null, "num_papers": 1204, "id": "pwc:method/logistic-regression", "type": "method"}
