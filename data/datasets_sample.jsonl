{"url": "https://paperswithcode.com/dataset/mnist", "name": "MNIST", "full_name": "", "homepage": "http://yann.lecun.com/exdb/mnist/", "description": "The **MNIST** database (**Modified National Institute of Standards and Technology** database) is a large collection of handwritten digits. It has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger NIST Special Database 3 (digits written by employees of the United States Census Bureau) and Special Database 1 (digits written by high school students) which contain monochrome images of handwritten digits. The digits have been size-normalized and centered in a fixed-size image. The original black and white (bilevel) images from NIST were size normalized to fit in a 20x20 pixel box while preserving their aspect ratio. The resulting images contain grey levels as a result of the anti-aliasing technique used by the normalization algorithm. the images were centered in a 28x28 image by computing the center of mass of the pixels, and translating the image so as to position this point at the center of the 28x28 field.\r\n\r\nSource: [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)\r\nImage Source: [https://en.wikipedia.org/wiki/MNIST_database#/media/File:MnistExamples.png](https://en.wikipedia.org/wiki/MNIST_database#/media/File:MnistExamples.png)", "paper": {"title": "Gradient-based learning applied to document recognition", "url": "http://arxiv.org/pdf/1102.0183.pdf"}, "introduced_date": null, "warning": null, "modalities": ["Images"], "languages": ["English"], "num_papers": 5821, "data_loaders": [{"url": "https://huggingface.co/datasets/severo/mnist", "repo": "https://github.com/huggingface/datasets", "frameworks": ["tf", "pytorch", "jax"]}, {"url": "https://huggingface.co/datasets/mnist", "repo": "https://github.com/huggingface/datasets", "frameworks": ["tf", "pytorch", "jax"]}, {"url": "https://huggingface.co/datasets/mqddb/test-dataset", "repo": "https://github.com/huggingface/datasets", "frameworks": ["tf", "pytorch", "jax"]}, {"url": "https://huggingface.co/datasets/filwsyl/video_understanding", "repo": "https://github.com/huggingface/datasets", "frameworks": ["tf", "pytorch", "jax"]}, {"url": "https://huggingface.co/datasets/filwsyl/video_tags", "repo": "https://github.com/huggingface/datasets", "frameworks": ["tf", "pytorch", "jax"]}, {"url": "https://pytorch.org/vision/stable/generated/torchvision.datasets.MNIST.html", "repo": "https://github.com/pytorch/vision", "frameworks": ["pytorch"]}, {"url": "https://docs.activeloop.ai/datasets/mnist", "repo": "https://github.com/activeloopai/Hub", "frameworks": ["tf", "pytorch"]}, {"url": "https://www.tensorflow.org/datasets/catalog/mnist", "repo": "https://github.com/tensorflow/datasets", "frameworks": ["tf", "jax"]}, {"url": "https://graphneural.network/datasets/#mnist", "repo": "https://github.com/danielegrattarola/spektral", "frameworks": ["tf"]}, {"url": "https://github.com/open-mmlab/mmclassification/blob/master/docs/getting_started.md", "repo": "https://github.com/open-mmlab/mmclassification", "frameworks": ["pytorch"]}, {"url": "https://gas.graviti.com/dataset/hellodataset/MNIST", "repo": "https://github.com/Graviti-AI/datasets", "frameworks": ["tf", "pytorch"]}, {"url": "https://gitlab.com/afagarap/pt-datasets", "repo": "https://gitlab.com/afagarap/pt-datasets", "frameworks": ["pytorch"]}], "id": "pwc:dataset/mnist", "type": "dataset", "year": -1, "month": -1, "day": -1, "variant_surface_forms": ["USPS-to-MNIST", "MNIST-to-USPS", "Rotating MNIST", "Noisy MNIST (Motion)", "Noisy MNIST (Contrast)", "Noisy MNIST (AWGN)", "MNIST (Conditional)", "Indexed Rotating MNIST", "Rotated MNIST", "Moving MNIST", "Sequential MNIST", "SVNH-to-MNIST", "MNIST-test", "MNIST-full", "75 Superpixel MNIST"]}
{"url": "https://paperswithcode.com/dataset/celeba", "name": "CelebA", "full_name": "CelebFaces Attributes Dataset", "homepage": "http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html", "description": "CelebFaces Attributes dataset contains 202,599 face images of the size 178\u00d7218 from 10,177 celebrities, each annotated with 40 binary labels indicating facial attributes like hair color, gender and age.\r\n\r\nSource: [Show, Attend and Translate: Unpaired Multi-Domain Image-to-Image Translation with Visual Attention](https://arxiv.org/abs/1811.07483)\r\nImage Source: [http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html)", "paper": {"title": "Deep Learning Face Attributes in the Wild", "url": "https://paperswithcode.com/paper/deep-learning-face-attributes-in-the-wild"}, "introduced_date": "2015-01-01", "warning": null, "modalities": ["Images"], "languages": [], "num_papers": 2345, "data_loaders": [{"url": "https://pytorch.org/vision/stable/generated/torchvision.datasets.CelebA.html", "repo": "https://github.com/pytorch/vision", "frameworks": ["pytorch"]}, {"url": "https://docs.activeloop.ai/datasets/celeba-dataset", "repo": "https://github.com/activeloopai/Hub", "frameworks": ["tf", "pytorch"]}, {"url": "https://www.tensorflow.org/datasets/catalog/celeb_a", "repo": "https://github.com/tensorflow/datasets", "frameworks": ["tf", "jax"]}], "id": "pwc:dataset/celeba", "type": "dataset", "year": 2015, "month": 1, "day": 1, "variant_surface_forms": ["CelebA Aligned", "CelebA 64x64", "CelebA 256x256", "CelebA 128 x 128", "CelebA + AFLW Unaligned", "CelebA 128x128"]}
{"url": "https://paperswithcode.com/dataset/jft-300m", "name": "JFT-300M", "full_name": "JFT-300M", "homepage": "", "description": "**JFT-300M** is an internal Google dataset used for training image classification models. Images are labeled using an algorithm that uses complex mixture of raw web signals, connections between web-pages and user feedback. This results in over one billion labels for the 300M images (a single image can have multiple labels). Of the billion image labels, approximately 375M are selected via an algorithm that aims to maximize label precision of selected images.", "paper": {"title": "Revisiting Unreasonable Effectiveness of Data in Deep Learning Era", "url": "https://paperswithcode.com/paper/revisiting-unreasonable-effectiveness-of-data"}, "introduced_date": "2017-07-10", "warning": null, "modalities": ["Images"], "languages": [], "num_papers": 86, "data_loaders": [], "id": "pwc:dataset/jft-300m", "type": "dataset", "year": 2017, "month": 7, "day": 10, "variant_surface_forms": []}
{"url": "https://paperswithcode.com/dataset/glue", "name": "GLUE", "full_name": "General Language Understanding Evaluation benchmark", "homepage": "https://gluebenchmark.com/", "description": "General Language Understanding Evaluation (**GLUE**) benchmark is a collection of nine natural language understanding tasks, including single-sentence tasks CoLA and SST-2, similarity and paraphrasing tasks MRPC, STS-B and QQP, and natural language inference tasks MNLI, QNLI, RTE and WNLI.\r\n\r\nSource: [Align, Mask and Select: A Simple Method for Incorporating Commonsense Knowledge into Language Representation Models](https://arxiv.org/abs/1908.06725)\r\nImage Source: [https://gluebenchmark.com/](https://gluebenchmark.com/)", "paper": {"title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding", "url": "https://paperswithcode.com/paper/glue-a-multi-task-benchmark-and-analysis"}, "introduced_date": "2019-01-01", "warning": null, "modalities": ["Texts"], "languages": ["English"], "num_papers": 1870, "data_loaders": [{"url": "https://huggingface.co/datasets/truezhichu/ttt", "repo": "https://github.com/huggingface/datasets", "frameworks": ["tf", "pytorch", "jax"]}, {"url": "https://huggingface.co/datasets/glue", "repo": "https://github.com/huggingface/datasets", "frameworks": ["tf", "pytorch", "jax"]}, {"url": "https://huggingface.co/datasets/quincyqiang/test", "repo": "https://github.com/huggingface/datasets", "frameworks": ["tf", "pytorch", "jax"]}, {"url": "https://huggingface.co/datasets/severo/glue", "repo": "https://github.com/huggingface/datasets", "frameworks": ["tf", "pytorch", "jax"]}, {"url": "https://huggingface.co/datasets/gsarti/change_it", "repo": "https://github.com/huggingface/datasets", "frameworks": ["tf", "pytorch", "jax"]}, {"url": "https://huggingface.co/datasets/ncats/EpiSet4BinaryClassification", "repo": "https://github.com/huggingface/datasets", "frameworks": ["tf", "pytorch", "jax"]}, {"url": "https://huggingface.co/datasets/vector/structuretest", "repo": "https://github.com/huggingface/datasets", "frameworks": ["tf", "pytorch", "jax"]}, {"url": "https://huggingface.co/datasets/evaluate/glue-ci", "repo": "https://github.com/huggingface/datasets", "frameworks": ["tf", "pytorch", "jax"]}, {"url": "https://parl.ai/docs/tasks.html#glue", "repo": "https://github.com/facebookresearch/ParlAI", "frameworks": ["pytorch"]}, {"url": "https://www.tensorflow.org/datasets/catalog/glue", "repo": "https://github.com/tensorflow/datasets", "frameworks": ["tf", "jax"]}], "id": "pwc:dataset/glue", "type": "dataset", "year": 2019, "month": 1, "day": 1, "variant_surface_forms": ["datasetX", "MNLI-mm", "MNLI-m", "qqp", "STS-B", "SST-2", "FinanceInc/auditor_sentiment_2021", "FinanceInc/auditor_sentiment", "CHANGE-IT", "GLUE STSB", "GLUE SST2", "GLUE RTE", "GLUE QQP", "GLUE QNLI", "GLUE MNLI", "GLUE COLA", "GLUE WNLI", "GLUE MRPC", "RTE", "WNLI", "QNLI", "MultiNLI", "Quora Question Pairs", "STS Benchmark", "MRPC", "SST-2 Binary classification", "CoLA"]}
{"url": "https://paperswithcode.com/dataset/multinli", "name": "MultiNLI", "full_name": "Multi-Genre Natural Language Inference", "homepage": "https://cims.nyu.edu/~sbowman/multinli/", "description": "The **Multi-Genre Natural Language Inference** (**MultiNLI**) dataset has 433K sentence pairs. Its size and mode of collection are modeled closely like [SNLI](/dataset/snli). MultiNLI offers ten distinct genres (Face-to-face, Telephone, 9/11, Travel, Letters, Oxford University Press, Slate, Verbatim, Goverment and Fiction) of written and spoken English data. There are matched dev/test sets which are derived from the same sources as those in the training set, and mismatched sets which do not closely resemble any seen at training time.\r\n\r\nSource: [Semantic Sentence Matching with Densely-connectedRecurrent and Co-attentive Information](https://arxiv.org/abs/1805.11360)", "paper": {"title": "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference", "url": "https://paperswithcode.com/paper/a-broad-coverage-challenge-corpus-for"}, "introduced_date": "2018-01-01", "warning": null, "modalities": ["Texts"], "languages": ["English"], "num_papers": 1240, "data_loaders": [{"url": "https://huggingface.co/datasets/multi_nli", "repo": "https://github.com/huggingface/datasets", "frameworks": ["tf", "pytorch", "jax"]}, {"url": "https://huggingface.co/datasets/multi_nli_mismatch", "repo": "https://github.com/huggingface/datasets", "frameworks": ["tf", "pytorch", "jax"]}, {"url": "https://parl.ai/docs/tasks.html#multinli", "repo": "https://github.com/facebookresearch/ParlAI", "frameworks": ["pytorch"]}, {"url": "https://www.tensorflow.org/datasets/catalog/multi_nli", "repo": "https://github.com/tensorflow/datasets", "frameworks": ["tf", "jax"]}], "id": "pwc:dataset/multinli", "type": "dataset", "year": 2018, "month": 1, "day": 1, "variant_surface_forms": ["MNLI", "MultiNLI-mismatched", "MultiNLI-matched", "multi_nli", "MultiNLI Dev"]}
{"url": "https://paperswithcode.com/dataset/imagenet", "name": "ImageNet", "full_name": "", "homepage": "https://image-net.org/index.php", "description": "The **ImageNet** dataset contains 14,197,122 annotated images according to the WordNet hierarchy. Since 2010 the dataset is used in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC), a benchmark in image classification and object detection.\r\nThe publicly released dataset contains a set of manually annotated training images. A set of test images is also released, with the manual annotations withheld.\r\nILSVRC annotations fall into one of two categories: (1) image-level annotation of a binary label for the presence or absence of an object class in the image, e.g., \u201cthere are cars in this image\u201d but \u201cthere are no tigers,\u201d and (2) object-level annotation of a tight bounding box and class label around an object instance in the image, e.g., \u201cthere is a screwdriver centered at position (20,25) with width of 50 pixels and height of 30 pixels\u201d.\r\nThe ImageNet project does not own the copyright of the images, therefore only thumbnails and URLs of images are provided.\r\n\r\n* Total number of non-empty WordNet synsets: 21841\r\n* Total number of images: 14197122\r\n* Number of images with bounding box annotations: 1,034,908\r\n* Number of synsets with SIFT features: 1000\r\n* Number of images with SIFT features: 1.2 million\r\n\r\nSource: [ImageNet Large Scale Visual Recognition Challenge](https://arxiv.org/abs/1409.0575)\r\nImage Source: [https://cs.stanford.edu/people/karpathy/cnnembed/](https://cs.stanford.edu/people/karpathy/cnnembed/)", "paper": {"title": "ImageNet: A large-scale hierarchical image database", "url": "https://doi.org/10.1109/CVPR.2009.5206848"}, "introduced_date": "2009-01-01", "warning": null, "modalities": ["Images"], "languages": ["English", "Chinese"], "num_papers": 9802, "data_loaders": [{"url": "https://huggingface.co/datasets/imagenet-1k", "repo": "https://github.com/huggingface/datasets", "frameworks": ["tf", "pytorch", "jax"]}, {"url": "https://huggingface.co/datasets/Maysee/tiny-imagenet", "repo": "https://github.com/huggingface/datasets", "frameworks": ["tf", "pytorch", "jax"]}, {"url": "https://pytorch.org/vision/stable/generated/torchvision.datasets.ImageNet.html", "repo": "https://github.com/pytorch/vision", "frameworks": ["pytorch"]}, {"url": "https://docs.activeloop.ai/datasets/imagenet-dataset", "repo": "https://github.com/activeloopai/Hub", "frameworks": ["tf", "pytorch"]}, {"url": "https://www.tensorflow.org/datasets/catalog/imagenet2012", "repo": "https://github.com/tensorflow/datasets", "frameworks": ["tf", "jax"]}, {"url": "https://github.com/open-mmlab/mmclassification/blob/master/docs/getting_started.md", "repo": "https://github.com/open-mmlab/mmclassification", "frameworks": ["pytorch"]}], "id": "pwc:dataset/imagenet", "type": "dataset", "year": 2009, "month": 1, "day": 1, "variant_surface_forms": ["imagenet-1k", "ImageNet V2", "ImageNet 50 samples per class", "ImageNet 512x512", "ImageNet - 500 classes + 10 steps of 50 classes", "ImageNet Detection", "ImageNet-Caltech", "ImageNet-50 (5 tasks) ", "ImageNet-100", "ImageNet ResNet-50 - 90 Epochs", "ImageNet ResNet-50 - 60 Epochs", "ImageNet ResNet-50 - 50 Epochs", "ImageNet - ResNet 50 - 90% sparsity", "ImageNet64x64", "ImageNet 64x64", "ImageNet - 500 classes + 50 steps of 10 classes", "ImageNet - 500 classes + 5 steps of 100 classes", "ImageNet-100 - 50 classes + 50 steps of 1 class", "ImageNet-100 - 50 classes + 5 steps of 10 classes", "ImageNet-100 - 50 classes + 25 steps of 2 classes", "ImageNet-100 - 50 classes + 10 steps of 5 classes", "NAS-Bench-201, ImageNet-16-120", "Imagenet-dog-15", "ImageNet-R", "ImageNet-LT", "ImageNet-A", "ImageNet-10", "ImageNet ReaL", "ImageNet 32x32", "ImageNet 128x128", "ImageNet - 10% labeled data", "ImageNet - 1% labeled data", "ImageNet - 0-Shot", "ImageNet (targeted PGD, max perturbation=16)", "ImageNet (non-targeted PGD, max perturbation=4)", "ImageNet (Fine-grained 6 Tasks)", "ILSVRC 2016", "ILSVRC 2015"]}
{"url": "https://paperswithcode.com/dataset/penn-treebank", "name": "Penn Treebank", "full_name": "", "homepage": "https://catalog.ldc.upenn.edu/docs/LDC95T7/cl93.html", "description": "The English **Penn Treebank** (**PTB**) corpus, and in particular the section of the corpus corresponding to the articles of Wall Street Journal (WSJ), is one of the most known and used corpus for the evaluation of models for sequence labelling. The task consists of annotating each word with its Part-of-Speech tag. In the most common split of this corpus,  sections from 0 to 18 are used for training (38 219 sentences, 912 344 tokens), sections from 19 to 21 are used for validation (5 527 sentences, 131 768 tokens), and sections from 22 to 24 are used for testing (5 462 sentences, 129 654 tokens).\r\nThe corpus is also commonly used for character-level and word-level Language Modelling.\r\n\r\nSource: [Seq2Biseq: Bidirectional Output-wise Recurrent Neural Networks for Sequence Modelling](https://arxiv.org/abs/1904.04733)\r\nImage Source: [https://dl.acm.org/doi/10.5555/972470.972475](https://dl.acm.org/doi/10.5555/972470.972475)", "paper": {"title": "Building a Large Annotated Corpus of English: The Penn Treebank", "url": "http://dl.acm.org/citation.cfm?id=972470.972475"}, "introduced_date": "1993-01-01", "warning": null, "modalities": ["Texts"], "languages": [], "num_papers": 1276, "data_loaders": [{"url": "https://pytorch.org/text/stable/datasets.html#torchtext.datasets.PennTreebank", "repo": "https://github.com/pytorch/text", "frameworks": ["pytorch"]}, {"url": "https://docs.allennlp.org/models/main/models/structured_prediction/dataset_readers/penn_tree_bank/", "repo": "https://github.com/allenai/allennlp-models", "frameworks": ["pytorch"]}], "id": "pwc:dataset/penn-treebank", "type": "dataset", "year": 1993, "month": 1, "day": 1, "variant_surface_forms": ["PTB dataset, ECG lead II", "PTB", "Penn Treebank (Character Level) 3x1000 LSTM - 500 Epochs", "Penn Treebank (Word Level)", "Penn Treebank (Character Level)"]}
{"url": "https://paperswithcode.com/dataset/wikitext-103", "name": "WikiText-103", "full_name": "WikiText-103", "homepage": "https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/", "description": "The WikiText language modeling dataset is a collection of over 100 million tokens extracted from the set of verified Good and Featured articles on Wikipedia. The dataset is available under the Creative Commons Attribution-ShareAlike License.\r\n\r\nCompared to the preprocessed version of Penn Treebank (PTB), WikiText-2 is over 2 times larger and WikiText-103 is over 110 times larger. The WikiText dataset also features a far larger vocabulary and retains the original case, punctuation and numbers - all of which are removed in PTB. As it is composed of full articles, the dataset is well suited for models that can take advantage of long term dependencies.\r\n\r\nSource: [The WikiText Long Term Dependency Language Modeling Dataset](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/)\r\nImage Source: [https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/)", "paper": {"title": "Pointer Sentinel Mixture Models", "url": "https://paperswithcode.com/paper/pointer-sentinel-mixture-models"}, "introduced_date": "2016-09-26", "warning": null, "modalities": ["Texts"], "languages": ["English"], "num_papers": 342, "data_loaders": [{"url": "https://pytorch.org/text/stable/datasets.html#torchtext.datasets.WikiText103", "repo": "https://github.com/pytorch/text", "frameworks": ["pytorch"]}], "id": "pwc:dataset/wikitext-103", "type": "dataset", "year": 2016, "month": 9, "day": 26, "variant_surface_forms": []}
{"url": "https://paperswithcode.com/dataset/lfw", "name": "LFW", "full_name": "Labeled Faces in the Wild", "homepage": "http://vis-www.cs.umass.edu/lfw/", "description": "The **LFW** dataset contains 13,233 images of faces collected from the web. This dataset consists of the 5749 identities with 1680 people with two or more images. In the standard LFW evaluation protocol the verification accuracies are reported on 6000 face pairs.\r\n\r\nSource: [A Performance Evaluation of Loss Functions for Deep Face Recognition](https://arxiv.org/abs/1901.05903)\r\nImage Source: [http://vis-www.cs.umass.edu/lfw](http://vis-www.cs.umass.edu/lfw)", "paper": {"title": "Labeled faces in the wild: A database for studying face recognition in unconstrained environments", "url": "http://vis-www.cs.umass.edu/lfw/lfw.pdf"}, "introduced_date": null, "warning": null, "modalities": ["Images"], "languages": [], "num_papers": 699, "data_loaders": [{"url": "https://pytorch.org/vision/stable/generated/torchvision.datasets.LFWPeople.html", "repo": "https://github.com/pytorch/vision", "frameworks": ["pytorch"]}, {"url": "https://docs.activeloop.ai/datasets/lfw-deep-funneled-dataset", "repo": "https://github.com/activeloopai/Hub", "frameworks": ["tf", "pytorch"]}, {"url": "https://docs.activeloop.ai/datasets/lfw-funneled-dataset", "repo": "https://github.com/activeloopai/Hub", "frameworks": ["tf", "pytorch"]}, {"url": "https://docs.activeloop.ai/datasets/lfw-dataset", "repo": "https://github.com/activeloopai/Hub", "frameworks": ["tf", "pytorch"]}, {"url": "https://www.tensorflow.org/datasets/catalog/lfw", "repo": "https://github.com/tensorflow/datasets", "frameworks": ["tf", "jax"]}], "id": "pwc:dataset/lfw", "type": "dataset", "year": -1, "month": -1, "day": -1, "variant_surface_forms": ["LFW (Online Open Set)"]}
{"url": "https://paperswithcode.com/dataset/wikisql", "name": "WikiSQL", "full_name": "WikiSQL", "homepage": "https://github.com/salesforce/WikiSQL", "description": "**WikiSQL** consists of a corpus of 87,726 hand-annotated SQL query and natural language question pairs. These SQL queries are further split into training (61,297 examples), development (9,145 examples) and test sets (17,284 examples). It can be used for natural language inference tasks related to relational databases.\r\n\r\nSource: [SQL-to-Text Generation with Graph-to-Sequence Model](https://arxiv.org/abs/1809.05255)\r\nImage Source: [https://blog.einstein.ai/how-to-talk-to-your-database/](https://blog.einstein.ai/how-to-talk-to-your-database/)", "paper": {"title": "Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning", "url": "https://paperswithcode.com/paper/seq2sql-generating-structured-queries-from"}, "introduced_date": "2017-01-01", "warning": null, "modalities": ["Texts"], "languages": [], "num_papers": 166, "data_loaders": [{"url": "https://huggingface.co/datasets/wikisql", "repo": "https://github.com/huggingface/datasets", "frameworks": ["tf", "pytorch", "jax"]}, {"url": "https://parl.ai/docs/tasks.html#wikisql-semantic-parsing-task", "repo": "https://github.com/facebookresearch/ParlAI", "frameworks": ["pytorch"]}, {"url": "https://github.com/salesforce/WikiSQL", "repo": "https://github.com/salesforce/WikiSQL", "frameworks": ["none"]}], "id": "pwc:dataset/wikisql", "type": "dataset", "year": 2017, "month": 1, "day": 1, "variant_surface_forms": []}
{"url": "https://paperswithcode.com/dataset/openai-gym", "name": "OpenAI Gym", "full_name": "OpenAI Gym", "homepage": "https://gym.openai.com/", "description": "**OpenAI Gym** is a toolkit for developing and comparing reinforcement learning algorithms. It includes environment such as Algorithmic, Atari, Box2D, Classic Control, MuJoCo, Robotics, and Toy Text.\r\n\r\nSource: [https://github.com/openai/gym](https://github.com/openai/gym)\r\nImage Source: [https://medium.com/@tuzzer/cart-pole-balancing-with-q-learning-b54c6068d947](https://medium.com/@tuzzer/cart-pole-balancing-with-q-learning-b54c6068d947)", "paper": {"title": "OpenAI Gym", "url": "https://paperswithcode.com/paper/openai-gym"}, "introduced_date": "2016-01-01", "warning": null, "modalities": ["Environment"], "languages": ["Azerbaijani"], "num_papers": 989, "data_loaders": [{"url": "https://github.com/openai/gym/blob/master/docs/environments.md", "repo": "https://github.com/openai/gym", "frameworks": ["tf", "pytorch", "jax"]}], "id": "pwc:dataset/openai-gym", "type": "dataset", "year": 2016, "month": 1, "day": 1, "variant_surface_forms": ["Cart Pole (OpenAI Gym)", "Lunar Lander (OpenAI Gym)"]}
{"url": "https://paperswithcode.com/dataset/wikitext-2", "name": "WikiText-2", "full_name": "WikiText-2", "homepage": "https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/", "description": "The WikiText language modeling dataset is a collection of over 100 million tokens extracted from the set of verified Good and Featured articles on Wikipedia. The dataset is available under the Creative Commons Attribution-ShareAlike License.\r\n\r\nCompared to the preprocessed version of Penn Treebank (PTB), WikiText-2 is over 2 times larger and WikiText-103 is over 110 times larger. The WikiText dataset also features a far larger vocabulary and retains the original case, punctuation and numbers - all of which are removed in PTB. As it is composed of full articles, the dataset is well suited for models that can take advantage of long term dependencies.\r\n\r\nSource: [The WikiText Long Term Dependency Language Modeling Dataset](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/)\r\nImage Source: [https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/)", "paper": {"title": "Pointer Sentinel Mixture Models", "url": "https://paperswithcode.com/paper/pointer-sentinel-mixture-models"}, "introduced_date": "2016-09-26", "warning": null, "modalities": ["Texts"], "languages": ["English"], "num_papers": 534, "data_loaders": [{"url": "https://huggingface.co/datasets/wikitext", "repo": "https://github.com/huggingface/datasets", "frameworks": ["tf", "pytorch", "jax"]}, {"url": "https://huggingface.co/datasets/Nart/abkhaz", "repo": "https://github.com/huggingface/datasets", "frameworks": ["tf", "pytorch", "jax"]}, {"url": "https://huggingface.co/datasets/rajeshradhakrishnan/malayalam_wiki", "repo": "https://github.com/huggingface/datasets", "frameworks": ["tf", "pytorch", "jax"]}, {"url": "https://huggingface.co/datasets/meliascosta/wiki_academic_subjects", "repo": "https://github.com/huggingface/datasets", "frameworks": ["tf", "pytorch", "jax"]}, {"url": "https://pytorch.org/text/stable/datasets.html#torchtext.datasets.WikiText2", "repo": "https://github.com/pytorch/text", "frameworks": ["pytorch"]}], "id": "pwc:dataset/wikitext-2", "type": "dataset", "year": 2016, "month": 9, "day": 26, "variant_surface_forms": ["WikiText-103", "wikitext wikitext-2-raw-v1"]}
{"url": "https://paperswithcode.com/dataset/wikilarge", "name": "WikiLarge", "full_name": "", "homepage": "https://github.com/XingxingZhang/dress", "description": "**WikiLarge** comprise 359 test sentences, 2000 development sentences and 300k training sentences. Each source sentences in test set has 8 simplified references\r\n\r\nSource: [Semi-Supervised Text Simplification with Back-Translation and Asymmetric Denoising Autoencoders](https://arxiv.org/abs/2004.14693)\r\nImage Source: [https://arxiv.org/pdf/1904.02767.pdf](https://arxiv.org/pdf/1904.02767.pdf)", "paper": {"title": "Sentence Simplification with Deep Reinforcement Learning", "url": "https://paperswithcode.com/paper/sentence-simplification-with-deep"}, "introduced_date": "2017-01-01", "warning": null, "modalities": ["Texts"], "languages": ["English"], "num_papers": 52, "data_loaders": [{"url": "https://github.com/XingxingZhang/dress", "repo": "https://github.com/XingxingZhang/dress", "frameworks": ["torch"]}], "id": "pwc:dataset/wikilarge", "type": "dataset", "year": 2017, "month": 1, "day": 1, "variant_surface_forms": []}
{"url": "https://paperswithcode.com/dataset/food-101", "name": "Food-101", "full_name": "", "homepage": "https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/", "description": "The  **Food-101** dataset consists of 101 food categories with 750 training and 250 test images per category, making a total of 101k images. The labels for the test images have been manually cleaned, while the training set contains some noise.\r\n\r\nSource: [Combining Weakly and Webly Supervised Learning for Classifying Food Images](https://arxiv.org/abs/1712.08730)\r\nImage Source: [https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/](https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/)", "paper": {"title": "Food-101 - Mining Discriminative Components with Random Forests", "url": "https://doi.org/10.1007/978-3-319-10599-4_29"}, "introduced_date": "2014-01-01", "warning": null, "modalities": ["Images"], "languages": [], "num_papers": 293, "data_loaders": [{"url": "https://huggingface.co/datasets/merty/nateraw-food101-copy", "repo": "https://github.com/huggingface/datasets", "frameworks": ["tf", "pytorch", "jax"]}, {"url": "https://huggingface.co/datasets/nateraw/food101", "repo": "https://github.com/huggingface/datasets", "frameworks": ["tf", "pytorch", "jax"]}, {"url": "https://huggingface.co/datasets/nateraw/sync_food101", "repo": "https://github.com/huggingface/datasets", "frameworks": ["tf", "pytorch", "jax"]}, {"url": "https://huggingface.co/datasets/nateraw/food101_old", "repo": "https://github.com/huggingface/datasets", "frameworks": ["tf", "pytorch", "jax"]}, {"url": "https://huggingface.co/datasets/food101", "repo": "https://github.com/huggingface/datasets", "frameworks": ["tf", "pytorch", "jax"]}, {"url": "https://pytorch.org/vision/stable/generated/torchvision.datasets.Food101.html", "repo": "https://github.com/pytorch/vision", "frameworks": ["pytorch"]}, {"url": "https://docs.activeloop.ai/datasets/food-101-dataset", "repo": "https://github.com/activeloopai/Hub", "frameworks": ["tf", "pytorch"]}, {"url": "https://www.tensorflow.org/datasets/catalog/food101", "repo": "https://github.com/tensorflow/datasets", "frameworks": ["tf", "jax"]}], "id": "pwc:dataset/food-101", "type": "dataset", "year": 2014, "month": 1, "day": 1, "variant_surface_forms": ["Food-101N", "food101"]}
{"url": "https://paperswithcode.com/dataset/fashion-mnist", "name": "Fashion-MNIST", "full_name": "", "homepage": "https://github.com/zalandoresearch/fashion-mnist", "description": "**Fashion-MNIST** is a dataset comprising of 28\u00d728 grayscale images of 70,000 fashion products from 10 categories, with 7,000 images per category. The training set has 60,000 images and the test set has 10,000 images. Fashion-MNIST shares the same image size, data format and the structure of training and testing splits with the original MNIST.\r\n\r\nSource: [Generative Probabilistic Novelty Detection with Adversarial Autoencoders](https://arxiv.org/abs/1807.02588)\r\nImage Source: [https://github.com/zalandoresearch/fashion-mnist](https://github.com/zalandoresearch/fashion-mnist)", "paper": {"title": "Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms", "url": "https://paperswithcode.com/paper/fashion-mnist-a-novel-image-dataset-for"}, "introduced_date": "2017-01-01", "warning": null, "modalities": ["Images"], "languages": [], "num_papers": 2102, "data_loaders": [{"url": "https://huggingface.co/datasets/fashion_mnist", "repo": "https://github.com/huggingface/datasets", "frameworks": ["tf", "pytorch", "jax"]}, {"url": "https://pytorch.org/vision/stable/generated/torchvision.datasets.FashionMNIST.html", "repo": "https://github.com/pytorch/vision", "frameworks": ["pytorch"]}, {"url": "https://github.com/zalandoresearch/fashion-mnist", "repo": "https://github.com/zalandoresearch/fashion-mnist", "frameworks": ["tf"]}, {"url": "https://docs.activeloop.ai/datasets/fashion-mnist-dataset", "repo": "https://github.com/activeloopai/Hub", "frameworks": ["tf", "pytorch"]}, {"url": "https://www.tensorflow.org/datasets/catalog/fashion_mnist", "repo": "https://github.com/tensorflow/datasets", "frameworks": ["tf", "jax"]}, {"url": "https://app.layer.ai/layer/fashion_mnist", "repo": "https://github.com/layerai/sdk", "frameworks": ["tf", "pytorch"]}, {"url": "https://gas.graviti.com/dataset/graviti/FashionMNIST", "repo": "https://github.com/Graviti-AI/datasets", "frameworks": ["tf", "pytorch"]}, {"url": "https://gitlab.com/afagarap/pt-datasets", "repo": "https://gitlab.com/afagarap/pt-datasets", "frameworks": ["pytorch"]}], "id": "pwc:dataset/fashion-mnist", "type": "dataset", "year": 2017, "month": 1, "day": 1, "variant_surface_forms": ["Rotated Fashion-MNIST", "fashion_mnist"]}
{"url": "https://paperswithcode.com/dataset/shapenet", "name": "ShapeNet", "full_name": "", "homepage": "https://www.shapenet.org/", "description": "**ShapeNet** is a large scale repository for 3D CAD models developed by researchers from Stanford University, Princeton University and the Toyota Technological Institute at Chicago, USA. The repository contains over 300M models with 220,000 classified into 3,135 classes arranged using WordNet hypernym-hyponym relationships. ShapeNet Parts subset contains 31,693 meshes categorised into 16 common object classes (i.e. table, chair, plane etc.). Each shapes ground truth contains 2-5 parts (with a total of 50 part classes).\r\n\r\nSource: [A review on deep learning techniques for 3D sensed data classification](https://arxiv.org/abs/1907.04444)\r\nImage Source: [ShapeNet: An Information-Rich 3D Model Repository](https://arxiv.org/abs/1512.03012)", "paper": {"title": "ShapeNet: An Information-Rich 3D Model Repository", "url": "https://paperswithcode.com/paper/shapenet-an-information-rich-3d-model"}, "introduced_date": "2015-01-01", "warning": null, "modalities": ["Images", "3D", "Point cloud"], "languages": [], "num_papers": 1178, "data_loaders": [], "id": "pwc:dataset/shapenet", "type": "dataset", "year": 2015, "month": 1, "day": 1, "variant_surface_forms": ["ShapeNet Car", "ShapeNet Chair", "ShapeNet Airplane"]}
{"url": "https://paperswithcode.com/dataset/cinic-10", "name": "CINIC-10", "full_name": "CINIC-10", "homepage": "https://github.com/BayesWatch/cinic-10", "description": "**CINIC-10** is a dataset for image classification. It has a total of 270,000 images, 4.5 times that of CIFAR-10. It is constructed from two different sources: ImageNet and CIFAR-10. Specifically, it was compiled as a bridge between CIFAR-10 and ImageNet. It is split into three equal subsets - train, validation, and test - each of which contain 90,000 images.\r\n\r\nSource: [Group Knowledge Transfer:Collaborative Training of Large CNNs on the Edge](https://arxiv.org/abs/2007.14513)\r\nImage Source: [https://arxiv.org/abs/1810.03505](https://arxiv.org/abs/1810.03505)", "paper": {"title": "CINIC-10 is not ImageNet or CIFAR-10", "url": "https://paperswithcode.com/paper/cinic-10-is-not-imagenet-or-cifar-10"}, "introduced_date": "2018-01-01", "warning": null, "modalities": ["Images"], "languages": [], "num_papers": 89, "data_loaders": [{"url": "https://github.com/BayesWatch/cinic-10", "repo": "https://github.com/BayesWatch/cinic-10", "frameworks": ["pytorch"]}], "id": "pwc:dataset/cinic-10", "type": "dataset", "year": 2018, "month": 1, "day": 1, "variant_surface_forms": []}
{"url": "https://paperswithcode.com/dataset/flickr30k", "name": "Flickr30k", "full_name": "Flickr30k", "homepage": "https://shannon.cs.illinois.edu/DenotationGraph/", "description": "The **Flickr30k** dataset contains 31,000 images collected from Flickr, together with 5 reference sentences provided by human annotators.\r\n\r\nSource: [Guiding Long-Short Term Memory for Image Caption Generation](https://arxiv.org/abs/1509.04942)\r\n\r\nImage Source: [Dual-Path Convolutional Image-Text Embedding with Instance Loss\r\n](https://arxiv.org/abs/1711.05535)", "paper": {"title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions", "url": "https://paperswithcode.com/paper/from-image-descriptions-to-visual-denotations"}, "introduced_date": "2014-01-01", "warning": null, "modalities": ["Images", "Texts"], "languages": ["English"], "num_papers": 503, "data_loaders": [{"url": "https://pytorch.org/vision/stable/generated/torchvision.datasets.Flickr30k.html", "repo": "https://github.com/pytorch/vision", "frameworks": ["pytorch"]}, {"url": "https://parl.ai/docs/tasks.html#flickr30k", "repo": "https://github.com/facebookresearch/ParlAI", "frameworks": ["pytorch"]}, {"url": "https://docs.activeloop.ai/datasets/flickr30k-dataset", "repo": "https://github.com/activeloopai/Hub", "frameworks": ["tf", "pytorch"]}], "id": "pwc:dataset/flickr30k", "type": "dataset", "year": 2014, "month": 1, "day": 1, "variant_surface_forms": ["Flickr", "Flickr30k Captions test", "Flickr30K 1K test"]}
{"url": "https://paperswithcode.com/dataset/coco", "name": "COCO", "full_name": "Microsoft Common Objects in Context", "homepage": "https://cocodataset.org/", "description": "The MS **COCO** (**Microsoft Common Objects in Context**) dataset is a large-scale object detection, segmentation, key-point detection, and captioning dataset. The dataset consists of 328K images.\r\n\r\n**Splits:**\r\nThe first version of MS COCO dataset was released in 2014. It contains 164K images split into training (83K), validation (41K) and test (41K) sets. In 2015 additional test set of 81K images was released, including all the previous test images and 40K new images.\r\n\r\nBased on community feedback, in 2017 the training/validation split was changed from 83K/41K to 118K/5K. The new split uses the same images and annotations. The 2017 test set is a subset of 41K images of the 2015 test set. Additionally, the 2017 release contains a new unannotated dataset of 123K images.\r\n\r\n**Annotations:**\r\nThe dataset has annotations for\r\n\r\n* object detection: bounding boxes and per-instance segmentation masks with 80 object categories,\r\n* captioning: natural language descriptions of the images (see MS COCO Captions),\r\n* keypoints detection: containing more than 200,000 images and 250,000 person instances labeled with keypoints (17 possible keypoints, such as left eye, nose, right hip, right ankle),\r\n* stuff image segmentation \u2013 per-pixel segmentation masks with 91 stuff categories, such as grass, wall, sky (see MS COCO Stuff),\r\n* panoptic: full scene segmentation, with 80 thing categories (such as person, bicycle, elephant) and a subset of 91 stuff categories (grass, sky, road),\r\n* dense pose: more than 39,000 images and 56,000 person instances labeled with DensePose annotations \u2013 each labeled person is annotated with an instance id and a mapping between image pixels that belong to that person body and a template 3D model.\r\nThe annotations are publicly available only for training and validation images.\r\n\r\nSource: [https://cocodataset.org/](https://cocodataset.org/)\r\nImage Source: [https://cocodataset.org/](https://cocodataset.org/)", "paper": {"title": "Microsoft COCO: Common Objects in Context", "url": "https://paperswithcode.com/paper/microsoft-coco-common-objects-in-context"}, "introduced_date": "2014-01-01", "warning": null, "modalities": ["Images", "Videos"], "languages": ["English"], "num_papers": 7007, "data_loaders": [{"url": "https://detectron2.readthedocs.io/en/latest/tutorials/builtin_datasets.html#expected-dataset-structure-for-coco-instance-keypoint-detection", "repo": "https://github.com/facebookresearch/detectron2", "frameworks": ["pytorch"]}, {"url": "https://github.com/open-mmlab/mmdetection/blob/master/docs/1_exist_data_model.md", "repo": "https://github.com/open-mmlab/mmdetection", "frameworks": ["pytorch"]}, {"url": "https://pytorch.org/vision/stable/datasets.html#torchvision.datasets.CocoDetection", "repo": "https://github.com/pytorch/vision", "frameworks": ["pytorch"]}, {"url": "https://docs.activeloop.ai/datasets/coco-dataset", "repo": "https://github.com/activeloopai/Hub", "frameworks": ["tf", "pytorch"]}, {"url": "https://www.tensorflow.org/datasets/catalog/coco", "repo": "https://github.com/tensorflow/datasets", "frameworks": ["tf", "jax"]}, {"url": "https://github.com/open-mmlab/mmpose/blob/master/docs/tasks/2d_body_keypoint.md#coco", "repo": "https://github.com/open-mmlab/mmpose", "frameworks": ["pytorch"]}], "id": "pwc:dataset/coco", "type": "dataset", "year": 2014, "month": 1, "day": 1, "variant_surface_forms": ["COCO 2017 (Sports, Food)", "COCO 2017 (Outdoor, Accessories, Appliance, Truck)", "COCO 2017 (Electronic, Indoor, Kitchen, Furniture)", "COCO Visual Question Answering (VQA) abstract images 1.0 open ended", "COCO Visual Question Answering (VQA) abstract 1.0 multiple choice", "COCO 2017", "COCO 2014", "coco minval", "COCO-Stuff-3", "COCO-Stuff 256x256", "COCO 256 x 256", "COCO 2015", "MS-COCO (10-shot)", "MS-COCO", "COCO Visual Question Answering (VQA) real images 2.0 open ended", "COCO Visual Question Answering (VQA) real images 1.0 open ended", "MSCOCO", "DensePose-COCO", "COCO_20k", "COCO-Animals", "COCO+", "COCO test-dev", "COCO test-challenge", "COCO panoptic", "COCO minival", "COCO count-test", "COCO Visual Question Answering (VQA) real images 1.0 multiple choice", "COCO (image as query)"]}
{"url": "https://paperswithcode.com/dataset/bsds500", "name": "BSDS500", "full_name": "Berkeley Segmentation Dataset 500", "homepage": "https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/resources.html", "description": "Berkeley Segmentation Data Set 500 (**BSDS500**) is a standard benchmark for contour detection. This dataset is designed for evaluating natural edge detection that includes not only object contours but also object interior boundaries and background boundaries. It includes 500 natural images with carefully annotated boundaries collected from multiple users. The dataset is divided into three parts: 200 for training, 100 for validation and the rest 200 for test.\r\n\r\nSource: [Object Contour Detection with a Fully Convolutional Encoder-Decoder Network](https://arxiv.org/abs/1603.04530)\r\nImage Source: [https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/resources.html](https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/resources.html)", "paper": {"title": "Contour Detection and Hierarchical Image Segmentation", "url": "https://doi.org/10.1109/TPAMI.2010.161"}, "introduced_date": "2011-01-01", "warning": null, "modalities": ["Point cloud"], "languages": [], "num_papers": 208, "data_loaders": [{"url": "https://github.com/mengyangpu/edter", "repo": "https://github.com/mengyangpu/edter", "frameworks": ["pytorch"]}], "id": "pwc:dataset/bsds500", "type": "dataset", "year": 2011, "month": 1, "day": 1, "variant_surface_forms": ["BSDS500 (Quality 30 Grayscale)", "BSDS500 (Quality 30 Color)", "BSDS500 (Quality 20 Grayscale)", "BSDS500 (Quality 20 Color)", "BSDS500 (Quality 10 Grayscale)", "BSDS500 (Quality 10 Color)"]}
{"url": "https://paperswithcode.com/dataset/mhp", "name": "MHP", "full_name": "Multiple-Human Parsing", "homepage": "https://arxiv.org/pdf/1705.07206.pdf", "description": "The MHP dataset contains multiple persons captured in real-world scenes with pixel-level fine-grained semantic annotations in an instance-aware setting.\r\n\r\nSource: [Multiple-Human Parsing in the Wild](https://arxiv.org/pdf/1705.07206)\r\nImage Source: [Li et al](https://arxiv.org/pdf/1705.07206.pdf)", "paper": {"title": "Multiple-Human Parsing in the Wild", "url": "https://paperswithcode.com/paper/multiple-human-parsing-in-the-wild"}, "introduced_date": "2017-05-19", "warning": null, "modalities": ["Images"], "languages": [], "num_papers": 11, "data_loaders": [{"url": "https://github.com/open-mmlab/mmpose/blob/master/docs/tasks/2d_body_keypoint.md#mhp", "repo": "https://github.com/open-mmlab/mmpose", "frameworks": ["pytorch"]}], "id": "pwc:dataset/mhp", "type": "dataset", "year": 2017, "month": 5, "day": 19, "variant_surface_forms": ["MHP v2.0", "MHP v1.0"]}
{"url": "https://paperswithcode.com/dataset/lrw", "name": "LRW", "full_name": "Lip Reading in the Wild", "homepage": "https://www.robots.ox.ac.uk/~vgg/data/lip_reading/lrw1.html", "description": "The **Lip Reading in the Wild** (**LRW**) dataset  a large-scale audio-visual database that contains 500 different words from over 1,000 speakers. Each utterance has 29 frames, whose boundary is centered around the target word. The database is divided into training, validation and test sets. The training set contains at least 800 utterances for each class while the validation and test sets contain 50 utterances.\r\n\r\nSource: [Towards Pose-invariant Lip-Reading](https://arxiv.org/abs/1911.06095)\r\nImage Source: [https://www.robots.ox.ac.uk/~vgg/data/lip_reading/lrw1.html](https://www.robots.ox.ac.uk/~vgg/data/lip_reading/lrw1.html)", "paper": {"title": "Lip Reading in the Wild", "url": "https://doi.org/10.1007/978-3-319-54184-6_6"}, "introduced_date": "2016-01-01", "warning": null, "modalities": ["Videos", "Texts", "Audio"], "languages": [], "num_papers": 131, "data_loaders": [], "id": "pwc:dataset/lrw", "type": "dataset", "year": 2016, "month": 1, "day": 1, "variant_surface_forms": ["Lipreading in the Wild"]}
{"url": "https://paperswithcode.com/dataset/fddb", "name": "FDDB", "full_name": "Face Detection Dataset and Benchmark", "homepage": "http://vis-www.cs.umass.edu/fddb/", "description": "The **Face Detection Dataset and Benchmark** (**FDDB**) dataset is a collection of labeled faces from Faces in the Wild dataset. It contains a total of 5171 face annotations, where images are also of various resolution, e.g. 363x450 and 229x410. The dataset incorporates a range of challenges, including difficult pose angles, out-of-focus faces and low resolution. Both greyscale and color images are included.\r\n\r\nSource: [A Comparison of CNN-based Face and Head Detectors for Real-Time Video Surveillance Applications](https://arxiv.org/abs/1809.03336)", "paper": {"title": "Fddb: A benchmark for face detection in unconstrained settings", "url": "http://vis-www.cs.umass.edu/fddb/fddb.pdf"}, "introduced_date": null, "warning": null, "modalities": ["Images"], "languages": [], "num_papers": 155, "data_loaders": [], "id": "pwc:dataset/fddb", "type": "dataset", "year": -1, "month": -1, "day": -1, "variant_surface_forms": []}
{"url": "https://paperswithcode.com/dataset/got-10k", "name": "GOT-10k", "full_name": "Generic Object Tracking Benchmark", "homepage": "http://got-10k.aitestunion.com/", "description": "The **GOT-10k** dataset contains more than 10,000 video segments of real-world moving objects and over 1.5 million manually labelled bounding boxes. The dataset contains more than 560 classes of real-world moving objects and 80+ classes of motion patterns.\r\n\r\nSource: [http://got-10k.aitestunion.com/](http://got-10k.aitestunion.com/)\r\nImage Source: [https://arxiv.org/pdf/1810.11981.pdf](https://arxiv.org/pdf/1810.11981.pdf)", "paper": {"title": "GOT-10k: A Large High-Diversity Benchmark for Generic Object Tracking in the Wild", "url": "https://paperswithcode.com/paper/got-10k-a-large-high-diversity-benchmark-for"}, "introduced_date": "2018-01-01", "warning": null, "modalities": ["Images", "Videos"], "languages": [], "num_papers": 140, "data_loaders": [], "id": "pwc:dataset/got-10k", "type": "dataset", "year": 2018, "month": 1, "day": 1, "variant_surface_forms": []}
{"url": "https://paperswithcode.com/dataset/kitti", "name": "KITTI", "full_name": "", "homepage": "http://www.cvlibs.net/datasets/kitti/", "description": "**KITTI** (Karlsruhe Institute of Technology and Toyota Technological Institute) is one of the most popular datasets for use in mobile robotics and autonomous driving. It consists of hours of traffic scenarios recorded with a variety of sensor modalities, including high-resolution RGB, grayscale stereo cameras, and a 3D laser scanner. Despite its popularity, the dataset itself does not contain ground truth for semantic segmentation. However, various researchers have manually annotated parts of the dataset to fit their necessities. [\u00c1lvarez et al.](http://yann.lecun.com/exdb/publis/pdf/alvarez-eccv-12.pdf) generated ground truth for 323 images from the road detection challenge with three classes: road, vertical, and sky. [Zhang et al.](http://www-video.eecs.berkeley.edu/papers/rzhang/zhang-icra-submission.pdf) annotated 252 (140 for training and 112 for testing) acquisitions \u2013 RGB and Velodyne scans \u2013 from the tracking challenge for ten object categories: building, sky, road, vegetation, sidewalk, car, pedestrian, cyclist, sign/pole, and fence. [Ros et al.](http://refbase.cvc.uab.es/files/rrg2015.pdf) labeled 170 training images and 46 testing images (from the visual odometry challenge) with 11 classes: building, tree, sky, car, sign, road, pedestrian, fence, pole, sidewalk, and bicyclist.\r\n\r\nSource: [A Review on Deep Learning Techniques Applied to Semantic Segmentation](https://arxiv.org/abs/1704.06857)\r\nImage Source: [http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d](http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d)", "paper": {"title": "Are we ready for autonomous driving? The KITTI vision benchmark suite", "url": "https://doi.org/10.1109/CVPR.2012.6248074"}, "introduced_date": "2012-01-01", "warning": null, "modalities": ["Images"], "languages": ["English"], "num_papers": 2412, "data_loaders": [{"url": "https://pytorch.org/vision/stable/generated/torchvision.datasets.KittiFlow.html", "repo": "https://github.com/pytorch/vision", "frameworks": ["pytorch"]}, {"url": "https://www.tensorflow.org/datasets/catalog/kitti", "repo": "https://github.com/tensorflow/datasets", "frameworks": ["tf", "jax"]}, {"url": "https://github.com/open-mmlab/mmdetection3d/blob/master/docs/data_preparation.md", "repo": "https://github.com/open-mmlab/mmdetection3d", "frameworks": ["pytorch"]}], "id": "pwc:dataset/kitti", "type": "dataset", "year": 2012, "month": 1, "day": 1, "variant_surface_forms": ["2019_test set", "KITTI (FCGF setting)", "2D KITTI Cars Moderate", "KITTI 2015 Scene Flow Test", "KITTI 2015 Scene Flow Training", "KITTI Novel View Synthesis", "KITTI 2015 - unsupervised", "KITTI 2012 - unsupervised", "KITTI Pedestrians Moderate val", "KITTI Pedestrian Hard", "KITTI Panoptic Segmentation", "KITTI Object Tracking Evaluation 2012", "KITTI Horizon", "KITTI Depth Completion Eigen Split", "KITTI Tracking test", "KITTI2015", "KITTI 2015 unsupervised", "KITTI 2015 - 4x upscaling", "KITTI 2015 - 2x upscaling", "KITTI 2015", "KITTI2012", "KITTI 2012 unsupervised", "KITTI 2012 - 4x upscaling", "KITTI 2012 - 2x upscaling", "KITTI 2012", "PreSIL to KITTI", "KITTI Semantic Segmentation", "KITTI Pedestrians Moderate", "KITTI Pedestrians Hard", "KITTI Pedestrians Easy", "KITTI Pedestrian Moderate val", "KITTI Pedestrian Hard val", "KITTI Pedestrian Easy val", "KITTI Eigen split unsupervised", "KITTI Eigen split", "KITTI Depth Completion", "KITTI Cyclists Moderate", "KITTI Cyclists Hard", "KITTI Cyclists Easy", "KITTI Cyclist Moderate val", "KITTI Cyclist Hard val", "KITTI Cyclist Easy val", "KITTI Cars Moderate val", "KITTI Cars Moderate", "KITTI Cars Hard val", "KITTI Cars Hard", "KITTI Cars Easy val", "KITTI Cars Easy"]}
{"url": "https://paperswithcode.com/dataset/ucf101", "name": "UCF101", "full_name": "UCF101 Human Actions dataset", "homepage": "https://www.crcv.ucf.edu/data/UCF101.php", "description": "**UCF101** dataset is an extension of UCF50 and consists of 13,320 video clips, which are classified into 101 categories. These 101 categories can be classified into 5 types (Body motion, Human-human interactions, Human-object interactions, Playing musical instruments and Sports). The total length of these video clips is over 27 hours. All the videos are collected from YouTube and have a fixed frame rate of 25 FPS with the resolution of 320 \u00d7 240.\r\n\r\nSource: [Two-stream Collaborative Learning with Spatial-Temporal Attention for Video Classification](https://arxiv.org/abs/1711.03273)\r\nImage Source: [https://www.crcv.ucf.edu/data/UCF101.php](https://www.crcv.ucf.edu/data/UCF101.php)", "paper": {"title": "UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild", "url": "https://paperswithcode.com/paper/ucf101-a-dataset-of-101-human-actions-classes"}, "introduced_date": "2012-12-03", "warning": null, "modalities": ["Videos"], "languages": [], "num_papers": 1246, "data_loaders": [{"url": "https://pytorch.org/vision/stable/generated/torchvision.datasets.UCF101.html", "repo": "https://github.com/pytorch/vision", "frameworks": ["pytorch"]}, {"url": "https://www.tensorflow.org/datasets/catalog/ucf101", "repo": "https://github.com/tensorflow/datasets", "frameworks": ["tf", "jax"]}, {"url": "https://github.com/open-mmlab/mmaction2/blob/master/tools/data/ucf101/README.md", "repo": "https://github.com/open-mmlab/mmaction2", "frameworks": ["pytorch"]}], "id": "pwc:dataset/ucf101", "type": "dataset", "year": 2012, "month": 12, "day": 3, "variant_surface_forms": ["UCF-HMDB", "UCF101-24", "UCF-101 16 frames, Unconditional, Single GPU", "UCF-101 16 frames, 64x64, Unconditional", "UCF-101 16 frames, 128x128, Unconditional", "UCF-101"]}
{"url": "https://paperswithcode.com/dataset/hmdb51", "name": "HMDB51", "full_name": "", "homepage": "https://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database", "description": "The **HMDB51** dataset is a large collection of realistic videos from various sources, including movies and web videos. The dataset is composed of 6,766 video clips from 51 action categories (such as \u201cjump\u201d, \u201ckiss\u201d and \u201claugh\u201d), with each category containing at least 101 clips. The original evaluation scheme uses three different training/testing splits. In each split, each action class has 70 clips for training and 30 clips for testing. The average accuracy over these three splits is used to measure the final performance.\r\n\r\nSource: [Action Recognition with Trajectory-Pooled Deep-Convolutional Descriptors](https://arxiv.org/abs/1505.04868)\r\nImage Source: [https://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database](https://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database)", "paper": {"title": "HMDB: A large video database for human motion recognition", "url": "https://doi.org/10.1109/ICCV.2011.6126543"}, "introduced_date": "2011-01-01", "warning": null, "modalities": ["Videos"], "languages": [], "num_papers": 627, "data_loaders": [{"url": "https://pytorch.org/vision/stable/generated/torchvision.datasets.HMDB51.html", "repo": "https://github.com/pytorch/vision", "frameworks": ["pytorch"]}, {"url": "https://docs.activeloop.ai/datasets/hmdb51-dataset", "repo": "https://github.com/activeloopai/Hub", "frameworks": ["tf", "pytorch"]}, {"url": "https://github.com/open-mmlab/mmaction2/blob/master/tools/data/hmdb51/README.md", "repo": "https://github.com/open-mmlab/mmaction2", "frameworks": ["pytorch"]}], "id": "pwc:dataset/hmdb51", "type": "dataset", "year": 2011, "month": 1, "day": 1, "variant_surface_forms": ["HMDB-UCF", "UCF-to-HMDBsmall", "UCF-to-HMDBfull", "UCF --> HMDB (full)", "Olympic-to-HMDBsmall", "HMDBsmall-to-UCF", "HMDBfull-to-UCF", "HMDB --> UCF (full)", "HMDB-51"]}
{"url": "https://paperswithcode.com/dataset/mimic-iii", "name": "MIMIC-III", "full_name": "The Medical Information Mart for Intensive Care III", "homepage": "https://mimic.physionet.org/", "description": "The Medical Information Mart for Intensive Care III (**MIMIC-III**) dataset is a large, de-identified and publicly-available collection of medical records. Each record in the dataset includes ICD-9 codes, which identify diagnoses and procedures performed. Each code is partitioned into sub-codes, which often include specific circumstantial details. The dataset consists of 112,000 clinical reports records (average length 709.3 tokens) and 1,159 top-level ICD-9 codes. Each report is assigned to 7.6 codes, on average. Data includes vital signs, medications, laboratory measurements, observations and notes charted by care providers, fluid balance, procedure codes, diagnostic codes, imaging reports, hospital length of stay, survival data, and more. \r\n\r\nThe database supports applications including academic and industrial research, quality improvement initiatives, and higher education coursework.\r\n\r\nSource: [MIT Laboratory for Computational Biology](https://imes.mit.edu/supporting-clinical-research-with-the-mimic-iii-critical-care-database/)", "paper": {"title": "MIMIC-III, a freely accessible critical care database", "url": "https://paperswithcode.com/paper/mimic-iii-a-freely-accessible-critical-care"}, "introduced_date": "2016-05-24", "warning": null, "modalities": ["Medical", "Tabular"], "languages": [], "num_papers": 632, "data_loaders": [{"url": "https://github.com/moustafa100/Data-Science-Advanced-Analytics", "repo": "https://github.com/moustafa100/Data-Science-Advanced-Analytics", "frameworks": ["tf"]}], "id": "pwc:dataset/mimic-iii", "type": "dataset", "year": 2016, "month": 5, "day": 24, "variant_surface_forms": []}
{"url": "https://paperswithcode.com/dataset/tid2013", "name": "TID2013", "full_name": "TID2013", "homepage": "http://www.ponomarenko.info/tid2013.htm", "description": "**TID2013** is a dataset for image quality assessment that contains 25 reference images and 3000 distorted images (25 reference images x 24 types of distortions x 5 levels of distortions). \r\n\r\nSource: [Lukin et al](https://www.researchgate.net/figure/Reference-images-in-databases-TID2008-and-TID2013_fig3_268038378)", "paper": null, "introduced_date": null, "warning": null, "modalities": ["Images"], "languages": [], "num_papers": 6, "data_loaders": [], "id": "pwc:dataset/tid2013", "type": "dataset", "year": -1, "month": -1, "day": -1, "variant_surface_forms": []}
{"url": "https://paperswithcode.com/dataset/live", "name": "LIVE", "full_name": "Laboratory for Image & Video Engineering", "homepage": "http://live.ece.utexas.edu/research/Quality/", "description": "Briefly describe the dataset. Provide:\r\n\r\n* a high-level explanation of the dataset characteristics\r\n* explain motivations and summary of its content\r\n* potential use cases of the dataset\r\n\r\nIf the description or image is from a different paper, please refer to it as follows:\r\nSource: [title](url)\r\nImage Source: [title](url)", "paper": null, "introduced_date": null, "warning": null, "modalities": [], "languages": [], "num_papers": 2, "data_loaders": [], "id": "pwc:dataset/live", "type": "dataset", "year": -1, "month": -1, "day": -1, "variant_surface_forms": []}
{"url": "https://paperswithcode.com/dataset/visual-genome", "name": "Visual Genome", "full_name": "", "homepage": "https://visualgenome.org/", "description": "**Visual Genome** contains Visual Question Answering data in a multi-choice setting. It consists of 101,174 images from MSCOCO with 1.7 million QA pairs, 17 questions per image on average. Compared to the Visual Question Answering dataset, Visual Genome represents a more balanced distribution over 6 question types: What, Where, When, Who, Why and How. The Visual Genome dataset also presents 108K images with densely annotated objects, attributes and relationships.\r\n\r\nSource: [RaAM: A Relation-aware Attention Model for Visual Question Answering](https://arxiv.org/abs/1903.12314)\r\nImage Source: [Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations](https://paperswithcode.com/paper/visual-genome-connecting-language-and-vision/)", "paper": {"title": "Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations", "url": "https://paperswithcode.com/paper/visual-genome-connecting-language-and-vision"}, "introduced_date": "2017-01-01", "warning": null, "modalities": ["Images", "Texts"], "languages": [], "num_papers": 858, "data_loaders": [{"url": "https://huggingface.co/datasets/visual_genome", "repo": "https://github.com/huggingface/datasets", "frameworks": ["tf", "pytorch", "jax"]}], "id": "pwc:dataset/visual-genome", "type": "dataset", "year": 2017, "month": 1, "day": 1, "variant_surface_forms": ["VG graph-text", "Visual Genome 256x256", "Visual Genome 64x64", "Visual Genome 128x128", "Visual Genome (subjects)", "Visual Genome (pairs)"]}
{"url": "https://paperswithcode.com/dataset/coco-stuff", "name": "COCO-Stuff", "full_name": "Common Objects in COntext-stuff", "homepage": "https://github.com/nightrome/cocostuff", "description": "The **Common Objects in COntext-stuff** (COCO-stuff) dataset is a dataset for scene understanding tasks like semantic segmentation, object detection and image captioning. It is constructed by annotating the original COCO dataset, which originally annotated things while neglecting stuff annotations. There are 164k images in COCO-stuff dataset that span over 172 categories including 80 things, 91 stuff, and 1 unlabeled class.\r\n\r\nSource: [Image Colorization: A Survey and Dataset](https://arxiv.org/abs/2008.10774)\r\nImage Source: [https://github.com/nightrome/cocostuff](https://github.com/nightrome/cocostuff)", "paper": {"title": "COCO-Stuff: Thing and Stuff Classes in Context", "url": "https://paperswithcode.com/paper/coco-stuff-thing-and-stuff-classes-in-context"}, "introduced_date": "2018-01-01", "warning": null, "modalities": ["Images"], "languages": [], "num_papers": 169, "data_loaders": [{"url": "https://github.com/open-mmlab/mmdetection/blob/master/docs/1_exist_data_model.md", "repo": "https://github.com/open-mmlab/mmdetection", "frameworks": ["pytorch"]}, {"url": "https://github.com/facebookresearch/MaskFormer", "repo": "https://github.com/facebookresearch/MaskFormer", "frameworks": ["pytorch"]}, {"url": "https://github.com/nightrome/cocostuff", "repo": "https://github.com/nightrome/cocostuff", "frameworks": ["pytorch"]}], "id": "pwc:dataset/coco-stuff", "type": "dataset", "year": 2018, "month": 1, "day": 1, "variant_surface_forms": ["COCO-Stuff-27", "COCO-Stuff full", "COCO-Stuff-3", "COCO-Stuff-15", "COCO-Stuff test", "COCO-Stuff Labels-to-Photos", "COCO-Stuff 64x64", "COCO-Stuff 128x128"]}
{"url": "https://paperswithcode.com/dataset/mars", "name": "MARS", "full_name": "Motion Analysis and Re-identification Set", "homepage": "http://zheng-lab.cecs.anu.edu.au/Project/project_mars.html", "description": "**MARS** (**Motion Analysis and Re-identification Set**) is a large scale video based person reidentification dataset, an extension of the Market-1501 dataset. It has been collected from six near-synchronized cameras. It consists of 1,261 different pedestrians, who are captured by at least 2 cameras. The variations in poses, colors and illuminations of pedestrians, as well as the poor image quality, make it very difficult to yield high matching accuracy. Moreover, the dataset contains 3,248 distractors in order to make it more realistic. Deformable Part Model and GMMCP tracker were used to automatically generate the tracklets (mostly 25-50 frames long).\r\n\r\nSource: [Multi-Target Tracking in Multiple Non-Overlapping Cameras using Constrained Dominant Sets](https://arxiv.org/abs/1706.06196)", "paper": {"title": "MARS: A Video Benchmark for Large-Scale Person Re-Identification", "url": "https://doi.org/10.1007/978-3-319-46466-4_52"}, "introduced_date": "2016-01-01", "warning": null, "modalities": ["Videos"], "languages": [], "num_papers": 156, "data_loaders": [{"url": "https://docs.activeloop.ai/datasets/mars-dataset", "repo": "https://github.com/activeloopai/Hub", "frameworks": ["tf", "pytorch"]}], "id": "pwc:dataset/mars", "type": "dataset", "year": 2016, "month": 1, "day": 1, "variant_surface_forms": []}
{"url": "https://paperswithcode.com/dataset/ilids-vid", "name": "iLIDS-VID", "full_name": "iLIDS-VID", "homepage": "http://www.eecs.qmul.ac.uk/~xiatian/downloads_qmul_iLIDS-VID_ReID_dataset.html", "description": "The **iLIDS-VID** dataset is a person re-identification dataset which involves 300 different pedestrians observed across two disjoint camera views in public open space. It comprises 600 image sequences of 300 distinct individuals, with one pair of image sequences from two camera views for each person. Each image sequence has variable length ranging from 23 to 192 image frames, with an average number of 73. The iLIDS-VID dataset is very challenging due to clothing similarities among people, lighting and viewpoint variations across camera views, cluttered background and random occlusions.\n\nSource: [http://www.eecs.qmul.ac.uk/~xiatian/downloads_qmul_iLIDS-VID_ReID_dataset.html](http://www.eecs.qmul.ac.uk/~xiatian/downloads_qmul_iLIDS-VID_ReID_dataset.html)\nImage Source: [http://www.eecs.qmul.ac.uk/~xiatian/downloads_qmul_iLIDS-VID_ReID_dataset.html](http://www.eecs.qmul.ac.uk/~xiatian/downloads_qmul_iLIDS-VID_ReID_dataset.html)", "paper": {"title": "Unsupervised Person Re-identification by Deep Learning Tracklet Association", "url": "https://paperswithcode.com/paper/unsupervised-person-re-identification-by-deep-1"}, "introduced_date": "2009-01-01", "warning": null, "modalities": ["Images", "Videos"], "languages": [], "num_papers": 16, "data_loaders": [], "id": "pwc:dataset/ilids-vid", "type": "dataset", "year": 2009, "month": 1, "day": 1, "variant_surface_forms": []}
{"url": "https://paperswithcode.com/dataset/market-1501", "name": "Market-1501", "full_name": "", "homepage": "https://www.kaggle.com/pengcw1/market-1501/data", "description": "**Market-1501** is a large-scale public benchmark dataset for person re-identification. It contains 1501 identities which are captured by six different cameras, and 32,668 pedestrian image bounding-boxes obtained using the Deformable Part Models pedestrian detector. Each person has 3.6 images on average at each viewpoint. The dataset is split into two parts: 750 identities are utilized for training and the remaining 751 identities are used for testing. In the official testing protocol 3,368 query images are selected as probe set to find the correct match across 19,732 reference gallery images.\r\n\r\nSource: [A Survey of Pruning Methods for Efficient Person Re-identification Across Domains](https://arxiv.org/abs/1907.02547)\r\nImage Source: [https://www.researchgate.net/publication/306358716_A_Discriminative_Null_Space_based_Deep_Learning_Approach_for_Person_Re-Identification](https://www.researchgate.net/publication/306358716_A_Discriminative_Null_Space_based_Deep_Learning_Approach_for_Person_Re-Identification)", "paper": {"title": "Scalable Person Re-Identification: A Benchmark", "url": "https://paperswithcode.com/paper/scalable-person-re-identification-a-benchmark"}, "introduced_date": "2015-01-01", "warning": null, "modalities": ["Images"], "languages": [], "num_papers": 706, "data_loaders": [], "id": "pwc:dataset/market-1501", "type": "dataset", "year": 2015, "month": 1, "day": 1, "variant_surface_forms": ["DukeMTMC-reID->Market-1501", "Duke to Market", "Market to Duke", "Market to MSMT"]}
{"url": "https://paperswithcode.com/dataset/viper", "name": "VIPeR", "full_name": "Viewpoint Invariant Pedestrian Recognition", "homepage": "https://github.com/KaiyangZhou/deep-person-reid/blob/master/torchreid/data/datasets/image/viper.py", "description": "The **Viewpoint Invariant Pedestrian Recognition** (**VIPeR**) dataset includes 632 people and two outdoor cameras under different viewpoints and light conditions. Each person has one image per camera and each image has been scaled to be 128\u00d748 pixels. It provides the pose angle of each person as 0\u00b0 (front), 45\u00b0, 90\u00b0 (right), 135\u00b0, and 180\u00b0 (back).\r\n\r\nSource: [PaMM: Pose-aware Multi-shot Matching for Improving Person Re-identification](https://arxiv.org/abs/1705.06011)\r\n\r\nImage Source: [Qin et al](https://www.researchgate.net/figure/Some-examples-from-the-VIPeR-dataset-Each-column-is-one-of-632-same-person-example_fig8_331482460)", "paper": {"title": "Evaluating appearance models for recognition, reacquisition, and tracking", "url": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.331.7285&rep=rep1&type=pdf"}, "introduced_date": null, "warning": null, "modalities": ["Images"], "languages": [], "num_papers": 129, "data_loaders": [{"url": "https://github.com/KaiyangZhou/deep-person-reid", "repo": "https://github.com/KaiyangZhou/deep-person-reid", "frameworks": ["pytorch"]}], "id": "pwc:dataset/viper", "type": "dataset", "year": -1, "month": -1, "day": -1, "variant_surface_forms": []}
{"url": "https://paperswithcode.com/dataset/cuhk01", "name": "CUHK01", "full_name": "CUHK Person Re-identification", "homepage": "http://www.ee.cuhk.edu.hk/~xgwang/CUHK_identification.html", "description": "This dataset contains 971 identities from two disjoint camera views. Each identity has two samples per camera view. It is used for Person Re-identification.\r\n\r\nPaper: [Li W., Zhao R., Wang X. (2013) Human Reidentification with Transferred Metric Learning. In: Lee K.M., Matsushita Y., Rehg J.M., Hu Z. (eds) Computer Vision \u2013 ACCV 2012. ACCV 2012. Lecture Notes in Computer Science, vol 7724. Springer, Berlin, Heidelberg](https://doi.org/10.1007/978-3-642-37331-2_3)", "paper": null, "introduced_date": null, "warning": null, "modalities": ["Images"], "languages": [], "num_papers": 1, "data_loaders": [], "id": "pwc:dataset/cuhk01", "type": "dataset", "year": -1, "month": -1, "day": -1, "variant_surface_forms": []}
{"url": "https://paperswithcode.com/dataset/prid2011", "name": "PRID2011", "full_name": "Person RE-ID 2011", "homepage": null, "description": "PRID 2011 is a person reidentification dataset that provides multiple person trajectories recorded from two different static surveillance cameras, monitoring crosswalks and sidewalks. The dataset shows a clean background, and the people in the dataset are rarely occluded. In the dataset, 200 people appear in both views. Among the 200 people, 178 people have more than 20 appearances\n\nSource: [PaMM: Pose-aware Multi-shot Matching for Improving Person Re-identification](https://arxiv.org/abs/1705.06011)", "paper": {"title": "Mask R-CNN", "url": "https://paperswithcode.com/paper/mask-r-cnn"}, "introduced_date": "2011-01-01", "warning": null, "modalities": [], "languages": [], "num_papers": 16, "data_loaders": [{"url": "https://github.com/Higashiguchi-Shingo/Siamese2", "repo": "https://github.com/Higashiguchi-Shingo/Siamese2", "frameworks": ["tf"]}], "id": "pwc:dataset/prid2011", "type": "dataset", "year": 2011, "month": 1, "day": 1, "variant_surface_forms": []}
{"url": "https://paperswithcode.com/dataset/cuhk03", "name": "CUHK03", "full_name": "Chinese University of Hong Kong Re-identification", "homepage": "http://www.ee.cuhk.edu.hk/~xgwang/CUHK_identification.html", "description": "The **CUHK03** consists of 14,097 images of 1,467 different identities, where 6 campus cameras were deployed for image collection and each identity is captured by 2 campus cameras. This dataset provides two types of annotations, one by manually labelled bounding boxes and the other by bounding boxes produced by an automatic detector. The dataset also provides 20 random train/test splits in which 100 identities are selected for testing and the rest for training\r\n\r\nSource: [Attention Driven Person Re-identification](https://arxiv.org/abs/1810.05866)\r\n\r\nImage Source: [Person Re-Identification Techniques for Intelligent Video Surveillance Systems\r\n](https://www.researchgate.net/publication/324031366_Person_Re-Identification_Techniques_for_Intelligent_Video_Surveillance_Systems)", "paper": {"title": "DeepReID: Deep Filter Pairing Neural Network for Person Re-Identification", "url": "https://paperswithcode.com/paper/deepreid-deep-filter-pairing-neural-network"}, "introduced_date": "2014-01-01", "warning": null, "modalities": ["Images"], "languages": [], "num_papers": 370, "data_loaders": [], "id": "pwc:dataset/cuhk03", "type": "dataset", "year": 2014, "month": 1, "day": 1, "variant_surface_forms": ["CUHK03-NP (detected)", "CUHK - Blur Detection Dataset", "CUHK", "CUHK03 labeled", "CUHK03 detected", "CUHK03 (detected)"]}
{"url": "https://paperswithcode.com/dataset/vehicleid", "name": "VehicleID", "full_name": "PKU VehicleID", "homepage": "https://www.pkuml.org/resources/pku-vehicleid.html", "description": "The \u201c**VehicleID**\u201d dataset contains CARS captured during the daytime by multiple real-world surveillance cameras distributed in a small city in China. There are 26,267 vehicles (221,763 images in total) in the entire dataset. Each image is attached with an id label corresponding to its identity in real world. In addition, the dataset contains manually labelled 10319 vehicles (90196 images in total) of their vehicle model information(i.e.\u201cMINI-cooper\u201d, \u201cAudi A6L\u201d and \u201cBWM 1 Series\u201d).\r\n\r\nSource: [https://www.pkuml.org/resources/pku-vehicleid.html](https://www.pkuml.org/resources/pku-vehicleid.html)\r\nImage Source: [https://www.pkuml.org/resources/pku-vehicleid.html](https://www.pkuml.org/resources/pku-vehicleid.html)", "paper": {"title": "Deep Relative Distance Learning: Tell the Difference Between Similar Vehicles", "url": "https://paperswithcode.com/paper/deep-relative-distance-learning-tell-the"}, "introduced_date": "2016-01-01", "warning": null, "modalities": ["Images"], "languages": [], "num_papers": 103, "data_loaders": [{"url": "https://github.com/michuanhaohao/reid-strong-baseline", "repo": "https://github.com/michuanhaohao/reid-strong-baseline", "frameworks": ["pytorch"]}], "id": "pwc:dataset/vehicleid", "type": "dataset", "year": 2016, "month": 1, "day": 1, "variant_surface_forms": ["VehicleID Small", "VehicleID Medium", "VehicleID Large"]}
{"url": "https://paperswithcode.com/dataset/bp4d", "name": "BP4D", "full_name": "", "homepage": "http://www.cs.binghamton.edu/~lijun/Research/3DFE/3DFE_Analysis.html", "description": "The **BP4D**-Spontaneous dataset is a 3D video database of spontaneous facial expressions in a diverse group of young adults. Well-validated emotion inductions were used to elicit expressions of emotion and paralinguistic communication. Frame-level ground-truth for facial actions was obtained using the Facial Action Coding System. Facial features were tracked in both 2D and 3D domains using both person-specific and generic approaches.\r\nThe database includes forty-one participants (23 women, 18 men). They were 18 \u2013 29 years of age; 11 were Asian, 6 were African-American, 4 were Hispanic, and 20 were Euro-American.  An emotion elicitation protocol was designed to elicit emotions of participants effectively. Eight tasks were covered with an interview process and a series of activities to elicit eight emotions.\r\nThe database is structured by participants. Each participant is associated with 8 tasks. For each task, there are both 3D and 2D videos. As well, the Metadata include manually annotated action units (FACS AU), automatically tracked head pose, and 2D/3D facial landmarks.  The database is in the size of about 2.6TB (without compression).\r\n\r\nSource: [http://www.cs.binghamton.edu/~lijun/Research/3DFE/3DFE_Analysis.html](http://www.cs.binghamton.edu/~lijun/Research/3DFE/3DFE_Analysis.html)\r\nImage Source: [http://www.cs.binghamton.edu/~lijun/Research/3DFE/3DFE_Analysis.html](http://www.cs.binghamton.edu/~lijun/Research/3DFE/3DFE_Analysis.html)", "paper": {"title": "BP4D-Spontaneous: a high-resolution spontaneous 3D dynamic facial expression database", "url": "https://doi.org/10.1016/j.imavis.2014.06.002"}, "introduced_date": "2014-01-01", "warning": null, "modalities": ["Images", "Videos", "3D"], "languages": [], "num_papers": 77, "data_loaders": [], "id": "pwc:dataset/bp4d", "type": "dataset", "year": 2014, "month": 1, "day": 1, "variant_surface_forms": []}
{"url": "https://paperswithcode.com/dataset/disfa", "name": "DISFA", "full_name": "Denver Intensity of Spontaneous Facial Action", "homepage": "http://mohammadmahoor.com/disfa/", "description": "The **Denver Intensity of Spontaneous Facial Action** (**DISFA**) dataset consists of 27 videos of 4844 frames each, with 130,788 images in total. Action unit annotations are on different levels of intensity, which are ignored in the following experiments and action units are either set or unset. DISFA was selected from a wider range of databases popular in the field of facial expression recognition because of the high number of smiles, i.e. action unit 12. In detail, 30,792 have this action unit set, 82,176 images have some action unit(s) set and 48,612 images have no action unit(s) set at all.\r\n\r\nSource: [Deep Learning For Smile Recognition](https://arxiv.org/abs/1602.00172)\r\nImage Source: [https://www.researchgate.net/figure/Examples-of-images-extracted-from-the-DISFA-dataset_fig5_301830237](https://www.researchgate.net/figure/Examples-of-images-extracted-from-the-DISFA-dataset_fig5_301830237)", "paper": {"title": "DISFA: A Spontaneous Facial Action Intensity Database", "url": "https://doi.org/10.1109/T-AFFC.2013.4"}, "introduced_date": "2013-01-01", "warning": null, "modalities": ["Images", "Videos"], "languages": [], "num_papers": 112, "data_loaders": [], "id": "pwc:dataset/disfa", "type": "dataset", "year": 2013, "month": 1, "day": 1, "variant_surface_forms": []}
{"url": "https://paperswithcode.com/dataset/cub-200-2011", "name": "CUB-200-2011", "full_name": "Caltech-UCSD Birds-200-2011", "homepage": "http://www.vision.caltech.edu/visipedia/CUB-200-2011.html", "description": "The **Caltech-UCSD Birds-200-2011** (**CUB-200-2011**) dataset is the most widely-used dataset for fine-grained visual categorization task. It contains 11,788 images of 200 subcategories belonging to birds, 5,994 for training and 5,794 for testing. Each image has detailed annotations: 1 subcategory label, 15 part locations, 312 binary attributes and 1 bounding box. The textual information comes from [Reed et al.]( https://paperswithcode.com/paper/learning-deep-representations-of-fine-grained). They expand the CUB-200-2011 dataset by collecting fine-grained natural language descriptions. Ten single-sentence descriptions are collected for each image. The natural language descriptions are collected through the Amazon Mechanical Turk (AMT) platform, and are required at least 10 words, without any information of subcategories and actions.\r\n\r\nSource: [Fine-grained Visual-textual Representation Learning](https://arxiv.org/abs/1709.00340)\r\nImage Source: [http://www.vision.caltech.edu/visipedia/CUB-200-2011.html](http://www.vision.caltech.edu/visipedia/CUB-200-2011.html)", "paper": {"title": "The Caltech-UCSD Birds-200-2011 Dataset", "url": "http://www.vision.caltech.edu/visipedia/CUB-200-2011.html"}, "introduced_date": null, "warning": null, "modalities": ["Images"], "languages": [], "num_papers": 1480, "data_loaders": [{"url": "https://huggingface.co/datasets/alkzar90/CC6204-Hackaton-Cub-Dataset", "repo": "https://github.com/huggingface/datasets", "frameworks": ["tf", "pytorch", "jax"]}, {"url": "https://docs.activeloop.ai/datasets", "repo": "https://github.com/activeloopai/Hub", "frameworks": ["tf", "pytorch"]}, {"url": "https://www.tensorflow.org/datasets/catalog/caltech_birds2011", "repo": "https://github.com/tensorflow/datasets", "frameworks": ["tf", "jax"]}, {"url": "https://github.com/sicara/easy-few-shot-learning", "repo": "https://github.com/sicara/easy-few-shot-learning", "frameworks": ["pytorch"]}], "id": "pwc:dataset/cub-200-2011", "type": "dataset", "year": -1, "month": -1, "day": -1, "variant_surface_forms": [" CUB-200-2011", "CUB-200-2011, 10 samples per class", "CUB-200-2011 5-way (5-shot)", "CUB-200-2011 5-way (1-shot)", "Imbalanced CUB-200-2011", "CUB-200-2011, 5 samples per class", "CUB-200-2011, 30 samples per class", "CUB-LT", "CUB-200-2011 - 0-Shot", "CUB-200 - 0-Shot Learning", "CUB Birds", "CUB 200 50-way (0-shot)", "CUB 200 5-way 5-shot", "CUB 200 5-way 1-shot", "CUB 128 x 128", "CUB"]}
{"url": "https://paperswithcode.com/dataset/sun397", "name": "SUN397", "full_name": "SUN397", "homepage": "https://vision.princeton.edu/projects/2010/SUN/", "description": "The Scene UNderstanding (SUN) database contains 899 categories and 130,519 images. There are 397 well-sampled categories to evaluate numerous state-of-the-art algorithms for scene recognition.\r\n\r\nImage Source: [The Selection of Useful Visual Words in Class-Imbalanced Image Classification](https://www.semanticscholar.org/paper/The-Selection-of-Useful-Visual-Words-in-Image-Chimlek-Pramokchon/2b23cff4d2072dfc85cf8b09f54475791690a68d)", "paper": null, "introduced_date": null, "warning": null, "modalities": ["Images"], "languages": [], "num_papers": 18, "data_loaders": [{"url": "https://pytorch.org/vision/stable/generated/torchvision.datasets.SUN397.html", "repo": "https://github.com/pytorch/vision", "frameworks": ["pytorch"]}, {"url": "https://www.tensorflow.org/datasets/catalog/sun397", "repo": "https://github.com/tensorflow/datasets", "frameworks": ["tf", "jax"]}], "id": "pwc:dataset/sun397", "type": "dataset", "year": -1, "month": -1, "day": -1, "variant_surface_forms": []}
{"url": "https://paperswithcode.com/dataset/fewrel", "name": "FewRel", "full_name": "Few-Shot Relation Classification Dataset", "homepage": "http://www.zhuhao.me/fewrel/", "description": "The **FewRel** (**Few-Shot Relation Classification Dataset**) contains 100 relations and 70,000 instances from Wikipedia. The dataset is divided into three subsets: training set (64 relations), validation set (16 relations) and test set (20 relations).\r\n\r\nSource: [Neural Snowball for Few-Shot Relation Learning](https://arxiv.org/abs/1908.11007)\r\nImage Source: [https://www.aclweb.org/anthology/D18-1514.pdf](https://www.aclweb.org/anthology/D18-1514.pdf)", "paper": {"title": "FewRel: A Large-Scale Supervised Few-Shot Relation Classification Dataset with State-of-the-Art Evaluation", "url": "https://paperswithcode.com/paper/fewrel-a-large-scale-supervised-few-shot"}, "introduced_date": "2018-01-01", "warning": null, "modalities": ["Texts"], "languages": ["English"], "num_papers": 123, "data_loaders": [{"url": "https://huggingface.co/datasets/few_rel", "repo": "https://github.com/huggingface/datasets", "frameworks": ["tf", "pytorch", "jax"]}], "id": "pwc:dataset/fewrel", "type": "dataset", "year": 2018, "month": 1, "day": 1, "variant_surface_forms": []}
{"url": "https://paperswithcode.com/dataset/dureader", "name": "DuReader", "full_name": "", "homepage": "http://ai.baidu.com/broad/subordinate?dataset=dureader", "description": "**DuReader** is a large-scale open-domain Chinese machine reading comprehension dataset. The dataset consists of 200K questions, 420K answers and 1M documents. The questions and documents are based on Baidu Search and Baidu Zhidao. The answers are manually generated. The dataset additionally provides question type annotations \u2013 each question was manually annotated as either Entity, Description or YesNo and one of Fact or Opinion.\r\n\r\nSource: [https://arxiv.org/pdf/1711.05073v4.pdf](https://arxiv.org/pdf/1711.05073v4.pdf)\r\nImage Source: [https://arxiv.org/pdf/1711.05073v4.pdf](https://arxiv.org/pdf/1711.05073v4.pdf)", "paper": {"title": "DuReader: a Chinese Machine Reading Comprehension Dataset from Real-world Applications", "url": "https://paperswithcode.com/paper/dureader-a-chinese-machine-reading"}, "introduced_date": "2018-01-01", "warning": null, "modalities": ["Texts"], "languages": ["Chinese"], "num_papers": 51, "data_loaders": [], "id": "pwc:dataset/dureader", "type": "dataset", "year": 2018, "month": 1, "day": 1, "variant_surface_forms": []}
{"url": "https://paperswithcode.com/dataset/searchqa", "name": "SearchQA", "full_name": "", "homepage": "https://github.com/nyu-dl/dl4ir-searchQA", "description": "SearchQA was built using an in-production, commercial search engine. It closely reflects the full pipeline of a (hypothetical) general question-answering system, which consists of information retrieval and answer synthesis. \r\n\r\nSource: [SearchQA: A New Q&A Dataset Augmented with Context from a Search Engine](https://arxiv.org/pdf/1704.05179.pdf)", "paper": {"title": "SearchQA: A New Q&A Dataset Augmented with Context from a Search Engine", "url": "https://paperswithcode.com/paper/searchqa-a-new-qa-dataset-augmented-with"}, "introduced_date": "2017-04-18", "warning": null, "modalities": ["Texts"], "languages": ["English"], "num_papers": 117, "data_loaders": [{"url": "https://huggingface.co/datasets/search_qa", "repo": "https://github.com/huggingface/datasets", "frameworks": ["tf", "pytorch", "jax"]}, {"url": "https://github.com/nyu-dl/dl4ir-searchQA", "repo": "https://github.com/nyu-dl/dl4ir-searchQA", "frameworks": ["pytorch"]}], "id": "pwc:dataset/searchqa", "type": "dataset", "year": 2017, "month": 4, "day": 18, "variant_surface_forms": []}
{"url": "https://paperswithcode.com/dataset/coqa", "name": "CoQA", "full_name": "Conversational Question Answering Challenge", "homepage": "https://stanfordnlp.github.io/coqa/", "description": "**CoQA** is a large-scale dataset for building Conversational Question Answering systems. The goal of the CoQA challenge is to measure the ability of machines to understand a text passage and answer a series of interconnected questions that appear in a conversation.\r\n\r\nCoQA contains 127,000+ questions with answers collected from 8000+ conversations. Each conversation is collected by pairing two crowdworkers to chat about a passage in the form of questions and answers. The unique features of CoQA include 1) the questions are conversational; 2) the answers can be free-form text; 3) each answer also comes with an evidence subsequence highlighted in the passage; and 4) the passages are collected from seven diverse domains. CoQA has a lot of challenging phenomena not present in existing reading comprehension datasets, e.g., coreference and pragmatic reasoning.\r\n\r\nSource: [https://stanfordnlp.github.io/coqa/](https://stanfordnlp.github.io/coqa/)\r\nImage Source: [https://stanfordnlp.github.io/coqa/](https://stanfordnlp.github.io/coqa/)", "paper": {"title": "CoQA: A Conversational Question Answering Challenge", "url": "https://paperswithcode.com/paper/coqa-a-conversational-question-answering"}, "introduced_date": "2018-01-01", "warning": null, "modalities": ["Texts"], "languages": ["English"], "num_papers": 178, "data_loaders": [{"url": "https://huggingface.co/datasets/coqa", "repo": "https://github.com/huggingface/datasets", "frameworks": ["tf", "pytorch", "jax"]}, {"url": "https://huggingface.co/datasets/Ruohao/pcmr", "repo": "https://github.com/huggingface/datasets", "frameworks": ["tf", "pytorch", "jax"]}, {"url": "https://parl.ai/docs/tasks.html#conversational-question-answering-challenge", "repo": "https://github.com/facebookresearch/ParlAI", "frameworks": ["pytorch"]}, {"url": "https://docs.activeloop.ai/datasets/coqa-dataset", "repo": "https://github.com/activeloopai/Hub", "frameworks": ["tf", "pytorch"]}, {"url": "https://www.tensorflow.org/datasets/catalog/coqa", "repo": "https://github.com/tensorflow/datasets", "frameworks": ["tf", "jax"]}], "id": "pwc:dataset/coqa", "type": "dataset", "year": 2018, "month": 1, "day": 1, "variant_surface_forms": []}
{"url": "https://paperswithcode.com/dataset/movieqa", "name": "MovieQA", "full_name": "MovieQA", "homepage": "http://movieqa.cs.toronto.edu/home/", "description": "The **MovieQA** dataset is a dataset for movie question answering. to evaluate automatic story comprehension from both video and text. The data set consists of almost 15,000 multiple choice question answers obtained from over 400 movies and features high semantic diversity. Each question comes with a set of five highly plausible answers; only one of which is correct. The questions can be answered using multiple sources of information: movie clips, plots, subtitles, and for a subset scripts and DVS.\r\n\r\nSource: [Movie Question Answering: Remembering the Textual Cues for Layered Visual Contents](https://arxiv.org/abs/1804.09412)\r\nImage Source: [https://www.researchgate.net/figure/Examples-of-multiple-choice-QA-from-the-MovieQA-dataset-Each-question-has-5_fig2_321379716](https://www.researchgate.net/figure/Examples-of-multiple-choice-QA-from-the-MovieQA-dataset-Each-question-has-5_fig2_321379716)", "paper": {"title": "MovieQA: Understanding Stories in Movies through Question-Answering", "url": "https://paperswithcode.com/paper/movieqa-understanding-stories-in-movies"}, "introduced_date": "2016-01-01", "warning": null, "modalities": ["Videos", "Texts"], "languages": ["English"], "num_papers": 73, "data_loaders": [{"url": "https://github.com/thanhdat77/videoquestionsansweringdataset", "repo": "https://github.com/thanhdat77/videoquestionsansweringdataset", "frameworks": ["tf"]}], "id": "pwc:dataset/movieqa", "type": "dataset", "year": 2016, "month": 1, "day": 1, "variant_surface_forms": []}
{"url": "https://paperswithcode.com/dataset/hutter-prize", "name": "Hutter Prize", "full_name": "", "homepage": "http://prize.hutter1.net/", "description": "The Hutter Prize Wikipedia dataset, also known as enwiki8, is a byte-level dataset consisting of the first 100 million bytes of a Wikipedia XML dump. For simplicity we shall refer to it as a character-level dataset. Within these 100 million bytes are 205 unique tokens.\r\n\r\nSource: [NLP Progress](http://nlpprogress.com/english/language_modeling.html)", "paper": null, "introduced_date": null, "warning": null, "modalities": ["Texts"], "languages": ["English"], "num_papers": 12, "data_loaders": [], "id": "pwc:dataset/hutter-prize", "type": "dataset", "year": -1, "month": -1, "day": -1, "variant_surface_forms": []}
{"url": "https://paperswithcode.com/dataset/text8", "name": "Text8", "full_name": "", "homepage": "http://mattmahoney.net/dc/textdata.html", "description": "Desc: [About of Text8](http://mattmahoney.net/dc/textdata.html)", "paper": null, "introduced_date": null, "warning": null, "modalities": ["Texts"], "languages": ["English"], "num_papers": 17, "data_loaders": [], "id": "pwc:dataset/text8", "type": "dataset", "year": -1, "month": -1, "day": -1, "variant_surface_forms": []}
{"url": "https://paperswithcode.com/dataset/icdar-2013", "name": "ICDAR 2013", "full_name": "ICDAR 2013", "homepage": "https://rrc.cvc.uab.es/?ch=2", "description": "The **ICDAR 2013** dataset consists of 229 training images and 233 testing images, with word-level annotations provided. It is the standard benchmark dataset for evaluating near-horizontal text detection.\r\n\r\nSource: [Single Shot Text Detector with Regional Attention](https://arxiv.org/abs/1709.00138)\r\nImage Source: [https://plos.figshare.com/articles/Detection_examples_of_the_proposed_method_on_the_ICDAR_2013_dataset_17_/5325856](https://plos.figshare.com/articles/Detection_examples_of_the_proposed_method_on_the_ICDAR_2013_dataset_17_/5325856)", "paper": {"title": "ICDAR 2013 Robust Reading Competition", "url": "https://doi.org/10.1109/ICDAR.2013.221"}, "introduced_date": "2013-01-01", "warning": null, "modalities": ["Images", "Texts"], "languages": ["English"], "num_papers": 203, "data_loaders": [{"url": "https://docs.activeloop.ai/datasets/icdar-2013-dataset", "repo": "https://github.com/activeloopai/Hub", "frameworks": ["tf", "pytorch"]}, {"url": "https://mindee.github.io/doctr/latest/datasets.html#doctr.datasets.IC13", "repo": "https://github.com/mindee/doctr", "frameworks": ["tf", "pytorch"]}, {"url": "https://github.com/tanglang96/DataLoaders_DALI", "repo": "https://github.com/tanglang96/DataLoaders_DALI", "frameworks": ["pytorch"]}], "id": "pwc:dataset/icdar-2013", "type": "dataset", "year": 2013, "month": 1, "day": 1, "variant_surface_forms": ["ICDAR2013"]}
{"url": "https://paperswithcode.com/dataset/visual-madlibs", "name": "Visual Madlibs", "full_name": "", "homepage": "http://tamaraberg.com/visualmadlibs/", "description": "Visual Madlibs is a dataset consisting of 360,001 focused natural language descriptions for 10,738 images. This dataset is collected using automatically produced fill-in-the-blank templates designed to gather targeted descriptions about: people and objects, their appearances, activities, and interactions, as well as inferences about the general scene or its broader context.\r\n\r\nSource: [Visual Madlibs: Fill in the blank Image Generation and Question Answering](/paper/visual-madlibs-fill-in-the-blank-image)\r\nImage Source: [Yu et al](https://arxiv.org/pdf/1506.00278v1.pdf)", "paper": {"title": "Visual Madlibs: Fill in the blank Image Generation and Question Answering", "url": "https://paperswithcode.com/paper/visual-madlibs-fill-in-the-blank-image"}, "introduced_date": null, "warning": null, "modalities": ["Images", "Texts"], "languages": [], "num_papers": 11, "data_loaders": [], "id": "pwc:dataset/visual-madlibs", "type": "dataset", "year": -1, "month": -1, "day": -1, "variant_surface_forms": []}
{"url": "https://paperswithcode.com/dataset/daquar", "name": "DAQUAR", "full_name": "", "homepage": "https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/research/vision-and-language/visual-turing-challenge/", "description": "DAQUAR (DAtaset for QUestion Answering on Real-world images) is a dataset of human question answer pairs about images.\r\n\r\nSource: [A Multi-World Approach to Question Answering about Real-World Scenes based on Uncertain Input](/paper/a-multi-world-approach-to-question-answering)\r\nImage Source: [https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/research/vision-and-language/visual-turing-challenge/](https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/research/vision-and-language/visual-turing-challenge/)", "paper": {"title": "A Multi-World Approach to Question Answering about Real-World Scenes based on Uncertain Input", "url": "https://paperswithcode.com/paper/a-multi-world-approach-to-question-answering"}, "introduced_date": null, "warning": null, "modalities": ["Images", "Texts"], "languages": [], "num_papers": 43, "data_loaders": [], "id": "pwc:dataset/daquar", "type": "dataset", "year": -1, "month": -1, "day": -1, "variant_surface_forms": []}
{"url": "https://paperswithcode.com/dataset/visual7w", "name": "Visual7W", "full_name": "", "homepage": "http://ai.stanford.edu/~yukez/visual7w/", "description": "**Visual7W** is a large-scale visual question answering (QA) dataset, with object-level groundings and multimodal answers. Each question starts with one of the seven Ws, what, where, when, who, why, how and which. It is collected from 47,300 COCO iamges and it has 327,929 QA pairs, together with 1,311,756 human-generated multiple-choices and 561,459 object groundings from 36,579 categories.\r\n\r\nSource: [https://github.com/yukezhu/visual7w-toolkit](https://github.com/yukezhu/visual7w-toolkit)\r\nImage Source: [http://ai.stanford.edu/~yukez/visual7w/](http://ai.stanford.edu/~yukez/visual7w/)", "paper": {"title": "Visual7W: Grounded Question Answering in Images", "url": "https://paperswithcode.com/paper/visual7w-grounded-question-answering-in"}, "introduced_date": "2016-01-01", "warning": null, "modalities": ["Images", "Texts"], "languages": [], "num_papers": 73, "data_loaders": [], "id": "pwc:dataset/visual7w", "type": "dataset", "year": 2016, "month": 1, "day": 1, "variant_surface_forms": []}
{"url": "https://paperswithcode.com/dataset/fm-iqa", "name": "FM-IQA", "full_name": "Freestyle Multilingual Image Question Answering", "homepage": "http://research.baidu.com/Downloads", "description": "**FM-IQA** is a question-answering dataset containing over 150,000 images and 310,000 freestyle Chinese question-answer pairs and their English translations.\r\n\r\nSource: [Are You Talking to a Machine? Dataset and Methods for Multilingual Image Question Answering](/paper/are-you-talking-to-a-machine-dataset-and)", "paper": {"title": "Are You Talking to a Machine? Dataset and Methods for Multilingual Image Question Answering", "url": "https://paperswithcode.com/paper/are-you-talking-to-a-machine-dataset-and"}, "introduced_date": null, "warning": null, "modalities": ["Images", "Texts"], "languages": ["English", "Chinese"], "num_papers": 9, "data_loaders": [], "id": "pwc:dataset/fm-iqa", "type": "dataset", "year": -1, "month": -1, "day": -1, "variant_surface_forms": []}
{"url": "https://paperswithcode.com/dataset/newsqa", "name": "NewsQA", "full_name": "", "homepage": "https://www.microsoft.com/en-us/research/project/newsqa-dataset/", "description": "The **NewsQA** dataset is a crowd-sourced machine reading comprehension dataset of 120,000 question-answer pairs.\r\n\r\n* Documents are CNN news articles.\r\n* Questions are written by human users in natural language.\r\n* Answers may be multiword passages of the source text.\r\n* Questions may be unanswerable.\r\n* NewsQA is collected using a 3-stage, siloed process.\r\n* Questioners see only an article\u2019s headline and highlights.\r\n* Answerers see the question and the full article, then select an answer passage.\r\n* Validators see the article, the question, and a set of answers that they rank.\r\n* NewsQA is more natural and more challenging than previous datasets.\r\n\r\nSource: [https://www.microsoft.com/en-us/research/project/newsqa-dataset/](https://www.microsoft.com/en-us/research/project/newsqa-dataset/)\r\nImage Source: [Trischler et al](https://arxiv.org/pdf/1611.09830v3.pdf)", "paper": {"title": "NewsQA: A Machine Comprehension Dataset", "url": "https://paperswithcode.com/paper/newsqa-a-machine-comprehension-dataset"}, "introduced_date": "2017-01-01", "warning": null, "modalities": ["Texts"], "languages": ["English"], "num_papers": 218, "data_loaders": [{"url": "https://huggingface.co/datasets/newsqa", "repo": "https://github.com/huggingface/datasets", "frameworks": ["tf", "pytorch", "jax"]}], "id": "pwc:dataset/newsqa", "type": "dataset", "year": 2017, "month": 1, "day": 1, "variant_surface_forms": []}
{"url": "https://paperswithcode.com/dataset/triviaqa", "name": "TriviaQA", "full_name": "", "homepage": "http://nlp.cs.washington.edu/triviaqa/", "description": "**TriviaQA** is a realistic text-based question answering dataset which includes 950K question-answer pairs from 662K documents collected from Wikipedia and the web. This dataset is more challenging than standard QA benchmark datasets such as Stanford Question Answering Dataset (SQuAD), as the answers for a question may not be directly obtained by span prediction and the context is very long. TriviaQA dataset consists of both human-verified and machine-generated QA subsets.\r\n\r\nSource: [Episodic Memory Reader: Learning What to Rememberfor Question Answering from Streaming Data](https://arxiv.org/abs/1903.06164)\r\nImage Source: [Joshi et al](https://arxiv.org/pdf/1705.03551v2.pdf)", "paper": {"title": "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension", "url": "https://paperswithcode.com/paper/triviaqa-a-large-scale-distantly-supervised"}, "introduced_date": "2017-01-01", "warning": null, "modalities": ["Texts"], "languages": ["English"], "num_papers": 378, "data_loaders": [{"url": "https://huggingface.co/datasets/trivia_qa", "repo": "https://github.com/huggingface/datasets", "frameworks": ["tf", "pytorch", "jax"]}, {"url": "https://parl.ai/docs/tasks.html#triviaqa", "repo": "https://github.com/facebookresearch/ParlAI", "frameworks": ["pytorch"]}, {"url": "https://www.tensorflow.org/datasets/catalog/trivia_qa", "repo": "https://github.com/tensorflow/datasets", "frameworks": ["tf", "jax"]}, {"url": "https://docs.allennlp.org/models/main/models/rc/dataset_readers/triviaqa/", "repo": "https://github.com/allenai/allennlp-models", "frameworks": ["pytorch"]}], "id": "pwc:dataset/triviaqa", "type": "dataset", "year": 2017, "month": 1, "day": 1, "variant_surface_forms": ["KILT: TriviaQA"]}
{"url": "https://paperswithcode.com/dataset/recipeqa", "name": "RecipeQA", "full_name": "", "homepage": "https://hucvl.github.io/recipeqa/", "description": "RecipeQA is a dataset for multimodal comprehension of cooking recipes. It consists of over 36K question-answer pairs automatically generated from approximately 20K unique recipes with step-by-step instructions and images. Each question in RecipeQA involves multiple modalities such as titles, descriptions or images, and working towards an answer requires (i) joint understanding of images and text, (ii) capturing the temporal flow of events, and (iii) making sense of procedural knowledge.\r\n\r\nSource: [RecipeQA](https://hucvl.github.io/recipeqa/)", "paper": {"title": "RecipeQA: A Challenge Dataset for Multimodal Comprehension of Cooking Recipes", "url": "https://paperswithcode.com/paper/recipeqa-a-challenge-dataset-for-multimodal"}, "introduced_date": "2018-09-04", "warning": null, "modalities": ["Images", "Texts"], "languages": [], "num_papers": 21, "data_loaders": [], "id": "pwc:dataset/recipeqa", "type": "dataset", "year": 2018, "month": 9, "day": 4, "variant_surface_forms": []}
{"url": "https://paperswithcode.com/dataset/squad", "name": "SQuAD", "full_name": "Stanford Question Answering Dataset", "homepage": "https://rajpurkar.github.io/SQuAD-explorer/", "description": "The **Stanford Question Answering Dataset** (**SQuAD**) is a collection of question-answer pairs derived from Wikipedia articles. In SQuAD, the correct answers of questions can be any sequence of tokens in the given text. Because the questions and answers are produced by humans through crowdsourcing, it is more diverse than some other question-answering datasets. SQuAD 1.1 contains 107,785 question-answer pairs on 536 articles. SQuAD2.0 (open-domain SQuAD, SQuAD-Open), the latest version, combines the 100,000 questions in SQuAD1.1 with over 50,000 un-answerable questions written adversarially by crowdworkers in forms that are similar to the answerable ones.\r\n\r\nSource: [Deep Learning Based Text Classification: A Comprehensive Review](https://arxiv.org/abs/2004.03705)\r\nImage Source: [https://rajpurkar.github.io/SQuAD-explorer/explore/v2.0/dev/Prime_number.html](https://rajpurkar.github.io/SQuAD-explorer/explore/v2.0/dev/Prime_number.html)", "paper": {"title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text", "url": "https://paperswithcode.com/paper/squad-100000-questions-for-machine"}, "introduced_date": "2016-01-01", "warning": null, "modalities": ["Texts"], "languages": ["Arabic"], "num_papers": 1540, "data_loaders": [{"url": "https://huggingface.co/datasets/weijiang2009/AlgmonQuestioningAnsweringDataset", "repo": "https://github.com/huggingface/datasets", "frameworks": ["tf", "pytorch", "jax"]}, {"url": "https://huggingface.co/datasets/GEM/squad", "repo": "https://github.com/huggingface/datasets", "frameworks": ["tf", "pytorch", "jax"]}, {"url": "https://huggingface.co/datasets/GEM/squad_v2", "repo": "https://github.com/huggingface/datasets", "frameworks": ["tf", "pytorch", "jax"]}, {"url": "https://huggingface.co/datasets/Sampson2022/demo", "repo": "https://github.com/huggingface/datasets", "frameworks": ["tf", "pytorch", "jax"]}, {"url": "https://huggingface.co/datasets/Lexi/NQ_squad_format", "repo": "https://github.com/huggingface/datasets", "frameworks": ["tf", "pytorch", "jax"]}, {"url": "https://huggingface.co/datasets/Lexi/spanextract", "repo": "https://github.com/huggingface/datasets", "frameworks": ["tf", "pytorch", "jax"]}, {"url": "https://huggingface.co/datasets/lewtun/autoevaluate__squad", "repo": "https://github.com/huggingface/datasets", "frameworks": ["tf", "pytorch", "jax"]}, {"url": "https://huggingface.co/datasets/PiC/phrase_retrieval", "repo": "https://github.com/huggingface/datasets", "frameworks": ["tf", "pytorch", "jax"]}, {"url": "https://huggingface.co/datasets/squad_v2", "repo": "https://github.com/huggingface/datasets", "frameworks": ["tf", "pytorch", "jax"]}, {"url": "https://huggingface.co/datasets/squad", "repo": "https://github.com/huggingface/datasets", "frameworks": ["tf", "pytorch", "jax"]}, {"url": "https://huggingface.co/datasets/qwant/squad_fr", "repo": "https://github.com/huggingface/datasets", "frameworks": ["tf", "pytorch", "jax"]}, {"url": "https://huggingface.co/datasets/tdklab/Hebrew_Squad_v1", "repo": "https://github.com/huggingface/datasets", "frameworks": ["tf", "pytorch", "jax"]}, {"url": "https://parl.ai/docs/tasks.html#squad2", "repo": "https://github.com/facebookresearch/ParlAI", "frameworks": ["pytorch"]}, {"url": "https://parl.ai/docs/tasks.html#squad", "repo": "https://github.com/facebookresearch/ParlAI", "frameworks": ["pytorch"]}, {"url": "https://docs.activeloop.ai/datasets/squad-dataset", "repo": "https://github.com/activeloopai/Hub", "frameworks": ["tf", "pytorch"]}, {"url": "https://www.tensorflow.org/datasets/catalog/squad", "repo": "https://github.com/tensorflow/datasets", "frameworks": ["tf", "jax"]}, {"url": "https://pytorch.org/text/stable/datasets.html#torchtext.datasets.SQuAD1", "repo": "https://github.com/pytorch/text", "frameworks": ["pytorch"]}, {"url": "https://pytorch.org/text/stable/datasets.html#torchtext.datasets.SQuAD2", "repo": "https://github.com/pytorch/text", "frameworks": ["pytorch"]}, {"url": "https://docs.allennlp.org/models/main/models/rc/dataset_readers/squad/", "repo": "https://github.com/allenai/allennlp-models", "frameworks": ["pytorch"]}], "id": "pwc:dataset/squad", "type": "dataset", "year": 2016, "month": 1, "day": 1, "variant_surface_forms": ["The Stanford Question Answering Dataset", "squad_v2", "SQuAD2.0 dev", "SQuAD2.0", "SQuAD1.1 dev", "SQuAD1.1"]}
{"url": "https://paperswithcode.com/dataset/narrativeqa", "name": "NarrativeQA", "full_name": "NarrativeQA", "homepage": "https://deepmind.com/research/open-source/narrativeqa", "description": "The NarrativeQA dataset includes a list of documents with Wikipedia summaries, links to full stories, and questions and answers.\r\n\r\nSource: [DeepMind](https://deepmind.com/research/open-source/narrativeqa)\r\nImage Source: [Ko\u010disk\u00fd et al ](https://arxiv.org/pdf/1712.07040v1.pdf)", "paper": {"title": "The NarrativeQA Reading Comprehension Challenge", "url": "https://paperswithcode.com/paper/the-narrativeqa-reading-comprehension"}, "introduced_date": "2017-12-19", "warning": null, "modalities": ["Texts"], "languages": ["English"], "num_papers": 94, "data_loaders": [{"url": "https://huggingface.co/datasets/narrativeqa_manual", "repo": "https://github.com/huggingface/datasets", "frameworks": ["tf", "pytorch", "jax"]}, {"url": "https://huggingface.co/datasets/narrativeqa", "repo": "https://github.com/huggingface/datasets", "frameworks": ["tf", "pytorch", "jax"]}, {"url": "https://parl.ai/docs/tasks.html#narrativeqa", "repo": "https://github.com/facebookresearch/ParlAI", "frameworks": ["pytorch"]}], "id": "pwc:dataset/narrativeqa", "type": "dataset", "year": 2017, "month": 12, "day": 19, "variant_surface_forms": []}
{"url": "https://paperswithcode.com/dataset/clicr", "name": "CliCR", "full_name": "CliCR", "homepage": "https://github.com/clips/clicr", "description": "CliCR is a new dataset for domain specific reading comprehension used to construct around 100,000 cloze queries from clinical case reports.\r\n\r\nSource: [CliCR: A Dataset of Clinical Case Reports for Machine Reading Comprehension](https://arxiv.org/pdf/1803.09720v1.pdf)", "paper": {"title": "CliCR: A Dataset of Clinical Case Reports for Machine Reading Comprehension", "url": "https://paperswithcode.com/paper/clicr-a-dataset-of-clinical-case-reports-for"}, "introduced_date": "2018-03-26", "warning": null, "modalities": ["Texts", "Medical"], "languages": ["English"], "num_papers": 16, "data_loaders": [{"url": "https://github.com/clips/clicr", "repo": "https://github.com/clips/clicr", "frameworks": ["none"]}], "id": "pwc:dataset/clicr", "type": "dataset", "year": 2018, "month": 3, "day": 26, "variant_surface_forms": []}
{"url": "https://paperswithcode.com/dataset/ms-marco", "name": "MS MARCO", "full_name": "Microsoft Machine Reading Comprehension Dataset", "homepage": "https://microsoft.github.io/msmarco/", "description": "The **MS MARCO** (Microsoft MAchine Reading Comprehension) is a collection of datasets focused on deep learning in search.\r\nThe first dataset was a question answering dataset featuring 100,000 real Bing questions and a human generated answer. Over time the collection was extended with a 1,000,000 question dataset, a natural language generation dataset, a passage ranking dataset, keyphrase extraction dataset, crawling dataset, and a conversational search.\r\n\r\nSource: [https://microsoft.github.io/msmarco/](https://microsoft.github.io/msmarco/)\r\nImage Source: [https://arxiv.org/pdf/1809.08267.pdf](https://arxiv.org/pdf/1809.08267.pdf)", "paper": {"title": "MS MARCO: A Human Generated MAchine Reading COmprehension Dataset", "url": "https://paperswithcode.com/paper/ms-marco-a-human-generated-machine-reading"}, "introduced_date": "2016-01-01", "warning": null, "modalities": ["Texts"], "languages": ["English"], "num_papers": 528, "data_loaders": [{"url": "https://huggingface.co/datasets/ms_marco", "repo": "https://github.com/huggingface/datasets", "frameworks": ["tf", "pytorch", "jax"]}, {"url": "https://parl.ai/docs/tasks.html#ms_marco", "repo": "https://github.com/facebookresearch/ParlAI", "frameworks": ["pytorch"]}], "id": "pwc:dataset/ms-marco", "type": "dataset", "year": 2016, "month": 1, "day": 1, "variant_surface_forms": ["MSMARCO", "MSMARCO (BEIR)"]}
{"url": "https://paperswithcode.com/dataset/multirc", "name": "MultiRC", "full_name": "Multi-Sentence Reading Comprehension", "homepage": "https://cogcomp.seas.upenn.edu/multirc/", "description": "**MultiRC** (**Multi-Sentence Reading Comprehension**) is a dataset of short paragraphs and multi-sentence questions, i.e., questions that can be answered by combining information from multiple sentences of the paragraph.\r\nThe dataset was designed with three key challenges in mind:\r\n* The number of correct answer-options for each question is not pre-specified. This removes the over-reliance on answer-options and forces them to decide on the correctness of each candidate answer independently of others. In other words, the task is not to simply identify the best answer-option, but to evaluate the correctness of each answer-option individually.\r\n* The correct answer(s) is not required to be a span in the text.\r\n* The paragraphs in the dataset have diverse provenance by being extracted from 7 different domains such as news, fiction, historical text etc., and hence are expected to be more diverse in their contents as compared to single-domain datasets.\r\nThe entire corpus consists of around 10K questions (including about 6K multiple-sentence questions). The 60% of the data is released as training and development data. The rest of the data is saved for evaluation and every few months a new unseen additional data is included for evaluation to prevent unintentional overfitting over time.\r\n\r\nSource: [https://cogcomp.seas.upenn.edu/multirc/](https://cogcomp.seas.upenn.edu/multirc/)\r\nImage Source: [https://paperswithcode.com/paper/looking-beyond-the-surface-a-challenge-set/](https://paperswithcode.com/paper/looking-beyond-the-surface-a-challenge-set/)", "paper": {"title": "Looking Beyond the Surface: A Challenge Set for Reading Comprehension over Multiple Sentences", "url": "https://paperswithcode.com/paper/looking-beyond-the-surface-a-challenge-set"}, "introduced_date": "2018-01-01", "warning": null, "modalities": ["Texts"], "languages": ["English"], "num_papers": 99, "data_loaders": [{"url": "https://www.tensorflow.org/datasets/catalog/eraser_multi_rc", "repo": "https://github.com/tensorflow/datasets", "frameworks": ["tf", "jax"]}], "id": "pwc:dataset/multirc", "type": "dataset", "year": 2018, "month": 1, "day": 1, "variant_surface_forms": []}
{"url": "https://paperswithcode.com/dataset/hotpotqa", "name": "HotpotQA", "full_name": "", "homepage": "https://hotpotqa.github.io/", "description": "**HotpotQA** is a question answering dataset collected on the English Wikipedia, containing about 113K crowd-sourced questions that are constructed to require the introduction paragraphs of two Wikipedia articles to answer. Each question in the dataset comes with the two gold paragraphs, as well as a list of sentences in these paragraphs that crowdworkers identify as supporting facts necessary to answer the question. \r\n\r\nA diverse range of reasoning strategies are featured in HotpotQA, including questions involving missing entities in the question, intersection questions (What satisfies property A and property B?), and comparison questions, where two entities are compared by a common attribute, among others. In the few-document distractor setting, the QA models are given ten paragraphs in which the gold paragraphs are guaranteed to be found; in the open-domain fullwiki setting, the models are only given the question and the entire Wikipedia. Models are evaluated on their answer accuracy and explainability, where the former is measured as overlap between the predicted and gold answers with exact match (EM) and unigram F1, and the latter concerns how well the predicted supporting fact sentences match human annotation (Supporting Fact EM/F1). A joint metric is also reported on this dataset, which encourages systems to perform well on both tasks simultaneously.\r\n\r\nSource: [Answering Complex Open-domain Questions Through Iterative Query Generation](https://arxiv.org/abs/1910.07000)\r\nImage Source: [Yang et al](https://arxiv.org/pdf/1809.09600v1.pdf)", "paper": {"title": "HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering", "url": "https://paperswithcode.com/paper/hotpotqa-a-dataset-for-diverse-explainable"}, "introduced_date": "2018-01-01", "warning": null, "modalities": ["Texts"], "languages": ["English"], "num_papers": 322, "data_loaders": [{"url": "https://huggingface.co/datasets/hotpot_qa", "repo": "https://github.com/huggingface/datasets", "frameworks": ["tf", "pytorch", "jax"]}, {"url": "https://parl.ai/docs/tasks.html#hotpotqa", "repo": "https://github.com/facebookresearch/ParlAI", "frameworks": ["pytorch"]}], "id": "pwc:dataset/hotpotqa", "type": "dataset", "year": 2018, "month": 1, "day": 1, "variant_surface_forms": ["HotpotQA (BEIR)", "hotpot_qa"]}
{"url": "https://paperswithcode.com/dataset/race", "name": "RACE", "full_name": "ReAding Comprehension dataset from Examinations", "homepage": "https://www.cs.cmu.edu/~glai1/data/race/", "description": "The **ReAding Comprehension dataset from Examinations** (**RACE**) dataset is a machine reading comprehension dataset consisting of 27,933 passages and 97,867 questions from English exams, targeting Chinese students aged 12-18. RACE consists of two subsets, RACE-M and RACE-H, from middle school and high school exams, respectively. RACE-M has 28,293 questions and RACE-H has 69,574. Each question is associated with 4 candidate answers, one of which is correct. The data generation process of RACE differs from most machine reading comprehension datasets - instead of generating questions and answers by heuristics or crowd-sourcing, questions in RACE are specifically designed for testing human reading skills, and are created by domain experts.\r\n\r\nSource: [Dynamic Fusion Networks for Machine Reading Comprehension](https://arxiv.org/abs/1711.04964)\r\nImage Source: [Lai et al](https://arxiv.org/pdf/1704.04683v5.pdf)", "paper": {"title": "RACE: Large-scale ReAding Comprehension Dataset From Examinations", "url": "https://paperswithcode.com/paper/race-large-scale-reading-comprehension"}, "introduced_date": "2017-01-01", "warning": null, "modalities": ["Texts"], "languages": ["English"], "num_papers": 261, "data_loaders": [{"url": "https://huggingface.co/datasets/race", "repo": "https://github.com/huggingface/datasets", "frameworks": ["tf", "pytorch", "jax"]}, {"url": "https://www.tensorflow.org/datasets/catalog/race", "repo": "https://github.com/tensorflow/datasets", "frameworks": ["tf", "jax"]}], "id": "pwc:dataset/race", "type": "dataset", "year": 2017, "month": 1, "day": 1, "variant_surface_forms": []}
{"url": "https://paperswithcode.com/dataset/quac", "name": "QuAC", "full_name": "Question Answering in Context", "homepage": "https://quac.ai/", "description": "Question Answering in Context is a large-scale dataset that consists of around 14K crowdsourced Question Answering dialogs with 98K question-answer pairs in total. Data instances consist of an interactive dialog between two crowd workers: (1) a student who poses a sequence of freeform questions to learn as much as possible about a hidden Wikipedia text, and (2) a teacher who answers the questions by providing short excerpts (spans) from the text.\r\n\r\nSource: [https://paperswithcode.com/paper/quac-question-answering-in-context-1/](https://paperswithcode.com/paper/quac-question-answering-in-context-1/)\r\nImage Source: [https://paperswithcode.com/paper/quac-question-answering-in-context-1/](https://paperswithcode.com/paper/quac-question-answering-in-context-1/)", "paper": {"title": "QuAC: Question Answering in Context", "url": "https://paperswithcode.com/paper/quac-question-answering-in-context-1"}, "introduced_date": "2018-01-01", "warning": null, "modalities": ["Texts"], "languages": ["English"], "num_papers": 134, "data_loaders": [{"url": "https://huggingface.co/datasets/quac", "repo": "https://github.com/huggingface/datasets", "frameworks": ["tf", "pytorch", "jax"]}, {"url": "https://parl.ai/docs/tasks.html#question-answering-in-context", "repo": "https://github.com/facebookresearch/ParlAI", "frameworks": ["pytorch"]}, {"url": "https://docs.activeloop.ai/datasets/quac-dataset", "repo": "https://github.com/activeloopai/Hub", "frameworks": ["tf", "pytorch"]}, {"url": "https://www.tensorflow.org/datasets/catalog/quac", "repo": "https://github.com/tensorflow/datasets", "frameworks": ["tf", "jax"]}, {"url": "https://docs.allennlp.org/models/main/models/rc/dataset_readers/quac/", "repo": "https://github.com/allenai/allennlp-models", "frameworks": ["pytorch"]}], "id": "pwc:dataset/quac", "type": "dataset", "year": 2018, "month": 1, "day": 1, "variant_surface_forms": []}
{"url": "https://paperswithcode.com/dataset/wizard-of-oz", "name": "Wizard-of-Oz", "full_name": "", "homepage": "https://arxiv.org/pdf/1606.03777.pdf", "description": "The WoZ 2.0 dataset is a newer dialogue state tracking dataset whose evaluation is detached from the noisy output of speech recognition systems. Similar to DSTC2, it covers the restaurant search domain and has identical evaluation.\r\n\r\nDescription from [NLP Progress](http://nlpprogress.com/english/dialogue.html)\r\n\r\nImage source: [Mrk\u0161i\u0107 et al.](https://arxiv.org/pdf/1606.03777.pdf)", "paper": {"title": "Neural Belief Tracker: Data-Driven Dialogue State Tracking", "url": "https://paperswithcode.com/paper/neural-belief-tracker-data-driven-dialogue"}, "introduced_date": "2016-06-12", "warning": null, "modalities": ["Texts"], "languages": ["English"], "num_papers": 27, "data_loaders": [{"url": "https://huggingface.co/datasets/woz_dialogue", "repo": "https://github.com/huggingface/datasets", "frameworks": ["tf", "pytorch", "jax"]}, {"url": "https://parl.ai/docs/tasks.html#woz-restuarant-reservation-goal-oriented-dialogue", "repo": "https://github.com/facebookresearch/ParlAI", "frameworks": ["pytorch"]}], "id": "pwc:dataset/wizard-of-oz", "type": "dataset", "year": 2016, "month": 6, "day": 12, "variant_surface_forms": []}
{"url": "https://paperswithcode.com/dataset/vcr", "name": "VCR", "full_name": "Visual Commonsense Reasoning", "homepage": "https://visualcommonsense.com/", "description": "**Visual Commonsense Reasoning** (**VCR**) is a large-scale dataset for cognition-level visual understanding. Given a challenging question about an image, machines need to present two sub-tasks: answer correctly and provide a rationale justifying its answer. The VCR dataset contains over 212K (training), 26K (validation) and 25K (testing) questions, answers and rationales derived from 110K movie scenes.\r\n\r\nSource: [Visual Commonsense R-CNN](https://arxiv.org/abs/2002.12204)\r\nImage Source: [From Recognition to Cognition: Visual Commonsense Reasoning](https://paperswithcode.com/paper/from-recognition-to-cognition-visual/)", "paper": {"title": "From Recognition to Cognition: Visual Commonsense Reasoning", "url": "https://paperswithcode.com/paper/from-recognition-to-cognition-visual"}, "introduced_date": "2019-01-01", "warning": null, "modalities": ["Images", "Texts"], "languages": ["English"], "num_papers": 100, "data_loaders": [], "id": "pwc:dataset/vcr", "type": "dataset", "year": 2019, "month": 1, "day": 1, "variant_surface_forms": ["VCR (QA-R) test", "VCR (QA-R) dev", "VCR (Q-AR) test", "VCR (Q-AR) dev", "VCR (Q-A) test", "VCR (Q-A) dev"]}
{"url": "https://paperswithcode.com/dataset/swag", "name": "SWAG", "full_name": "Situations With Adversarial Generations", "homepage": "https://rowanzellers.com/swag/", "description": "Given a partial description like \"she opened the hood of the car,\" humans can reason about the situation and anticipate what might come next (\"then, she examined the engine\"). SWAG (Situations With Adversarial Generations) is a large-scale dataset for this task of grounded commonsense inference, unifying natural language inference and physically grounded reasoning.\r\n\r\nThe dataset consists of 113k multiple choice questions about grounded situations. Each question is a video caption from LSMDC or ActivityNet Captions, with four answer choices about what might happen next in the scene. The correct answer is the (real) video caption for the next event in the video; the three incorrect answers are adversarially generated and human verified, so as to fool machines but not humans. The authors aim for SWAG to be a benchmark for evaluating grounded commonsense NLI and for learning representations.\r\n\r\nSource: [SWAG](https://rowanzellers.com/swag/)\r\nImage Source: [Zellers et al](https://arxiv.org/pdf/1808.05326v1.pdf)", "paper": {"title": "SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference", "url": "https://paperswithcode.com/paper/swag-a-large-scale-adversarial-dataset-for"}, "introduced_date": "2018-08-16", "warning": null, "modalities": ["Texts"], "languages": ["English"], "num_papers": 115, "data_loaders": [{"url": "https://huggingface.co/datasets/swag", "repo": "https://github.com/huggingface/datasets", "frameworks": ["tf", "pytorch", "jax"]}, {"url": "https://docs.activeloop.ai/datasets/swag-dataset", "repo": "https://github.com/activeloopai/Hub", "frameworks": ["tf", "pytorch"]}, {"url": "https://docs.allennlp.org/models/main/models/mc/dataset_readers/swag/", "repo": "https://github.com/allenai/allennlp-models", "frameworks": ["pytorch"]}], "id": "pwc:dataset/swag", "type": "dataset", "year": 2018, "month": 8, "day": 16, "variant_surface_forms": []}
{"url": "https://paperswithcode.com/dataset/event2mind", "name": "Event2Mind", "full_name": "", "homepage": "https://uwnlp.github.io/event2mind/", "description": "Event2Mind is a corpus of 25,000 event phrases covering a diverse range of everyday events and situations.\r\n\r\nSource: [Event2Mind: Commonsense Inference on Events, Intents, and Reactions](/paper/event2mind-commonsense-inference-on-events)", "paper": {"title": "Event2Mind: Commonsense Inference on Events, Intents, and Reactions", "url": "https://paperswithcode.com/paper/event2mind-commonsense-inference-on-events"}, "introduced_date": null, "warning": null, "modalities": ["Texts"], "languages": ["English"], "num_papers": 19, "data_loaders": [{"url": "https://huggingface.co/datasets/event2Mind", "repo": "https://github.com/huggingface/datasets", "frameworks": ["tf", "pytorch", "jax"]}], "id": "pwc:dataset/event2mind", "type": "dataset", "year": -1, "month": -1, "day": -1, "variant_surface_forms": ["Event2Mind dev", "Event2Mind test"]}
{"url": "https://paperswithcode.com/dataset/xnli", "name": "XNLI", "full_name": "Cross-lingual Natural Language Inference", "homepage": "https://github.com/facebookresearch/XNLI", "description": "The **Cross-lingual Natural Language Inference** (**XNLI**) corpus is the extension of the Multi-Genre NLI (MultiNLI) corpus to 15 languages. The dataset was created by manually translating the validation and test sets of MultiNLI into each of those 15 languages. The English training set was machine translated for all languages. The dataset is composed of 122k train, 2490 validation and 5010 test examples.\r\n\r\nSource: [CamemBERT: a Tasty French Language Model](https://arxiv.org/abs/1911.03894)\r\nImage Source: [https://github.com/facebookresearch/XNLI](https://github.com/facebookresearch/XNLI)", "paper": {"title": "XNLI: Evaluating Cross-lingual Sentence Representations", "url": "https://paperswithcode.com/paper/xnli-evaluating-cross-lingual-sentence"}, "introduced_date": "2018-01-01", "warning": null, "modalities": ["Texts"], "languages": ["French", "Spanish", "German", "Chinese", "Multilingual", "Russian", "Arabic", "Bulgarian", "Hindi", "Thai", "Turkish", "Urdu", "Vietnamese", "Greek", "Swahili"], "num_papers": 260, "data_loaders": [{"url": "https://huggingface.co/datasets/xnli", "repo": "https://github.com/huggingface/datasets", "frameworks": ["tf", "pytorch", "jax"]}, {"url": "https://www.tensorflow.org/datasets/catalog/xnli", "repo": "https://github.com/tensorflow/datasets", "frameworks": ["tf", "jax"]}, {"url": "https://github.com/facebookresearch/XNLI", "repo": "https://github.com/facebookresearch/XNLI", "frameworks": ["none"]}], "id": "pwc:dataset/xnli", "type": "dataset", "year": 2018, "month": 1, "day": 1, "variant_surface_forms": ["XNLI Dev", "XNLI Zero-Shot English-to-Spanish", "XNLI Zero-Shot English-to-German", "XNLI Zero-Shot English-to-French", "XNLI French", "XNLI Chinese Dev", "XNLI Chinese"]}
{"url": "https://paperswithcode.com/dataset/snli", "name": "SNLI", "full_name": "Stanford Natural Language Inference", "homepage": "https://nlp.stanford.edu/projects/snli/.", "description": "The **SNLI** dataset (**Stanford Natural Language Inference**) consists of 570k sentence-pairs manually labeled as entailment, contradiction, and neutral. Premises are image captions from Flickr30k, while hypotheses were generated by crowd-sourced annotators who were shown a premise and asked to generate entailing, contradicting, and neutral sentences. Annotators were instructed to judge the relation between sentences given that they describe the same event. Each pair is labeled as \u201centailment\u201d, \u201cneutral\u201d, \u201ccontradiction\u201d or \u201c-\u201d, where \u201c-\u201d indicates that an agreement could not be reached.\r\n\r\nSource: [Breaking NLI Systemswith Sentences that Require Simple Lexical Inferences](https://arxiv.org/abs/1805.02266)", "paper": {"title": "A large annotated corpus for learning natural language inference", "url": "https://paperswithcode.com/paper/a-large-annotated-corpus-for-learning-natural"}, "introduced_date": "2015-01-01", "warning": null, "modalities": ["Texts"], "languages": ["English"], "num_papers": 1027, "data_loaders": [{"url": "https://huggingface.co/datasets/snli", "repo": "https://github.com/huggingface/datasets", "frameworks": ["tf", "pytorch", "jax"]}, {"url": "https://huggingface.co/datasets/shibing624/nli_zh", "repo": "https://github.com/huggingface/datasets", "frameworks": ["tf", "pytorch", "jax"]}, {"url": "https://parl.ai/docs/tasks.html#the-stanford-natural-language-inference-snli-corpus", "repo": "https://github.com/facebookresearch/ParlAI", "frameworks": ["pytorch"]}, {"url": "https://www.tensorflow.org/datasets/catalog/snli", "repo": "https://github.com/tensorflow/datasets", "frameworks": ["tf", "jax"]}, {"url": "https://docs.allennlp.org/models/main/models/pair_classification/dataset_readers/snli/", "repo": "https://github.com/allenai/allennlp-models", "frameworks": ["pytorch"]}], "id": "pwc:dataset/snli", "type": "dataset", "year": 2015, "month": 1, "day": 1, "variant_surface_forms": ["SNLI (8 training examples per class)", "JSNLI"]}
{"url": "https://paperswithcode.com/dataset/scitail", "name": "SciTail", "full_name": "", "homepage": "https://allenai.org/data/scitail", "description": "The **SciTail** dataset is an entailment dataset created from multiple-choice science exams and web sentences. Each question and the correct answer choice are converted into an assertive statement to form the hypothesis. We use information retrieval to obtain relevant text from a large text corpus of web sentences, and use these sentences as a premise P. We crowdsource the annotation of such premise-hypothesis pair as supports (entails) or not (neutral), in order to create the SciTail dataset. The dataset contains 27,026 examples with 10,101 examples with entails label and 16,925 examples with neutral label.\r\n\r\nSource: [Allen Institute for AI](https://allenai.org/data/scitail)\r\nImage source: [Allen Institute for AI](https://allenai.org/data/scitail)", "paper": null, "introduced_date": null, "warning": null, "modalities": ["Texts"], "languages": ["English"], "num_papers": 8, "data_loaders": [{"url": "https://huggingface.co/datasets/bigbio/scitail", "repo": "https://github.com/huggingface/datasets", "frameworks": ["tf", "pytorch", "jax"]}, {"url": "https://huggingface.co/datasets/scitail", "repo": "https://github.com/huggingface/datasets", "frameworks": ["tf", "pytorch", "jax"]}, {"url": "https://www.tensorflow.org/datasets/catalog/sci_tail", "repo": "https://github.com/tensorflow/datasets", "frameworks": ["tf", "pytorch", "jax"]}], "id": "pwc:dataset/scitail", "type": "dataset", "year": -1, "month": -1, "day": -1, "variant_surface_forms": []}
{"url": "https://paperswithcode.com/dataset/chb-mit", "name": "CHB-MIT", "full_name": "CHB-MIT Scalp EEG", "homepage": "https://physionet.org/content/chbmit/1.0.0/", "description": "The **CHB-MIT** dataset is a dataset of EEG recordings from pediatric subjects with intractable seizures. Subjects were monitored for up to several days following withdrawal of anti-seizure mediation in order to characterize their seizures and assess their candidacy for surgical intervention. The dataset contains 23 patients divided among 24 cases (a patient has 2 recordings, 1.5 years apart). The dataset consists of 969 Hours of scalp EEG recordings with 173 seizures. There exist various types of seizures in the dataset (clonic, atonic, tonic). The diversity of patients (Male, Female, 10-22 years old) and different types of seizures contained in the datasets are ideal for assessing the performance of automatic seizure detection methods in realistic settings.\r\n\r\nSource: [Learning Robust Features using Deep Learning for Automatic Seizure Detection](https://arxiv.org/abs/1608.00220)\nImage Source: [https://archive.physionet.org/pn6/chbmit/](https://archive.physionet.org/pn6/chbmit/)", "paper": null, "introduced_date": null, "warning": null, "modalities": ["Audio", "Medical", "EEG"], "languages": [], "num_papers": 3, "data_loaders": [{"url": "https://github.com/nidj23/epilepsy", "repo": "https://github.com/nidj23/epilepsy", "frameworks": ["tf"]}], "id": "pwc:dataset/chb-mit", "type": "dataset", "year": -1, "month": -1, "day": -1, "variant_surface_forms": []}
{"url": "https://paperswithcode.com/dataset/2010-i2b2-va", "name": "2010 i2b2/VA", "full_name": "2010 i2b2/VA", "homepage": "https://www.i2b2.org/NLP/Relations/", "description": "**2010 i2b2/VA** is a biomedical dataset for relation classification and entity typing.", "paper": null, "introduced_date": null, "warning": null, "modalities": ["Texts"], "languages": [], "num_papers": 17, "data_loaders": [], "id": "pwc:dataset/2010-i2b2-va", "type": "dataset", "year": -1, "month": -1, "day": -1, "variant_surface_forms": []}
{"url": "https://paperswithcode.com/dataset/afw", "name": "AFW", "full_name": "Annotated Faces in the Wild", "homepage": "", "description": "**AFW** (**Annotated Faces in the Wild**) is a face detection dataset that contains 205 images with 468 faces. Each face image is labeled with at most 6 landmarks with visibility labels, as well as a bounding box.\r\n\r\nSource: [Face detection, pose estimation, and landmark localization in the wild](https://ieeexplore.ieee.org/document/6248014)", "paper": {"title": "Face detection, pose estimation, and landmark localization in the wild", "url": "https://doi.org/10.1109/CVPR.2012.6248014"}, "introduced_date": "2012-01-01", "warning": null, "modalities": ["Images"], "languages": [], "num_papers": 147, "data_loaders": [{"url": "https://docs.activeloop.ai/datasets/afw-dataset", "repo": "https://github.com/activeloopai/Hub", "frameworks": ["tf", "pytorch"]}], "id": "pwc:dataset/afw", "type": "dataset", "year": 2012, "month": 1, "day": 1, "variant_surface_forms": []}
{"url": "https://paperswithcode.com/dataset/tempeval-3", "name": "TempEval-3", "full_name": "TempEval-3: events, times, and temporal relations", "homepage": "https://aclanthology.org/S13-2001/", "description": "Within the SemEval-2013 evaluation exercise, the TempEval-3 shared task aims to advance research on temporal information processing. It follows on from TempEval-1 and -2, with: a three-part structure covering temporal expression, event, and temporal relation extraction; a larger dataset; and new single measures to rank systems \u2013 in each task and in general.\r\n\r\nWe present TempEval-3 Silver data, with 666K words, and TempEval-3 Platinum, an evaluation set with 6K words. Documents are annotation with EVENT and TIMEX3 spans and also TLINKs, following the TimeML standard.", "paper": {"title": "TempEval-3: Evaluating Events, Time Expressions, and Temporal Relations", "url": "https://paperswithcode.com/paper/tempeval-3-evaluating-events-time-expressions"}, "introduced_date": "2012-06-22", "warning": null, "modalities": ["Texts"], "languages": ["English"], "num_papers": 17, "data_loaders": [], "id": "pwc:dataset/tempeval-3", "type": "dataset", "year": 2012, "month": 6, "day": 22, "variant_surface_forms": []}
{"url": "https://paperswithcode.com/dataset/timebank", "name": "TimeBank", "full_name": "", "homepage": "https://catalog.ldc.upenn.edu/LDC2006T08", "description": "Enriches the TimeML annotations of TimeBank by adding information about the Topic Time in terms of Klein (1994). The annotations are partly automatic, partly inferential and partly manual. The corpus was converted into the native format of the annotation software GraphAnno and POS-tagged using the Stanford bidirectional dependency network tagger. \r\n\r\nSource: [Enriching TimeBank: Towards a more precise annotation of temporal relations in a text](/paper/enriching-timebank-towards-a-more-precise)", "paper": {"title": "Enriching TimeBank: Towards a more precise annotation of temporal relations in a text", "url": "https://paperswithcode.com/paper/enriching-timebank-towards-a-more-precise"}, "introduced_date": null, "warning": null, "modalities": [], "languages": [], "num_papers": 6, "data_loaders": [], "id": "pwc:dataset/timebank", "type": "dataset", "year": -1, "month": -1, "day": -1, "variant_surface_forms": []}
{"url": "https://paperswithcode.com/dataset/semeval-2010-task-8", "name": "SemEval-2010 Task 8", "full_name": "", "homepage": "http://www.kozareva.com/downloads.html", "description": "The dataset for the **SemEval-2010 Task 8** is a dataset for multi-way classification of mutually exclusive semantic relations between pairs of nominals.", "paper": {"title": "SemEval-2010 Task 8: Multi-Way Classification of Semantic Relations Between Pairs of Nominals", "url": "https://paperswithcode.com/paper/semeval-2010-task-8-multi-way-classification"}, "introduced_date": "2019-11-23", "warning": null, "modalities": [], "languages": [], "num_papers": 107, "data_loaders": [{"url": "https://huggingface.co/datasets/sem_eval_2010_task_8", "repo": "https://github.com/huggingface/datasets", "frameworks": ["tf", "pytorch", "jax"]}], "id": "pwc:dataset/semeval-2010-task-8", "type": "dataset", "year": 2019, "month": 11, "day": 23, "variant_surface_forms": []}
{"url": "https://paperswithcode.com/dataset/iemocap", "name": "IEMOCAP", "full_name": "The Interactive Emotional Dyadic Motion Capture\u00a0(IEMOCAP) Database", "homepage": "https://sail.usc.edu/iemocap/iemocap_publication.htm", "description": "Multimodal Emotion Recognition **IEMOCAP** The IEMOCAP dataset consists of 151 videos of recorded dialogues, with 2 speakers per session for a total of 302 videos across the dataset. Each segment is annotated for the presence of 9 emotions (angry, excited, fear, sad, surprised, frustrated, happy, disappointed and neutral) as well as valence, arousal and dominance. The dataset is recorded across 5 sessions with 5 pairs of speakers.\r\n\r\nSource: [Multi-attention Recurrent Network for Human Communication Comprehension](https://arxiv.org/abs/1802.00923)\r\nImage Source: [https://sail.usc.edu/iemocap/Busso_2008_iemocap.pdf](https://sail.usc.edu/iemocap/Busso_2008_iemocap.pdf)", "paper": {"title": "IEMOCAP: interactive emotional dyadic motion capture database", "url": "https://doi.org/10.1007/s10579-008-9076-6"}, "introduced_date": "2008-01-01", "warning": null, "modalities": ["Videos", "Audio"], "languages": [], "num_papers": 427, "data_loaders": [{"url": "https://github.com/macaixia84/dialogue-generation", "repo": "https://github.com/macaixia84/dialogue-generation", "frameworks": ["tf", "pytorch"]}], "id": "pwc:dataset/iemocap", "type": "dataset", "year": 2008, "month": 1, "day": 1, "variant_surface_forms": []}
{"url": "https://paperswithcode.com/dataset/charades-sta", "name": "Charades-STA", "full_name": "", "homepage": "https://github.com/jiyanggao/TALL", "description": "Charades-STA is a new dataset built on top of Charades by adding sentence temporal annotations.\r\n\r\nSource: [TALL: Temporal Activity Localization via Language Query](/paper/tall-temporal-activity-localization-via)", "paper": {"title": "TALL: Temporal Activity Localization via Language Query", "url": "https://paperswithcode.com/paper/tall-temporal-activity-localization-via"}, "introduced_date": null, "warning": null, "modalities": ["Videos", "Texts"], "languages": [], "num_papers": 115, "data_loaders": [{"url": "https://github.com/jiyanggao/TALL", "repo": "https://github.com/jiyanggao/TALL", "frameworks": ["tf"]}], "id": "pwc:dataset/charades-sta", "type": "dataset", "year": -1, "month": -1, "day": -1, "variant_surface_forms": []}
{"url": "https://paperswithcode.com/dataset/senteval", "name": "SentEval", "full_name": "", "homepage": "https://arxiv.org/abs/1803.05449", "description": "SentEval is a toolkit for evaluating the quality of universal sentence representations. SentEval encompasses a variety of tasks, including binary and multi-class classification, natural language inference and sentence similarity. The set of tasks was selected based on what appears to be the community consensus regarding the appropriate evaluations for universal sentence representations. The toolkit comes with scripts to download and preprocess datasets, and an easy interface to evaluate sentence encoders.\r\n\r\nSource: [SentEval: An Evaluation Toolkit for Universal Sentence Representations](/paper/senteval-an-evaluation-toolkit-for-universal)", "paper": {"title": "SentEval: An Evaluation Toolkit for Universal Sentence Representations", "url": "https://paperswithcode.com/paper/senteval-an-evaluation-toolkit-for-universal"}, "introduced_date": null, "warning": null, "modalities": ["Texts"], "languages": ["English"], "num_papers": 131, "data_loaders": [], "id": "pwc:dataset/senteval", "type": "dataset", "year": -1, "month": -1, "day": -1, "variant_surface_forms": []}
{"url": "https://paperswithcode.com/dataset/jfleg", "name": "JFLEG", "full_name": "JHU FLuency-Extended GUG corpus", "homepage": "https://github.com/keisks/jfleg", "description": "JFLEG is for developing and evaluating grammatical error correction (GEC). Unlike other corpora, it represents a broad range of language proficiency levels and uses holistic fluency edits to not only correct grammatical errors but also make the original text more native sounding. \r\n\r\nSource: [JFLEG: A Fluency Corpus and Benchmark for Grammatical Error Correction](https://arxiv.org/pdf/1702.04066v1.pdf)", "paper": {"title": "JFLEG: A Fluency Corpus and Benchmark for Grammatical Error Correction", "url": "https://paperswithcode.com/paper/jfleg-a-fluency-corpus-and-benchmark-for"}, "introduced_date": null, "warning": null, "modalities": ["Texts"], "languages": [], "num_papers": 69, "data_loaders": [{"url": "https://huggingface.co/datasets/jfleg", "repo": "https://github.com/huggingface/datasets", "frameworks": ["tf", "pytorch", "jax"]}, {"url": "https://github.com/keisks/jfleg", "repo": "https://github.com/keisks/jfleg", "frameworks": ["none"]}], "id": "pwc:dataset/jfleg", "type": "dataset", "year": -1, "month": -1, "day": -1, "variant_surface_forms": ["Restricted", "Unrestricted", "_Restricted_"]}
{"url": "https://paperswithcode.com/dataset/esc-50", "name": "ESC-50", "full_name": "ESC-50", "homepage": "https://github.com/karolpiczak/ESC-50", "description": "The **ESC-50** dataset is a labeled collection of 2000 environmental audio recordings suitable for benchmarking methods of environmental sound classification. It comprises 2000 5s-clips of 50 different classes across natural, human and domestic sounds, again, drawn from Freesound.org.\r\n\r\nSource: [The NIGENS General Sound Events Database](https://arxiv.org/abs/1902.08314)\r\nImage Source: [https://github.com/karolpiczak/ESC-50](https://github.com/karolpiczak/ESC-50)", "paper": {"title": "ESC: Dataset for Environmental Sound Classification", "url": "https://doi.org/10.1145/2733373.2806390"}, "introduced_date": "2015-01-01", "warning": null, "modalities": ["Audio"], "languages": [], "num_papers": 183, "data_loaders": [{"url": "https://docs.activeloop.ai/datasets/esc-50-dataset", "repo": "https://github.com/activeloopai/Hub", "frameworks": ["tf", "pytorch"]}, {"url": "https://github.com/karolpiczak/ESC-50", "repo": "https://github.com/karolpiczak/ESC-50", "frameworks": ["none"]}], "id": "pwc:dataset/esc-50", "type": "dataset", "year": 2015, "month": 1, "day": 1, "variant_surface_forms": []}
{"url": "https://paperswithcode.com/dataset/quora-question-pairs", "name": "Quora Question Pairs", "full_name": "", "homepage": "https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs", "description": "**Quora Question Pairs** (QQP) dataset consists of over 400,000 question pairs, and each question pair is annotated with a binary value indicating whether the two questions are paraphrase of each other.\r\n\r\nSource: [Bilateral Multi-Perspective Matching for Natural Language Sentences](/paper/bilateral-multi-perspective-matching-for)", "paper": null, "introduced_date": null, "warning": null, "modalities": ["Texts"], "languages": ["English"], "num_papers": 49, "data_loaders": [{"url": "https://docs.allennlp.org/models/main/models/pair_classification/dataset_readers/quora_paraphrase/", "repo": "https://github.com/allenai/allennlp-models", "frameworks": ["pytorch"]}], "id": "pwc:dataset/quora-question-pairs", "type": "dataset", "year": -1, "month": -1, "day": -1, "variant_surface_forms": []}
{"url": "https://paperswithcode.com/dataset/mmi", "name": "MMI", "full_name": "MMI Facial Expression Database", "homepage": "https://mmifacedb.eu/", "description": "The **MMI** Facial Expression Database consists of over 2900 videos and high-resolution still images of 75 subjects. It is fully annotated for the presence of AUs in videos (event coding), and partially coded on frame-level, indicating for each frame whether an AU is in either the neutral, onset, apex or offset phase. A small part was annotated for audio-visual laughters.\r\n\r\nSource: [https://mmifacedb.eu/](https://mmifacedb.eu/)\r\nImage Source: [https://mmifacedb.eu/](https://mmifacedb.eu/)", "paper": {"title": "Web-based database for facial expression analysis", "url": "https://doi.org/10.1109/ICME.2005.1521424"}, "introduced_date": "2005-01-01", "warning": null, "modalities": ["Images", "Videos"], "languages": [], "num_papers": 55, "data_loaders": [], "id": "pwc:dataset/mmi", "type": "dataset", "year": 2005, "month": 1, "day": 1, "variant_surface_forms": []}
{"url": "https://paperswithcode.com/dataset/jaffe", "name": "JAFFE", "full_name": "Japanese Female Facial Expression", "homepage": "https://zenodo.org/record/3451524", "description": "The **JAFFE** dataset consists of 213 images of different facial expressions from 10 different Japanese female subjects. Each subject was asked to do 7 facial expressions (6 basic facial expressions and neutral) and the images were annotated with average semantic ratings on each facial expression by 60 annotators.\r\n\r\nSource: [Balanced k-Means and Min-Cut Clustering](https://arxiv.org/abs/1411.6235)\r\nImage Source: [https://www.researchgate.net/figure/Examples-of-facial-expression-images-from-the-JAFFE-database_fig11_51873190](https://www.researchgate.net/figure/Examples-of-facial-expression-images-from-the-JAFFE-database_fig11_51873190)", "paper": {"title": "Coding Facial Expressions with Gabor Wavelets", "url": "http://www.kasrl.org/jaffe_download.html"}, "introduced_date": "1998-01-01", "warning": null, "modalities": ["Images"], "languages": ["Japanese"], "num_papers": 72, "data_loaders": [], "id": "pwc:dataset/jaffe", "type": "dataset", "year": 1998, "month": 1, "day": 1, "variant_surface_forms": []}
{"url": "https://paperswithcode.com/dataset/oulu-casia", "name": "Oulu-CASIA", "full_name": "Oulu-CASIA NIR&VIS facial expression database", "homepage": "https://www.oulu.fi/cmvs/node/41316", "description": "The **Oulu-CASIA** NIR&VIS facial expression database consists of six expressions (surprise, happiness, sadness, anger, fear and disgust) from 80 people between 23 and 58 years old. 73.8% of the subjects are males. The subjects were asked to sit on a chair in the observation room in a way that he/ she is in front of camera. Camera-face distance is about 60 cm. Subjects were asked to make a facial expression according to an expression example shown in picture sequences. The imaging hardware works at the rate of 25 frames per second and the image resolution is 320 \u00d7 240 pixels.\r\n\r\nSource: [Facial expression recognition from near-infrared videos](https://ieeexplore.ieee.org/abstract/document/4761697)\r\nImage Source: [https://arxiv.org/abs/1712.03474](https://arxiv.org/abs/1712.03474)", "paper": {"title": "Facial expression recognition from near-infrared videos", "url": "https://doi.org/10.1016/j.imavis.2011.07.002"}, "introduced_date": "2011-01-01", "warning": null, "modalities": ["Images", "Videos"], "languages": [], "num_papers": 70, "data_loaders": [], "id": "pwc:dataset/oulu-casia", "type": "dataset", "year": 2011, "month": 1, "day": 1, "variant_surface_forms": ["CASIA NIR-VIS 2.0", "Oulu-CASIA NIR-VIS"]}
{"url": "https://paperswithcode.com/dataset/sfew", "name": "SFEW", "full_name": "Static Facial Expression in the Wild", "homepage": "https://cs.anu.edu.au/few/AFEW.html", "description": "The Static Facial Expressions in the Wild (**SFEW**) dataset is a dataset for facial expression recognition. It was created by selecting static frames from the AFEW database by computing key frames based on facial point clustering. The most commonly used version, SFEW 2.0, was the benchmarking data for the SReco sub-challenge in EmotiW 2015. SFEW 2.0 has been divided into three sets: Train (958 samples), Val (436 samples) and Test (372 samples). Each of the images is assigned to one of seven expression categories, i.e., anger, disgust, fear, neutral, happiness, sadness, and surprise. The expression labels of the training and validation sets are publicly available, whereas those of the testing set are held back by the challenge organizer.\r\n\r\nSource: [Deep Facial Expression Recognition: A Survey](https://arxiv.org/abs/1804.08348)\r\nImage Source: [https://computervisiononline.com/dataset/1105138659](https://computervisiononline.com/dataset/1105138659)", "paper": {"title": "Static facial expression analysis in tough conditions: Data, evaluation protocol and benchmark", "url": "https://doi.org/10.1109/ICCVW.2011.6130508"}, "introduced_date": "2011-01-01", "warning": null, "modalities": ["Images", "Videos"], "languages": [], "num_papers": 47, "data_loaders": [], "id": "pwc:dataset/sfew", "type": "dataset", "year": 2011, "month": 1, "day": 1, "variant_surface_forms": []}
{"url": "https://paperswithcode.com/dataset/atis", "name": "ATIS", "full_name": "Airline Travel Information Systems", "homepage": "https://github.com/howl-anderson/ATIS_dataset/blob/master/README.en-US.md", "description": "The **ATIS** (**Airline Travel Information Systems**) is a dataset consisting of audio recordings and corresponding manual transcripts about humans asking for flight information on automated airline travel inquiry systems. The data consists of 17 unique intent categories. The original split contains 4478, 500 and 893 intent-labeled reference utterances in train, development and test set respectively.\r\n\r\nSource: [Spoken Language Intent Detection using Confusion2Vec](https://arxiv.org/abs/1904.03576)", "paper": {"title": "The ATIS Spoken Language Systems Pilot Corpus", "url": "https://www.aclweb.org/anthology/H90-1021/"}, "introduced_date": "1990-01-01", "warning": null, "modalities": ["Texts", "Audio"], "languages": ["English"], "num_papers": 213, "data_loaders": [{"url": "https://docs.activeloop.ai/datasets/atis-dataset", "repo": "https://github.com/activeloopai/Hub", "frameworks": ["tf", "pytorch"]}, {"url": "https://github.com/howl-anderson/ATIS_dataset", "repo": "https://github.com/howl-anderson/ATIS_dataset", "frameworks": ["none"]}], "id": "pwc:dataset/atis", "type": "dataset", "year": 1990, "month": 1, "day": 1, "variant_surface_forms": []}
{"url": "https://paperswithcode.com/dataset/activitynet", "name": "ActivityNet", "full_name": "", "homepage": "http://activity-net.org/", "description": "The **ActivityNet** dataset contains 200 different types of activities and a total of 849 hours of videos collected from YouTube. ActivityNet is the largest benchmark for temporal activity detection to date in terms of both the number of activity categories and number of videos, making the task particularly challenging. Version 1.3 of the dataset contains 19994 untrimmed videos in total and is divided into three disjoint subsets, training, validation, and testing by a ratio of 2:1:1. On average, each activity category has 137 untrimmed videos. Each video on average has 1.41 activities which are annotated with temporal boundaries. The ground-truth annotations of test videos are not public.\r\n\r\nSource: [Dynamic Temporal Pyramid Network: A Closer Look at Multi-Scale Modeling for Activity Detection](https://arxiv.org/abs/1808.02536)", "paper": {"title": "ActivityNet: A Large-Scale Video Benchmark for Human Activity Understanding", "url": "https://paperswithcode.com/paper/activitynet-a-large-scale-video-benchmark-for"}, "introduced_date": "2015-01-01", "warning": null, "modalities": ["Videos"], "languages": [], "num_papers": 509, "data_loaders": [{"url": "https://github.com/open-mmlab/mmaction2/blob/master/tools/data/activitynet/README.md", "repo": "https://github.com/open-mmlab/mmaction2", "frameworks": ["pytorch"]}], "id": "pwc:dataset/activitynet", "type": "dataset", "year": 2015, "month": 1, "day": 1, "variant_surface_forms": ["ActivityNet-Entities", "ActivityNet-1.3", "ActivityNet-1.2"]}
{"url": "https://paperswithcode.com/dataset/msra-td500", "name": "MSRA-TD500", "full_name": "MSRA Text Detection 500 Database", "homepage": "http://www.iapr-tc11.org/mediawiki/index.php/MSRA_Text_Detection_500_Database_(MSRA-TD500)", "description": "The **MSRA-TD500** dataset is a text detection dataset that contains 300 training images and 200 test images. Text regions are arbitrarily orientated and annotated at sentence level. Different from the other datasets, it contains both English and Chinese text.\r\n\r\nSource: [Detecting Text in the Wild with Deep Character Embedding Network](https://arxiv.org/abs/1901.00363)\r\nImage Source: [http://www.iapr-tc11.org/mediawiki/images/MSRA-TD500_Example.jpg](http://www.iapr-tc11.org/mediawiki/images/MSRA-TD500_Example.jpg)", "paper": {"title": "Detecting texts of arbitrary orientations in natural images", "url": "https://doi.org/10.1109/CVPR.2012.6247787"}, "introduced_date": "2012-01-01", "warning": null, "modalities": ["Images"], "languages": ["English", "Chinese"], "num_papers": 106, "data_loaders": [], "id": "pwc:dataset/msra-td500", "type": "dataset", "year": 2012, "month": 1, "day": 1, "variant_surface_forms": []}
{"url": "https://paperswithcode.com/dataset/icdar-2015", "name": "ICDAR 2015", "full_name": "", "homepage": "https://iapr.org/archives/icdar2015/index.html%3Fp=254.html", "description": "**ICDAR 2015** was a scene text detection used for the ICDAR 2015 conference.", "paper": null, "introduced_date": null, "warning": null, "modalities": [], "languages": [], "num_papers": 36, "data_loaders": [], "id": "pwc:dataset/icdar-2015", "type": "dataset", "year": -1, "month": -1, "day": -1, "variant_surface_forms": []}
{"url": "https://paperswithcode.com/dataset/total-text", "name": "Total-Text", "full_name": "", "homepage": "https://github.com/cs-chan/Total-Text-Dataset", "description": "**Total-Text** is a text detection dataset that consists of 1,555 images with a variety of text types including horizontal, multi-oriented, and curved text instances. The training split and testing split have 1,255 images and 300 images, respectively.\r\n\r\nSource: [Convolutional Character Networks](https://arxiv.org/abs/1910.07954)\r\nImage Source: [https://github.com/cs-chan/Total-Text-Dataset](https://github.com/cs-chan/Total-Text-Dataset)", "paper": {"title": "Total-Text: A Comprehensive Dataset for Scene Text Detection and Recognition", "url": "https://paperswithcode.com/paper/total-text-a-comprehensive-dataset-for-scene"}, "introduced_date": "2017-01-01", "warning": null, "modalities": ["Images"], "languages": ["English", "Chinese"], "num_papers": 106, "data_loaders": [{"url": "https://www.kaggle.com/datasets/konradb/text-recognition-total-text-dataset", "repo": "https://github.com/Kaggle/kaggle-api", "frameworks": []}, {"url": "https://github.com/cs-chan/Total-Text-Dataset", "repo": "https://github.com/cs-chan/Total-Text-Dataset", "frameworks": ["none"]}], "id": "pwc:dataset/total-text", "type": "dataset", "year": 2017, "month": 1, "day": 1, "variant_surface_forms": []}
{"url": "https://paperswithcode.com/dataset/dota", "name": "DOTA", "full_name": "Dataset for Object deTection in Aerial Images", "homepage": "https://captain-whu.github.io/DOTA/", "description": "DOTA is a large-scale dataset for object detection in aerial images. It can be used to develop and evaluate object detectors in aerial images. The images are collected from different sensors and platforms. Each image is of the size in the range from 800 \u00d7 800 to 20,000 \u00d7 20,000 pixels and contains objects exhibiting a wide variety of scales, orientations, and shapes. The instances in DOTA images are annotated by experts in aerial image interpretation by arbitrary (8 d.o.f.) quadrilateral. We will continue to update DOTA, to grow in size and scope to reflect evolving real-world conditions. Now it has three versions:\r\n\r\nDOTA-v1.0 contains 15 common categories, 2,806 images and 188, 282 instances. The proportions of the training set, validation set, and testing set in DOTA-v1.0 are 1/2, 1/6, and 1/3, respectively.\r\n\r\nDOTA-v1.5 uses the same images as DOTA-v1.0, but the extremely small instances (less than 10 pixels) are also annotated. Moreover, a new category, \u201dcontainer crane\u201d is added. It contains 403,318 instances in total. The number of images and dataset splits are the same as DOTA-v1.0. This version was released for the DOAI Challenge 2019 on Object Detection in Aerial Images in conjunction with IEEE CVPR 2019.\r\n\r\nDOTA-v2.0 collects more Google Earth, GF-2 Satellite, and aerial images. There are 18 common categories, 11,268 images and 1,793,658 instances in DOTA-v2.0. Compared to DOTA-v1.5, it further adds the new categories of \u201dairport\u201d and \u201dhelipad\u201d. The 11,268 images of DOTA are split into training, validation, test-dev, and test-challenge sets. To avoid the problem of overfitting, the proportion of training and validation set is smaller than the test set. Furthermore, we have two test sets, namely test-dev and test-challenge. Training contains 1,830 images and 268,627 instances. Validation contains 593 images and 81,048 instances. We released the images and ground truths for training and validation sets. Test-dev contains 2,792 images and 353,346 instances. We released the images but not the ground truths. Test-challenge contains 6,053 images and 1,090,637 instances.\r\n\r\nSource: [https://captain-whu.github.io/DOTA/index.html](https://captain-whu.github.io/DOTA/index.html)\r\nImage Source: [https://captain-whu.github.io/DOTA/](https://captain-whu.github.io/DOTA/)", "paper": {"title": "DOTA: A Large-scale Dataset for Object Detection in Aerial Images", "url": "https://paperswithcode.com/paper/dota-a-large-scale-dataset-for-object"}, "introduced_date": "2018-01-01", "warning": null, "modalities": ["Images"], "languages": [], "num_papers": 166, "data_loaders": [], "id": "pwc:dataset/dota", "type": "dataset", "year": 2018, "month": 1, "day": 1, "variant_surface_forms": ["DOTA 1.5", "DOTA 1.0"]}
{"url": "https://paperswithcode.com/dataset/hrsc2016", "name": "HRSC2016", "full_name": "High resolution ship collections 2016", "homepage": "https://www.kaggle.com/guofeng/hrsc2016", "description": "High-resolution ship collections 2016 (HRSC2016) is a data set used for scientific research. Currently, all of the images in HRSC2016 were collected from Google Earth.", "paper": null, "introduced_date": null, "warning": null, "modalities": ["Images"], "languages": ["Chinese"], "num_papers": 7, "data_loaders": [], "id": "pwc:dataset/hrsc2016", "type": "dataset", "year": -1, "month": -1, "day": -1, "variant_surface_forms": []}
{"url": "https://paperswithcode.com/dataset/shanghaitech", "name": "ShanghaiTech", "full_name": "", "homepage": "https://github.com/desenzhou/ShanghaiTechDataset", "description": "The Shanghaitech dataset is a large-scale crowd counting dataset. It consists of 1198 annotated crowd images. The dataset is divided into two parts, Part-A containing 482 images and Part-B containing 716 images. Part-A is split into train and test subsets consisting of 300 and 182 images, respectively. Part-B is split into train and test subsets consisting of 400 and 316 images. Each person in a crowd image is annotated with one point close to the center of the head. In total, the dataset consists of 330,165 annotated people. Images from Part-A were collected from the Internet, while images from Part-B were collected on the busy streets of Shanghai.\r\n\r\nSource: [Iterative Crowd Counting](https://arxiv.org/abs/1807.09959)\r\n\r\nImage Source: [Li et al](https://www.researchgate.net/figure/Test-images-from-the-ShanghaiTech-A-28-dataset-The-goal-of-this-paper-is-to-calculate_fig1_322652466)", "paper": {"title": "Single-Image Crowd Counting via Multi-Column Convolutional Neural Network", "url": "https://paperswithcode.com/paper/single-image-crowd-counting-via-multi-column-1"}, "introduced_date": "2016-01-01", "warning": null, "modalities": ["Images"], "languages": [], "num_papers": 218, "data_loaders": [{"url": "https://github.com/desenzhou/ShanghaiTechDataset", "repo": "https://github.com/desenzhou/ShanghaiTechDataset", "frameworks": ["none"]}], "id": "pwc:dataset/shanghaitech", "type": "dataset", "year": 2016, "month": 1, "day": 1, "variant_surface_forms": ["ShanghaiTech A", "ShanghaiTech B"]}
{"url": "https://paperswithcode.com/dataset/ucsd", "name": "UCSD Ped2", "full_name": "UCSD Anomaly Detection Dataset", "homepage": "http://www.svcl.ucsd.edu/projects/anomaly/dataset.htm", "description": "The **UCSD** Anomaly Detection Dataset was acquired with a stationary camera mounted at an elevation, overlooking pedestrian walkways. The crowd density in the walkways was variable, ranging from sparse to very crowded. In the normal setting, the video contains only pedestrians. Abnormal events are due to either:\r\nthe circulation of non pedestrian entities in the walkways\r\nanomalous pedestrian motion patterns\r\nCommonly occurring anomalies include bikers, skaters, small carts, and people walking across a walkway or in the grass that surrounds it. A few instances of people in wheelchair were also recorded. All abnormalities are naturally occurring, i.e. they were not staged for the purposes of assembling the dataset. The data was split into 2 subsets, each corresponding to a different scene. The video footage recorded from each scene was split into various clips of around 200 frames.\r\n\r\nSource: [The UCSD Anomaly Detection Dataset](http://www.svcl.ucsd.edu/projects/anomaly/dataset.htm)\r\nImage Source: [http://www.svcl.ucsd.edu/publications/conference/2010/cvpr2010/cvpr_anomaly_2010.pdf](http://www.svcl.ucsd.edu/publications/conference/2010/cvpr2010/cvpr_anomaly_2010.pdf)", "paper": {"title": "Anomaly detection in crowded scenes", "url": "https://doi.org/10.1109/CVPR.2010.5539872"}, "introduced_date": "2010-01-01", "warning": null, "modalities": ["Images", "Videos"], "languages": [], "num_papers": 66, "data_loaders": [], "id": "pwc:dataset/ucsd", "type": "dataset", "year": 2010, "month": 1, "day": 1, "variant_surface_forms": ["UCSD-MIT Human Motion"]}
{"url": "https://paperswithcode.com/dataset/dcase-2017", "name": "DCASE 2017", "full_name": "DCASE 2017", "homepage": "http://dcase.community/challenge2017/index", "description": "The **DCASE 2017** rare sound events dataset contains isolated sound events for three classes: 148 crying babies (mean duration 2.25s), 139 glasses breaking (mean duration 1.16s), and 187 gun shots (mean duration 1.32s). As with the DCASE 2016 data, silences are not excluded from active event markings in the annotations. While this data set contains many samples per class, there are only three classes\n\nSource: [The NIGENS General Sound Events Database](https://arxiv.org/abs/1902.08314)\nImage Source: [https://arxiv.org/pdf/1911.06878.pdf](https://arxiv.org/pdf/1911.06878.pdf)", "paper": null, "introduced_date": null, "warning": null, "modalities": ["Audio"], "languages": [], "num_papers": 2, "data_loaders": [], "id": "pwc:dataset/dcase-2017", "type": "dataset", "year": -1, "month": -1, "day": -1, "variant_surface_forms": []}
